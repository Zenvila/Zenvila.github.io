[{"content":"","date":"20 December 2025","externalUrl":null,"permalink":"/tags/ci/cd/","section":"Tags","summary":"","title":"CI/CD","type":"tags"},{"content":" CI/CD (Github ACtions) # Understanding GitHub Actions with CI/CD (Ubuntu + Docker) # In this blog post, we\u0026rsquo;ll learn what GitHub Actions is, what CI/CD means, and how to implement a real-world automated software workflow using GitHub Actions on a sample Node.js project. Everything is explained clearly with theory, purpose, file structure, and commands.\nWhat is GitHub Actions? # GitHub Actions is a platform to automate software workflows like testing, building, or deploying your code. It\u0026rsquo;s not a CI/CD tool itself, but it enables automation where CI/CD is one of the many workflows.\nCI/CD is a development practice. GitHub Actions is a platform to automate it.\nWhat are workflows? # Let‚Äôs understand it simply. Suppose you\u0026rsquo;re hosting a website on GitHub Pages. When you make changes on your local system and push them, you expect GitHub to reflect those changes.\nBut what if:\nYou made a mistake\nYou pushed code with an error\nThe deployment silently fails?\nGitHub Actions gives you an \u0026ldquo;Actions\u0026rdquo; tab where you can see every deployment and whether your push was successful. That background process of validating, testing, and deploying is called a workflow.\nWhat is an Event? # An event is a trigger that causes a GitHub workflow to run.\nExamples:\nSomeone pushes code (git push)\nA pull request is opened\nA schedule is reached (cron jobs)\nWhat is CI/CD? # | Term | Meaning | | | | | CI (Continuous Integration) | Automatically test code after every push | | CD (Continuous Delivery/Deployment) | Automatically build and deploy the app if tests pass |\nCI/CD ensures software updates are automatic, reliable, and repeatable.\nGitHub Actions runs workflows inside a temporary Ubuntu machine (called a \u0026ldquo;runner\u0026rdquo;).\nAre GitHub Actions OS Dependent? # No. GitHub Actions is not system dependent. You can choose the operating system via:\nruns-on: ubuntu-latest # or windows-latest, macos-latest GitHub creates a virtual machine (VM) based on your config, installs dependencies, runs your code, and deletes the VM.\nYour code is tested according to the test cases you provide. This step is called CI. If successful, it proceeds to CD, which builds a container and deploys the app.\nLet‚Äôs Do It Practically (CI + CD) # Project Structure # ubuntu-ci-cd/ # Main directory ‚îú‚îÄ‚îÄ index.js # App logic ‚îú‚îÄ‚îÄ index.test.js # Test cases ‚îú‚îÄ‚îÄ package.json # Dependencies/scripts ‚îú‚îÄ‚îÄ Dockerfile # Docker build config (CD) ‚îî‚îÄ‚îÄ .github/ # GitHub workflow directory ‚îî‚îÄ‚îÄ workflows/ ‚îî‚îÄ‚îÄ ci-cd.yml # Workflow definition File-by-File Breakdown # 1. index.js # Purpose:\nYour actual app code (e.g., a function, backend, etc.) function hello(name) { return `Hello, ${name}`; } module.exports = hello; Why: This is the code to test and deploy.\n2. index.test.js # Purpose:\nContains test cases for your app. const hello = require(\u0026#39;./index\u0026#39;); test(\u0026#39;says hello to Zen\u0026#39;, () =\u0026gt; { expect(hello(\u0026#39;Zen\u0026#39;)).toBe(\u0026#39;Hello, Zen\u0026#39;); }); Why: This is used for CI. If tests fail, deployment is skipped.\n3. package.json # Purpose:\nNode.js config for scripts and dependencies { \u0026#34;name\u0026#34;: \u0026#34;ubuntu-ci-cd\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.0.0\u0026#34;, \u0026#34;scripts\u0026#34;: { \u0026#34;test\u0026#34;: \u0026#34;jest\u0026#34; }, \u0026#34;devDependencies\u0026#34;: { \u0026#34;jest\u0026#34;: \u0026#34;^29.0.0\u0026#34; } } Why:\nDeclares dependencies\nTells GitHub how to run tests\n4. Dockerfile # Purpose:\nBuilds a Docker image (CD step) FROM node:18-slim WORKDIR /app COPY . . RUN npm install CMD [\u0026#34;npm\u0026#34;, \u0026#34;test\u0026#34;] Why:\nContainerizes your app\nMakes it deployable to DockerHub/Kubernetes, etc.\n5. .github/workflows/ci-cd.yml # Purpose:\nThe CI/CD automation script name: CI + CD on Ubuntu on: push: branches: [main] jobs: build-test-deploy: runs-on: ubuntu-latest steps: - name: Checkout code uses: actions/checkout@v3 - name: Set up Node.js uses: actions/setup-node@v3 with: node-version: \u0026#39;18\u0026#39; - name: Install dependencies run: npm install - name: Run tests (CI) run: npm test - name: Build Docker image (CD) run: docker build -t ubuntu-test-app . - name: Run container (simulate deploy) run: docker run --rm ubuntu-test-app Local Testing # cd ubuntu-ci-cd # Run tests manually npm install npm test # Build Docker image docker build -t ubuntu-test-app . # Run container docker run --rm ubuntu-test-app Remote Testing with GitHub Actions # git init git remote add origin https://github.com/YOUR_USERNAME/ubuntu-ci-cd.git git add . git commit -m \u0026#34;Initial commit\u0026#34; git branch -M main git push -u origin main GitHub will trigger .github/workflows/ci-cd.yml automatically.\nGo to Actions tab in your GitHub repo to see logs and status.\nSummary Table # | File | Role | Purpose | | | | | | index.js | App code | Code you want to test/deploy | | index.test.js | Test file | Runs automatically to check code | | package.json | Config file | Declares dependencies \u0026amp; scripts | | Dockerfile | Build instructions | Builds Docker image with Ubuntu | | ci-cd.yml | Workflow | Automates test + build + deploy |\nGitHub Actions: Operating System Runners # | runs-on: | GitHub Runner VM | | | | | ubuntu-latest | Ubuntu 22.04 | | windows-latest | Windows Server | | macos-latest | macOS Big Sur/Catalina |\nWhy CI Might Fail in GitHub Actions # Bug in your code\nWrong or missing dependencies\nJest test failed\nThis has nothing to do with Arch or Ubuntu. It‚Äôs logic-related. Now you know:\nWhat GitHub Actions is\nWhat CI/CD means\nHow workflows trigger\nHow to build and test an app locally and remotely\nYou\u0026rsquo;re ready to automate!\nP.S. # If you spot any mistakes, feel free to point them out ‚Äî we‚Äôre all here to learn together! üòä\nHaris\nFAST-NUCES\nBS Computer Science | Class of 2027\nüîó Portfolio: zenvila.github.io\nüîó GitHub: github.com/Zenvila\nüîó LinkedIn: linkedin.com/in/haris-shahzad-7b8746291\nüî¨ Member: COLAB (Research Lab)\n","date":"20 December 2025","externalUrl":null,"permalink":"/posts/cicd-github-actions/","section":"Posts","summary":"","title":"Cicd Github Actions","type":"posts"},{"content":"","date":"20 December 2025","externalUrl":null,"permalink":"/tags/containers/","section":"Tags","summary":"","title":"Containers","type":"tags"},{"content":"","date":"20 December 2025","externalUrl":null,"permalink":"/","section":"Home","summary":"","title":"Home","type":"page"},{"content":"","date":"20 December 2025","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"20 December 2025","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"20 December 2025","externalUrl":null,"permalink":"/tags/technology/","section":"Tags","summary":"","title":"Technology","type":"tags"},{"content":"","date":"8 December 2025","externalUrl":null,"permalink":"/tags/data-science/","section":"Tags","summary":"","title":"Data Science","type":"tags"},{"content":"","date":"8 December 2025","externalUrl":null,"permalink":"/tags/testing/","section":"Tags","summary":"","title":"Testing","type":"tags"},{"content":" Unit Testing in Python # A Must-Know for Every Programmer # Unit testing is one of the most crucial topics that every programmer should be familiar with. It ensures the correctness of individual components of a program by testing them in isolation. In this blog, we will focus on unit testing in Python and explore five popular libraries that can be used for this purpose:\nPytest\nPyUnit\nRobot Framework\nSplinter\nBehave\nWhy Do We Need Unit Testing? # The primary purpose of unit testing is to validate that each part of your application works as expected. Let‚Äôs break it down with an example:\nAs a developer, you might build an application or write a piece of code for a specific task. Unit testing ensures that your application functions correctly from every perspective. By storing test cases in a separate test file and importing your code‚Äôs functions, you can efficiently run tests using a library like Pytest to verify the correctness of your code. The terminal output will indicate whether the tests pass or fail.\nUnit Testing Frameworks in Different Programming Languages # Different programming languages have their own frameworks for unit testing. For Python, we‚Äôll focus on using Pytest in this tutorial.\n![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj-Jn1eisOSN9QB47QZCrMsVYzdpErSRlkFOTulO5AfZbRwusbxwfjOCYHeFkkglZeySelrgf4RGwM7cIf-nGgY3UjDp5rmIOkP2fiMPn6bjP2vRH4Pi-S8Jv3Pwcr6BE2OoV6YN72z6Zd_jAPtfaLOIhgmV0ExKteuRHwcbZJ73EN8NKkpcHiapPKNLHVV/w541-h220/Test-Framework%20.png align=\u0026ldquo;left\u0026rdquo;)\nTechnical Aspects: A Simple Example # Before diving into unit testing, ensure you have Python 3 and pip installed on your system. You can do this with the following command (for systems using Pacman package manager):\nNote: The provided command is for Arch; you can adapt it according to your operating system.\nsudo pacman -S python python-pip ![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiELb4e-j9Cn44MtTiCqdXK2KpXoK1FD9ILB7M_tIs_YSiSDZpXZ5QIPh7E3Wga_oNLh_eUPSdsezJicFytruYDxva6A_Z5y2Zn7LLPmt5kz8EXcUY0_a8WwxviiDCAtcvLfL6w85AgqB38GhyHkNSrpJBls8sfwrEdaXfrcN3MuGur5wKnNNpnQDRLZaKq/w399-h281/Screenshot_20250117_063409.png align=\u0026ldquo;left\u0026rdquo;)\nNext, install the Pytest library for testing:\nsudo pacman -S python-pytest Step 1: Create a File for the Code to Be Tested # Let‚Äôs create a file named basicmath.py containing a simple function:\n# basicmath.py def div(a, b): \u0026#34;\u0026#34;\u0026#34;Divide two numbers.\u0026#34;\u0026#34;\u0026#34; return a / b # Example usage print(div(12, 4)) You can run this file to ensure it works as expected:\npython3 basicmath.py¬†Step 2: Create a Test File # Now, create another file named test.py in the same directory. This file will contain test cases for the div function in basicmath.py.\n# test.py # Import the function to be tested from basicmath import div def test_div(): \u0026#34;\u0026#34;\u0026#34;Test integer division.\u0026#34;\u0026#34;\u0026#34; assert div(4, 2) == 2 def test_div2(): \u0026#34;\u0026#34;\u0026#34;Test another integer division.\u0026#34;\u0026#34;\u0026#34; assert div(6, 2) == 3 def test_for_float(): \u0026#34;\u0026#34;\u0026#34;Test division resulting in a float.\u0026#34;\u0026#34;\u0026#34; assert div(1, 2) == 0.5 Step 3: Run the Tests # To run the test file, use the following command:\npytest test.py Pytest will execute the test cases and display whether they pass or fail.\nPass :¬†![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiNGgTHGFn-9-xS7yUTNfeira71KoTEmaB5tmiSnOPPHMzKfW7VkMhO-bDeOrpkeFE5pd7eV61DVYMmJe3iM00tvEPNocP9AhifLFCrum1ReLqc-Asmfpw4Se2sD9G3tQ_jIm9j6m3PC0t3p5HvRXWPevt9Godt7u6GjLhRzEC4QDGD4O90SWCegEgPqUXs/w583-h122/Screenshot_20250117_063751.png align=\u0026ldquo;left\u0026rdquo;)\nFail:\nHere in this we can see one test case is not pass and is clearly mentioned¬†**(test for float) is FAIL.\n**\n![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjOGt0ZtjSroGhP2xT5VJgcfaZ3Hz5Nt3kXJgsO8dFkUZKB10SFxqPpC3-9XUy0DzCa2M57BgtiqW-9h_cCCXBsqe9jmdkPeU_jx1faRytH4ITo-XOdcSx9udcawUTlq2xXLNcI92jQ2_3dN4kvFc_UsXSgBCPl2IezqBzXm0ecaID5iue8iyu-nKv3wNqi/w567-h122/Screenshot_20250117_064006.png align=\u0026ldquo;left\u0026rdquo;)\nKey Points to Remember # Always keep the test file in the same directory as the code file (or configure the test runner accordingly).\nUse descriptive function names and comments for better readability.\nUnit testing helps catch errors early, saving time and effort in the long run.\nBy following these steps, you can efficiently write and execute unit tests, ensuring your Python code is robust and reliable. Happy testing!\nP.S. If you spot any mistakes, please don\u0026rsquo;t hesitate to point them out. We\u0026rsquo;re all here to learn together!\nP.S. # If you spot any mistakes, please don\u0026rsquo;t hesitate to point them out. We\u0026rsquo;re all here to learn together! üòä\nHaris\nFAST (NUCES)\nBS Computer Science | Class of 2027\nüìå GitHub: https://github.com/Zenvila\nüìå LinkedIn: https://www.linkedin.com/in/haris-shahzad-7b8746291/\nüìå Member: COLAB (Research Lab)\n","date":"8 December 2025","externalUrl":null,"permalink":"/posts/unit-testing-in-python/","section":"Posts","summary":"","title":"Unit Testing In Python","type":"posts"},{"content":"","date":"23 November 2025","externalUrl":null,"permalink":"/tags/linux/","section":"Tags","summary":"","title":"Linux","type":"tags"},{"content":"","date":"23 November 2025","externalUrl":null,"permalink":"/tags/open-source/","section":"Tags","summary":"","title":"Open Source","type":"tags"},{"content":" Step-by-Step Arch Linux Installation \u0026amp; Post-Setup # Complete Arch Linux Installation and Configuration Guide # Introduction # This guide provides a step-by-step process for installing Arch Linux and setting up essential configurations, including networking, user management, and software installation. This is a minimal setup guide without any graphical user interface (GUI) installation.\nStep 1: Boot Into Arch Linux Live Environment # Boot from the Arch Linux installation media and verify the internet connection using:\nping -c 3 archlinux.org If it fails, follow network setup steps below.\nStep 2: Set Up Network Connection # List available network interfaces:\nip link Connect to Wi-Fi (if required):\nwifi-menu # If using netctl For NetworkManager users:\npacman -S networkmanager --noconfirm systemctl enable --now NetworkManager Verify connection again:\nping -c 3 archlinux.org Step 3: Partition and Format the Disk # Check available disks:\nfdisk -l Partition the disk using:\ncfdisk /dev/sdX # Replace X with your drive letter Create partitions and format them:\nmkfs.ext4 /dev/sdX1 # Format root partition mkfs.vfat -F32 /dev/sdX2 # Format EFI partition (if using UEFI) Step 4: Mount and Install Arch Linux # mount /dev/sdX1 /mnt pacstrap /mnt base linux linux-firmware Generate fstab:\ngenfstab -U /mnt \u0026gt;\u0026gt; /mnt/etc/fstab Step 5: System Configuration # Chroot into the new system:\narch-chroot /mnt Set time zone:\nln -sf /usr/share/zoneinfo/Region/City /etc/localtime hwclock --systohc Enable localization:\necho \u0026#34;en_US.UTF-8 UTF-8\u0026#34; \u0026gt;\u0026gt; /etc/locale.gen locale-gen echo \u0026#34;LANG=en_US.UTF-8\u0026#34; \u0026gt; /etc/locale.conf Set hostname:\necho \u0026#34;archlinux\u0026#34; \u0026gt; /etc/hostname Step 6: Set Root Password and Create User # passwd Create a new user:\nuseradd -m -G wheel -s /bin/bash dawood passwd dawood Grant sudo privileges:\nEDITOR=nano visudo Uncomment:\n%dawood ALL=(ALL:ALL) ALL Step 7: Install and Configure Bootloader # For systemd-boot (UEFI):\nbootctl install echo \u0026#34;title Arch Linux\u0026#34; \u0026gt; /boot/loader/entries/arch.conf echo \u0026#34;linux /vmlinuz-linux\u0026#34; \u0026gt;\u0026gt; /boot/loader/entries/arch.conf echo \u0026#34;initrd /initramfs-linux.img\u0026#34; \u0026gt;\u0026gt; /boot/loader/entries/arch.conf echo \u0026#34;options root=/dev/sdX1 rw\u0026#34; \u0026gt;\u0026gt; /boot/loader/entries/arch.conf For GRUB (BIOS/UEFI):\npacman -S grub os-prober --noconfirm grub-install --target=x86_64-efi --efi-directory=/boot --bootloader-id=GRUB Generate GRUB config:\ngrub-mkconfig -o /boot/grub/grub.cfg Exit chroot and reboot:\nexit reboot Step 8: Post-Installation Configurations # Login as the new user:\nsu - dawood Enable networking:\nsystemctl enable --now NetworkManager Check internet:\nping -c 3 archlinux.org Step 9: Install Essential Packages # sudo pacman -Syu sudo pacman -S base-devel neofetch git wget curl tree xdg-user-dirs --noconfirm Set Neofetch to run on terminal start:\necho \u0026#34;neofetch\u0026#34; \u0026gt;\u0026gt; ~/.bashrc Step 10: Utilities Installation # sudo pacman -S os-prober xdg-user-dirs tree ranger thunar wezterm dmenu nitrogen bluez bluez-utils --noconfirm Install Yay for AUR package management:\ngit clone https://aur.archlinux.org/yay.git cd yay makepkg -si Install Google Chrome and Visual Studio Code:\nyay -S google-chrome visual-studio-code-bin --noconfirm Verify Yay installation:\nyay --version yay -Syu Step 11: Install and Configure SSH (Optional) # sudo pacman -S openssh --noconfirm sudo systemctl enable --now sshd Check SSH status:\nsystemctl status sshd Connect via SSH:\nssh dawood@your-ip-address Conclusion # With these steps, you\u0026rsquo;ve successfully installed Arch Linux, set up networking, created a user, installed X11, configured WezTerm, installed essential software, and enabled SSH for remote access.\nEnjoy your minimal and efficient Arch Linux setup! Haris\nFAST (NUCES)\nBS Computer Science | Class of 2027\nüìå GitHub: https://github.com/Zenvila\nüìå LinkedIn: https://www.linkedin.com/in/haris-shahzad-7b8746291/\nüìå Member: COLAB (Research Lab)\n","date":"23 November 2025","externalUrl":null,"permalink":"/posts/step-by-step-arch-linux-installation-and-post-setup-1--deleted/","section":"Posts","summary":"","title":"Step By Step Arch Linux Installation And Post Setup 1  Deleted","type":"posts"},{"content":"","date":"23 November 2025","externalUrl":null,"permalink":"/tags/system-administration/","section":"Tags","summary":"","title":"System Administration","type":"tags"},{"content":"","date":"4 November 2025","externalUrl":null,"permalink":"/tags/devops/","section":"Tags","summary":"","title":"DevOps","type":"tags"},{"content":"","date":"4 November 2025","externalUrl":null,"permalink":"/tags/docker/","section":"Tags","summary":"","title":"Docker","type":"tags"},{"content":" Introduction to Docker # Getting Started with Docker on Arch Linux\nTo use Docker, you need to install and configure it on your system. If you\u0026rsquo;re not familiar with SSH or setting up your environment, check my previous blog.\nInstalling Docker # On Arch Linux, install Docker using pacman:\nsudo pacman -S docker Starting and Enabling Docker Service # After installation, start the Docker service:\nsudo systemctl start docker To enable Docker at system startup:\nsudo systemctl enable docker To verify that Docker is running:\nsudo systemctl status docker Creating a Docker Container # Pulling an Image from Docker Hub # Download the Ubuntu image:\nsudo docker pull ubuntu Creating a New Container # To create a new container:\nsudo docker create --name my-container ubuntu ‚ö† Note: If you encounter a timeout error, you may need to increase the HTTP timeout settings. Check the official documentation for additional troubleshooting.\nRunning a Container # To create and start a container named mysql-container using Ubuntu:\nsudo docker run --name mysql-container -it ubuntu Breakdown of the command:\n--name ‚Üí Specifies the container name\n-it ‚Üí Opens an interactive terminal\nubuntu ‚Üí The image we downloaded\nNow, you\u0026rsquo;re inside the container\u0026rsquo;s shell and can configure it as needed.\nManaging Docker Containers # Restarting and Accessing Containers # If you exit the container or restart your system, you can start and access your container again:\nsudo docker start mysql-container sudo docker exec -it mysql-container bash Checking Running Containers # To check the status, uptime, or container ID, use:\nsudo docker ps -a Checking Ports # To see which ports are listening inside your container, install net-tools:\npacman -S net-tools Note: Do not use sudo inside the container, as you already have elevated permissions.\nStopping and Removing Containers # Stop a Running Container: # sudo docker stop container_id_or_name Remove a Container: # sudo docker rm container_id_or_name Remove Multiple Containers at Once: # sudo docker rm container_id1 container_id2 Conclusion # Docker simplifies application deployment and management. Follow these steps to set up Docker on Arch Linux and start exploring its capabilities. üöÄ\nüìå Note: This guide is tailored for Arch Linux. If you‚Äôre using another distro, adjust the installation steps accordingly.\nüìå Additional Resources # üé• Watch this Docker guide on YouTube\n‚úç Haris\nüìå GitHub: https://github.com/Zenvila\nüìå LinkedIn: https://www.linkedin.com/in/haris-shahzad-7b8746291/\nüìå Member: COLAB (Research Lab)\n","date":"4 November 2025","externalUrl":null,"permalink":"/posts/introduction-to-docker/","section":"Posts","summary":"","title":"Introduction To Docker","type":"posts"},{"content":" Cryptography # Cryptography is the science of securing information by converting it into unreadable formats for unauthorized users while ensuring it can still be read by those with the correct decryption keys. Two significant components in modern cryptographic systems are OpenSSL and RSA, both widely used for securing data transmissions, especially over the internet.\nOpenSSL # OpenSSL is an open-source software library that provides tools for implementing secure communications. It supports various cryptographic functions and is essential in many secure web protocols, such as HTTPS. Here are some key features of OpenSSL:\nCryptographic Algorithms Support: OpenSSL supports a wide range of encryption algorithms, including AES, RSA, and SHA, making it versatile for different encryption needs.\nSSL and TLS Protocols: It provides robust support for SSL (Secure Sockets Layer) and TLS (Transport Layer Security) protocols, which are crucial for establishing secure connections over networks.\nCertificate Management: OpenSSL facilitates the generation, signing, and management of SSL certificates, which authenticate websites and encrypt communication between a client and a server.\nCommand-Line Utility: It offers powerful command-line tools for encryption, decryption, and certificate management, which makes it easy to use for both testing and practical deployment.\nOpen-Source and Extensible: Being open-source, OpenSSL is widely accessible and continually improved upon by the community, making it highly adaptable to emerging security standards.\nRSA (Rivest-Shamir-Adleman) # RSA is one of the earliest and most widely used public-key encryption algorithms. It operates on the principles of asymmetric encryption, which uses two keys‚Äîa public key for encryption and a private key for decryption. Key features of RSA include:\nAsymmetric Encryption: RSA\u0026rsquo;s core concept is based on public and private key pairs, enhancing security as only the private key can decrypt data encrypted with the corresponding public key.\nMathematical Foundation: RSA relies on the computational difficulty of factoring large prime numbers, which makes the encryption process highly secure against unauthorized decryption.\nKey Length and Security: RSA typically uses key lengths of 2048 bits or more, ensuring a high level of security. Longer key lengths make it exponentially more challenging for attackers to compromise the encryption.\nDigital Signatures: RSA is commonly used to generate digital signatures, allowing for verification of the authenticity and integrity of data, a critical feature in secure communications.\nWidely Supported: Due to its long-standing reputation and robustness, RSA is supported across various platforms and protocols, making it compatible with numerous applications in secure data transmission. By combining the strengths of OpenSSL and RSA, cryptographic systems can ensure secure, authenticated, and private communication in many digital environments.\nAll the practical aspects are available in the GitHub repository at¬†https://github.com/ZenTeknik/COLAB.git. You can check it there!\nP.S. # If you spot any mistakes, please don\u0026rsquo;t hesitate to point them out. We\u0026rsquo;re all here to learn together! üòä\nHaris\nFAST (NUCES)\nBS Computer Science | Class of 2027\nüìå GitHub: https://github.com/Zenvila\nüìå LinkedIn: https://www.linkedin.com/in/haris-shahzad-7b8746291/\nüìå Member: COLAB (Research Lab)\n","date":"31 October 2025","externalUrl":null,"permalink":"/posts/cryptography/","section":"Posts","summary":"","title":"Cryptography","type":"posts"},{"content":"","date":"31 October 2025","externalUrl":null,"permalink":"/tags/security/","section":"Tags","summary":"","title":"Security","type":"tags"},{"content":"","date":"1 October 2025","externalUrl":null,"permalink":"/tags/ai/ml/","section":"Tags","summary":"","title":"AI/ML","type":"tags"},{"content":" The Challenge # Analyzing thousands of drone log entries manually is time-consuming and error-prone. Traditional keyword searches miss contextual relationships between events.\nThe Solution # Built a semantic search system that converts log entries into vector embeddings, allowing analysts to query by meaning rather than exact keywords.\nKey Achievement # Reduced log analysis time from hours to seconds, enabling real-time insights into drone behavior patterns and mission-critical events.\nTechnologies Used # Python, sentence-transformers, NumPy, Pandas, Tkinter (GUI)\n","date":"1 October 2025","externalUrl":"","permalink":"/projects/dronelognlp-real-time-semantic-analysis-of-drone-logs/","section":"Projects","summary":"Transforms raw drone logs into semantic embeddings using Sentence Transformers for meaning-based search and analysis.","title":"DroneLogNLP: Real Time Semantic Analysis of Drone Logs","type":"projects"},{"content":"","date":"1 October 2025","externalUrl":null,"permalink":"/tags/nlp/","section":"Tags","summary":"","title":"NLP","type":"tags"},{"content":"I consistently seek out opportunities to learn and build through new projects. Although many of these personal initiatives don\u0026rsquo;t always reach a public release, they are instrumental in allowing me to experiment, grow, and apply new skills in practical scenarios.\n","date":"1 October 2025","externalUrl":null,"permalink":"/projects/","section":"Projects","summary":"","title":"Projects","type":"projects"},{"content":"","date":"1 October 2025","externalUrl":null,"permalink":"/tags/python/","section":"Tags","summary":"","title":"Python","type":"tags"},{"content":"","date":"1 October 2025","externalUrl":null,"permalink":"/tags/vector-databases/","section":"Tags","summary":"","title":"Vector Databases","type":"tags"},{"content":"","date":"25 September 2025","externalUrl":null,"permalink":"/tags/cloud/","section":"Tags","summary":"","title":"Cloud","type":"tags"},{"content":" Understanding CRIU # Let‚Äôs understand what CRIU actually is. To make it simple, think about story-based games. Have you ever noticed how games let us save our progress and later load the game from the same spot? That‚Äôs just like what CRIU does, but for real processes running on Linux.\nGames don‚Äôt use CRIU though ‚Äî they use more advanced systems like Steam Cloud. The game example is just to help you understand the basic idea.\nSo, what is CRIU? # CRIU (Checkpoint/Restore In Userspace) allows us to create a checkpoint of a running process and save it to the disk. Later, we can restore the same process from that saved point ‚Äî even on another system.\nCRIU is a utility mainly used on Linux systems. It works in the user space, which means it\u0026rsquo;s not part of the kernel, and that makes it more flexible.\nBeyond Games ‚Äì How It Works Technically # It‚Äôs not just for games. CRIU is powerful in operating systems for:\nProcess migration (move a process to another system)\nLive updates (update without stopping the app)\nRunning the same process on another machine even after reboot\nWhat is CRIU? # CRIU is a powerful tool that lets you freeze a running process and later restore it from disk. You can save its memory, open files, and more. You can even bring it back to life on another machine.\nCRIU = Checkpoint/Restore In Userspace # CRIU allows saving and restoring apps or containers to/from disk images.\nBreakdown of Features # Checkpoint/Restore: Take a snapshot of an app‚Äôs current state and restore it later.\nIn Userspace: Not part of the kernel, more adaptable to different Linux versions.\nKey Capabilities:\nüõ∞Ô∏è Live Migration\nüì∏ Snapshots\nüß™ Remote Debugging\nüê≥ Works with Docker \u0026amp; Podman\nHow Does CRIU Work? # CRIU can manage multi-threaded apps, open network sockets, and even containers. But it needs root access (using sudo, SUID bit, or Linux capabilities).\nWhy Was CRIU Developed? # Let‚Äôs look at the real-world problems it solves:\n| Problem | Explanation | | | | | ‚úÖ Long-running apps | Don‚Äôt restart apps every time you update or reboot. | | ‚úÖ Process Migration | Move apps between machines. | | ‚úÖ Live Upgrades | Update the system without shutting apps. | | ‚úÖ Fault Recovery | Crash? Restore from last checkpoint. | | ‚úÖ Cloud Scaling | Move Docker containers between data centers. |\nWhat Does CRIU Save? # When creating a checkpoint, CRIU saves:\nüß† Memory (RAM)\nüìÇ Open files and file descriptors\nüåê Network connections\nüå≤ Process tree\nüßµ Threads and signals\nüß¨ Kernel namespaces\n‚ö†Ô∏è Limitation: It doesn‚Äôt support VRAM-heavy apps like games using CUDA/OpenCL.\nWhere is CUDA Used? # CUDA is an NVIDIA software platform. Since it heavily relies on VRAM, CRIU isn\u0026rsquo;t suitable for GPU-heavy tasks.\nCRIU is best for CPU-based processes and Docker containers.\nBenefits of CRIU # ‚è∏Ô∏è Pause/resume long-running apps\nüöÄ Faster start (from saved state)\nüß≥ Migrate running apps\nüê≥ Supports Docker \u0026amp; containers\nüîÅ Less downtime during updates\nüõ†Ô∏è Debug or recover apps\nLet\u0026rsquo;s Try It ‚Äì A Simple CRIU Implementation # 1. Install CRIU on Arch Linux # sudo pacman -S criu ![](https://cdn.hashnode.com/res/hashnode/image/upload/v1749669219351/a8ec3448-e867-487e-90a7-715953c14bc2.png align=\u0026ldquo;center\u0026rdquo;)\nIf you get a \u0026ldquo;target not found\u0026rdquo; error, enable the [community] or [extra] repo.\n2. Verify Installation # criu check ![](https://cdn.hashnode.com/res/hashnode/image/upload/v1749720320836/677b9231-c32c-4278-99d7-74452429cdb7.png align=\u0026ldquo;center\u0026rdquo;)\n3. Create a Test Process # sleep 1000 \u0026amp; ![](https://cdn.hashnode.com/res/hashnode/image/upload/v1749720406087/47dff455-1c53-443d-9971-24b011b34c57.png align=\u0026ldquo;center\u0026rdquo;)\nGet the PID using:\nps aux | grep sleep ![](https://cdn.hashnode.com/res/hashnode/image/upload/v1749720431085/6850cd88-5dfd-4b3a-b356-aa831de0efee.png align=\u0026ldquo;center\u0026rdquo;)\n4. Checkpoint the Process # mkdir ~/criu-dump Then:\nsudo criu dump -t \u0026lt;PID\u0026gt; -D ~/criu-dump --shell-job --leave-running What these options mean: # -t \u0026lt;PID\u0026gt; ‚Üí Process ID to checkpoint\n-D ~/criu-dump ‚Üí Directory to save checkpoint files\n--shell-job ‚Üí Needed if it\u0026rsquo;s a shell-launched process\n--leave-running ‚Üí Keep the original process running\n5. Kill the Process # Simulate a crash:\nkill \u0026lt;PID\u0026gt; 6. Restore from Checkpoint\nsudo criu restore -D ~/criu-dump --shell-job 7. Check the Saved Files # cd ~/criu-dump ![](https://cdn.hashnode.com/res/hashnode/image/upload/v1749720468618/81b2a20d-0ec6-4bcf-990a-504e3b336e71.png align=\u0026ldquo;center\u0026rdquo;)\nYou‚Äôll see:\nimages/ ‚Üí Memory, open files, etc.\nstats-dump ‚Üí Process stats\ndump.log ‚Üí Log file (useful if it failed)\nIn the attached picture above, you can see some images representing the process states that we dumped into the directory. These images include everything related to the process, such as memory, file descriptors, and more. If we want to restore the process, we can do so using these images. Moreover, if we want to restore the process on another system, we can simply move these images to that system. This allows us to open the process in the same state, with the same memory and configuration it had when it was saved.\nConclusion : # CRIU is not just a cool Linux tool ‚Äî it‚Äôs a real-world solution for system admins, container managers, cloud engineers, and even developers. Whether you\u0026rsquo;re live migrating services or debugging frozen apps, CRIU gives you full control over running processes.\nP.S.\nIf you spot any mistakes, please don\u0026rsquo;t hesitate to point them out. We\u0026rsquo;re all here to learn together! üòä Haris\nFAST (NUCES)\nBS Computer Science | Class of 2027\nüìå Portfolio: zenvila.github.io\nüìå GitHub: github.com/Zenvila\nüìå LinkedIn: linkedin.com/in/haris-shahzad-7b8746291\nüìå Member: COLAB (Research Lab)\n","date":"25 September 2025","externalUrl":null,"permalink":"/posts/understanding-criu/","section":"Posts","summary":"","title":"Understanding Criu","type":"posts"},{"content":" Build Your First Docker Swarm Cluster # If you\u0026rsquo;re new to Docker and eager to dive into container orchestration, you\u0026rsquo;ve come to the right place. Before jumping into Docker Swarm, it‚Äôs essential to first understand what Docker is and how Docker Compose works. These foundational tools will help you grasp the power of Docker Swarm more effectively.\nüìå Recommended Reads First: # What is Docker? ‚Äì A beginner\u0026rsquo;s guide to containers.\nhttps://systemadmin-insights.hashnode.dev/introduction-to-docker\nSo, What is Docker Swarm? # In simple terms, Docker Swarm is a container orchestration tool. It allows you to manage and deploy containers across multiple machines (nodes), treating them as a single virtual Docker host.\nThink of Docker Swarm as a way to scale your application and maintain high availability by distributing containers across various systems.\nIt comes built-in with Docker and is incredibly useful when you want to:\nRun containers on multiple hosts\nAutomatically replicate your containers\nEnsure fault tolerance and high availability\nManage services more efficiently\nüîÑ What is Orchestration? # Orchestration refers to automating the deployment, management, and scaling of containers. When you‚Äôre managing dozens or hundreds of containers, orchestration helps you:\nAutomatically deploy containers\nBalance load across nodes\nRestart failed containers\nMaintain availability even during failures\nüéØ Key Features of Docker Swarm # ‚úÖ Automated Deployment\n‚úÖ Efficient Resource Allocation\n‚úÖ Load Balancing \u0026amp; Networking\n‚úÖ Service Discovery\n‚úÖ High Availability \u0026amp; Fault Tolerance\nLet‚Äôs say one container crashes ‚Äî Swarm detects it and spins up a replica on another available node, keeping your application alive. How it actually works:\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1744559040862/b6d35cf2-672b-40bb-a783-27dfa37678ed.png align=\u0026ldquo;center\u0026rdquo;)\nAs you have two working nodes‚Äîone as the manager node and the other as the worker node‚Äîhere is how they work. The worker node communicates with the manager node using HTTP.\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1744559207214/2c66a498-de64-4a05-8054-a3b3e4b99cc9.png align=\u0026ldquo;center\u0026rdquo;)\n‚öôÔ∏è Setting Up Docker Swarm # We‚Äôll set this up on two machines:\nüß† zenvilla ‚Äì Swarm Manager\n‚öíÔ∏è dawood ‚Äì Swarm Worker\nNote: This can be done using two physical machines or virtual machines on the same network.\nüîß Pre-requisites (Run on both machines)\n# Step 1: Install Docker sudo pacman -Syu docker # Step 2: Enable and Start Docker sudo systemctl enable docker sudo systemctl start docker # Step 3: Add user to docker group sudo usermod -aG docker $USER newgrp docker ![](https://cdn.hashnode.com/res/hashnode/image/upload/v1744559279318/ec015c71-3121-41e4-96ed-b0405fa3f56a.png align=\u0026ldquo;center\u0026rdquo;)\nüåê Ensure both systems are on the same network # Check connectivity:\n# On zenvilla: ping \u0026lt;dawood-IP\u0026gt; # On dawood: ping \u0026lt;zenvilla-IP\u0026gt; ![](https://cdn.hashnode.com/res/hashnode/image/upload/v1744559313492/fffd0f84-5348-4c9a-9af7-3e8b0bec2063.png align=\u0026ldquo;center\u0026rdquo;)\nAlso check the ping on Dawood\u0026rsquo;s laptop.\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1744559404591/0b4bf9ea-81d4-40b6-b307-f39554bc6797.jpeg align=\u0026ldquo;center\u0026rdquo;)\nüß† On ZENVILLA (Manager Node) # Step 5: Initialize the Swarm: # docker swarm init --advertise-addr \u0026lt;ZENVILLA-IP\u0026gt; ![](https://cdn.hashnode.com/res/hashnode/image/upload/v1744559451781/a4fa8d7b-b711-457f-ba6e-ef3b3f548f8b.png align=\u0026ldquo;center\u0026rdquo;)\nReplace \u0026lt;ZENVILLA-IP\u0026gt; with the actual IP of your machine, e.g. 192.168.1.10.\nThis command will output a token used to join other nodes to the swarm.\n‚öíÔ∏è On DAWOOD (Worker Node) # Step 6: Join the Swarm # Use the command you received from the manager:\ndocker swarm join --token \u0026lt;TOKEN\u0026gt; \u0026lt;MANAGER-IP\u0026gt;:2377 This command connects dawood to zenvilla as a worker node.\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1744559492340/85670b10-88fb-4ca7-aadd-7b5a8098c4c3.jpeg align=\u0026ldquo;center\u0026rdquo;)\n‚úÖ Back to ZENVILLA (Verify the Swarm) # Step 7: Check Node Status # docker node ls ![](https://cdn.hashnode.com/res/hashnode/image/upload/v1744559517593/5300d3a5-4247-413f-bdd6-c183efcd8b78.png align=\u0026ldquo;center\u0026rdquo;)\nüöÄ Deploying a Test Service # Let‚Äôs deploy a lightweight Alpine service to test our setup.\ndocker service create --name alpine-test \\ --replicas 3 \\ alpine ping 8.8.8.8 ![](https://cdn.hashnode.com/res/hashnode/image/upload/v1744559537731/fc06659c-1d4f-4d17-98d1-2f25655a613e.png align=\u0026ldquo;center\u0026rdquo;)\nCheck if the service is running:\ndocker service ls To see where containers are running:\ndocker service ps alpine-test ![](https://cdn.hashnode.com/res/hashnode/image/upload/v1744559569015/37e664d6-33e2-4252-adbc-94f1b6c5ac6f.png align=\u0026ldquo;center\u0026rdquo;)\nOn dawood, run:\ndocker ps ![](https://cdn.hashnode.com/res/hashnode/image/upload/v1744559620733/233859d9-3a4c-41f7-b973-0b87cb0b5955.jpeg align=\u0026ldquo;center\u0026rdquo;)\nYou should see some containers running from the alpine-test service.\n‚ú® Final Thoughts # Docker Swarm is a powerful yet beginner-friendly way to orchestrate containers. It allows you to:\nRun and manage containers across multiple systems\nAutomatically recover from failures\nScale your services seamlessly\nIf you\u0026rsquo;re comfortable with Docker and Docker Compose, exploring Swarm is your next step toward mastering container orchestration. If you liked this guide, feel free to reach out or drop comments for queries. Keep experimenting, and happy containerizing! üê≥‚öôÔ∏è\nP.S. # If you spot any mistakes, please don\u0026rsquo;t hesitate to point them out. We\u0026rsquo;re all here to learn together! üòä\nHaris\nFAST (NUCES)\nBS Computer Science | Class of 2027\nüìå GitHub: https://github.com/Zenvila\nüìå LinkedIn: https://www.linkedin.com/in/haris-shahzad-7b8746291/\nüìå Member: COLAB (Research Lab)\n","date":"6 September 2025","externalUrl":null,"permalink":"/posts/build-your-first-docker-swarm-cluster/","section":"Posts","summary":"","title":"Build Your First Docker Swarm Cluster","type":"posts"},{"content":" The Challenge # Manual penetration testing is slow and repetitive. Security teams need automated tools that can adapt and learn from previous test results.\nThe Solution # Developed an AI-driven scanner that generates fuzzing commands dynamically and uses NLP to interpret terminal outputs, running multiple test cases automatically.\nKey Achievement # Automated 80% of routine security testing, discovering vulnerabilities 5x faster than traditional manual methods.\nTechnologies Used # Python, AI/ML, NLP, NLU, Nuclei, Penetration Testing Tools\n","date":"1 August 2025","externalUrl":"","permalink":"/projects/ai-powered-web-vulnerability-scanner/","section":"Projects","summary":"AI-driven vulnerability scanner that automates penetration testing by generating fuzzing commands and using NLP to analyze terminal outputs.","title":"AI-Powered Web Vulnerability Scanner","type":"projects"},{"content":"","date":"1 August 2025","externalUrl":null,"permalink":"/tags/cybersecurity/","section":"Tags","summary":"","title":"Cybersecurity","type":"tags"},{"content":" Machine Learning and Its Tools # What is AI? # Artificial Intelligence (AI)\nDefinition: AI is a broad field of computer science that aims to create systems that can mimic human intelligence. It can include rule-based logic.\nWhat is Machine Learning? # ML is a subset of AI that enables machines to learn from data without being explicitly programmed for every task.\nGoal: Make predictions or decisions based on data by training algorithms.\nIt learns patterns from data, is always data-driven, and is more focused and technical.\nHow is it different from Data Mining? # Actually, data mining is when we extract some meaning from the data.\nWhen we extract data, we then give it to a human, and the human is responsible for whatever informed decisions are made.\nMachine learning is used as a small part ‚Äî a tool ‚Äî in this process.\nBut usually in machine learning, we are not making or extracting meaning for a person, but we are extracting meaning that the machine can understand. The machine extracts the meaning from the data and also tries to understand it in order to do some extra work on the same data.\nExample: How a Machine Extracts Meaning from an Image # Let‚Äôs try to understand how a machine extracts meaning from a static image.\nIt‚Äôs just pixels, and pixels just have colors (red, blue, green). But the machine finds some sort of combination and then tells us what it actually is.\nWhat Tools Are Used for Machine Learning? # Torch # It is an open-source tool which is built on Lua. It is a Python-like language. It is very powerful.\nTorch is extremely scalable. You can run it on a cluster or multiple GPUs. It is easily scalable with just a change of a couple of lines of code.\nIf you write code for GPU and want to use the same code for CPU, you only need to change a few lines of code ‚Äî not more than that ‚Äî and it will be successful.\nModern GPUs today use the Torch library. We can write GPU-accelerated code using the Torch library.\nTheano # Theano is basically an alternative model based on Python. It is symbolic computation.\nTheano is a Python library (like PyTorch or TensorFlow) that was made for:\nDeep learning using symbolic computation\nFirst building a \u0026ldquo;math graph\u0026rdquo; of your model\nThen optimizing and running it efficiently on CPU and GPU\nIn Theano, when you define a model or calculation:\nYou‚Äôre not doing it immediately\nYou‚Äôre just describing the steps (like writing a math formula)\nThen later, Theano:\nBuilds a graph of your operations (symbolic)\nOptimizes it\nRuns it efficiently on CPU or GPU Note: First understand ‚Äî Torch is dynamic, which means as we write the code, it runs the same way.\nOn the other hand, Theano is symbolic, meaning it builds computations first, then runs them.\nBut Theano is hard to debug and learn, and it is replaced by TensorFlow, JAX, and PyTorch. Now, let‚Äôs discuss this further:\nTheano is replaced by JAX (it is symbolic graph-based but more optimized than Theano)\nTensorFlow is both symbolic and dynamic\nTensorFlow 1.x is static graph\nTensorFlow 2.x is dynamic by default\nDynamic execution makes coding and debugging easier.\nStill, Python is slow for such tasks, so what does symbolic execution actually do?\nIt creates a backend C code, and then it runs that C code ‚Äî which makes Python code faster. It does all the backend computation internally.\nKeras # Keras is a high-level deep learning library in Python that lets you build and train neural networks easily.\nYou don‚Äôt need to write complicated code ‚Äî just a few lines to define layers, and Keras handles the rest.\nWhen you write code in Keras today, it‚Äôs really using TensorFlow under the hood.\nScikit-learn # Scikit-learn (or sklearn) is a Python library used for machine learning, especially traditional ML ‚Äî like decision trees, SVMs, clustering, and regression. It is very well documented.\nScikit-learn is the go-to tool for classical machine learning ‚Äî it‚Äôs fast, simple, and great for real-world datasets like CSV files, customer data, etc.\nWhere we need traditional ML (no GPU support), and for data types like CSV, tables, NumPy ‚Äî Scikit-learn is very easy to use.\nOn the other hand, PyTorch and TensorFlow are for deep learning ‚Äî where we need GPU support and work with tensor data types. These are more complex.\nMahout (Hadoop) # Apache Mahout is a machine learning library built to run on big data systems like Hadoop.\nMahout lets you run ML algorithms on huge datasets that can‚Äôt fit on a normal computer ‚Äî using Hadoop\u0026rsquo;s distributed power. Example:\nImagine you have data from 10 million customers.\nWith Mahout + Hadoop:\nIt splits the work across many machines\nAll machines work together\nYou get results fast, even for huge data\n| Component | Purpose | | | | | Hadoop | Handles big data \u0026amp; distributes it | | Mahout | Runs ML algorithms on that data | How does it run on many machines?\nSuppose we have very big data ‚Äî say, 5000 GB. If we want to do classification or apply some ML task on that data, it will take too much time.\nWhat Mahout does:\nBreaks the data into chunks (e.g., 1000 GB each)\nRuns it on 5 different servers\nEach server processes its chunk using ML algorithms\nResults are collected and merged from all machines\nWe get the final output fast ‚Äî even for big data.\nCaffe # Caffe (Convolutional Architecture for Fast Feature Embedding) is a deep learning framework made by the Berkeley Vision and Learning Center (BVLC), focused mainly on image-related tasks like classification, detection, and segmentation.\nIt is used for:\nImage classification\nObject detection\nRunning fast pre-trained CNNs\nEmbedded systems (older edge AI projects)\nCaffe is a deep learning framework made for fast image processing, especially CNNs. It‚Äôs fast and good for deployment, but less flexible ‚Äî that‚Äôs why today most people use PyTorch or TensorFlow instead.\nFinal Note # A library gives you tools ‚Äî you decide how to use them.\nA framework gives you structure ‚Äî it tells you how to build.\nP.S.\nIf you spot any mistakes, feel free to point them out ‚Äî we‚Äôre all here to learn together! üòä\nHaris\nFAST-NUCES\nBS Computer Science | Class of 2027\nüîó Portfolio: zenvila.github.io\nüîó GitHub: github.com/Zenvila\nüîó LinkedIn: linkedin.com/in/haris-shahzad-7b8746291**\nüî¨ Member: COLAB (Research Lab)**\n","date":"30 July 2025","externalUrl":null,"permalink":"/posts/machine-learning-and-its-tools/","section":"Posts","summary":"","title":"Machine Learning And Its Tools","type":"posts"},{"content":"","date":"30 July 2025","externalUrl":null,"permalink":"/tags/programming/","section":"Tags","summary":"","title":"Programming","type":"tags"},{"content":"","date":"15 July 2025","externalUrl":null,"permalink":"/tags/data-analysis/","section":"Tags","summary":"","title":"Data Analysis","type":"tags"},{"content":" The Challenge # Real estate investment decisions lack data-driven insights. Investors need predictive models to assess ROI and risk before committing capital.\nThe Solution # Created a platform that scrapes 90,000+ property listings and uses machine learning to predict ROI, price trends, and investment scores.\nKey Achievement # Helped identify high-value investment opportunities with 85% accuracy, combining location data, market trends, and external factors.\nTechnologies Used # Python, Web Scraping (Playwright), Machine Learning, Data Analysis, FastAPI\n","date":"15 July 2025","externalUrl":"","permalink":"/projects/smart-real-estate-investor/","section":"Projects","summary":"Data-driven real estate investment analysis platform that scrapes property data and implements predictive models for ROI and investment risk assessment.","title":"Smart Real Estate Investor","type":"projects"},{"content":"","date":"15 July 2025","externalUrl":null,"permalink":"/tags/web-scraping/","section":"Tags","summary":"","title":"Web Scraping","type":"tags"},{"content":"","date":"8 July 2025","externalUrl":null,"permalink":"/tags/architecture/","section":"Tags","summary":"","title":"Architecture","type":"tags"},{"content":" The Hidden Control Loop of Modern Software # Introduction # Modern systems‚Äîwhether they‚Äôre robots, ATMs, or AI assistants like ChatGPT‚Äîneed to stay responsive. Imagine pressing a button on an ATM and the whole machine freezing until cash dispenses. Or asking ChatGPT something, and it makes you wait in a queue behind 100 people.\nThis is where Asynchronous (async) programming comes in. Async isn‚Äôt just a coding trick; it‚Äôs a principle of software efficiency, comparable to control theory in engineering.\nSynchronous (Sync): Tasks run one after another, each must finish before the next starts.\nAsynchronous (Async): Tasks can overlap in time, allowing the CPU to handle other work while waiting.\nWhat is Async Programming? # In traditional (synchronous) code:\nTasks run one after another.\nIf one task waits (e.g., network call, disk read), the whole program waits.\nIn async programming:\nTasks don‚Äôt block each other.\nWhile one task waits, the CPU does other work.\nResult ‚Üí smooth multitasking without needing multiple CPUs/threads.\nAsync and Control Theory # Control theory ensures systems remain stable by reacting quickly to changes. Delayed feedback can make a physical system unstable.\nAsync plays the same role in computing:\nWithout async ‚Üí software stalls, delays cascade, users lose responsiveness.\nWith async ‚Üí systems remain stable, responsive, and efficient.\nExample:\nA robot running sync code ‚Üí can‚Äôt read sensors until motor stops. Unstable.\nA robot with async ‚Üí reads sensors + moves simultaneously. Stable.\nAsync in LLM Applications (AI) # One of the best examples today is Large Language Models (LLMs):\nSync LLM requests: Each user request waits until the previous one is fully served.\n‚Üí Slow, costly, unscalable.\nAsync LLM requests: While one request waits on GPU, others are scheduled.\n‚Üí Many users served at once.\nOther AI benefits:\nStreaming outputs ‚Üí get words as they‚Äôre generated.\nParallel pipelines in RAG ‚Üí data fetching, embedding, and querying overlap.\nCost efficiency ‚Üí GPUs never sit idle.\nWithout async, tools like ChatGPT would feel like waiting in a long bank queue.\nAsync in Broader Computer Science # Async bridges software and hardware:\nHardware (OS level) ‚Üí event loops (epoll, IOCP) notify when I/O is ready.\nSoftware (language level) ‚Üí async/await in Python, JS, Rust, Go.\nApplications:\nWeb servers (FastAPI, Node.js).\nRobotics (ROS2 event-driven sensors).\nSecurity (fuzzing multiple endpoints in parallel).\nAI (non-blocking training/inference pipelines).\nIt‚Äôs a unifying technique across operating systems, software frameworks, and applications.\nDemo: Async in Action # import asyncio import time # Simulate an LLM request (each takes 3 seconds) def sync_llm_request(name): time.sleep(3) print(f\u0026#34;{name} response ready\u0026#34;) async def async_llm_request(name): await asyncio.sleep(3) print(f\u0026#34;{name} response ready\u0026#34;) # Synchronous version def sync_main(): start = time.time() for i in range(1, 4): sync_llm_request(f\u0026#34;Request {i}\u0026#34;) end = time.time() print(f\u0026#34;Synchronous total time: {end - start:.2f} seconds\u0026#34;) # Asynchronous version async def async_main(): start = time.time() tasks = [async_llm_request(f\u0026#34;Request {i}\u0026#34;) for i in range(1, 4)] await asyncio.gather(*tasks) end = time.time() print(f\u0026#34;Asynchronous total time: {end - start:.2f} seconds\u0026#34;) # Run demo print(\u0026#34;\\n Sync Execution \u0026#34;) sync_main() print(\u0026#34;\\n Async Execution \u0026#34;) asyncio.run(async_main()) Result:\nSync ‚Üí 9 seconds.\nAsync ‚Üí 3 seconds.\nJust like in real AI servers ‚Üí async cuts waiting time dramatically.\nConclusion # Async programming isn‚Äôt just about speed. It‚Äôs about:\nResponsiveness: Systems feel alive.\nEfficiency: Hardware is fully utilized.\nScalability: Serve more users without more CPUs.\nJust as control theory keeps machines stable, async keeps digital systems efficient and responsive. From robots to LLMs, async is the silent engine of modern computing.\nAsync is not faster computing‚Äîit‚Äôs smarter computing.\nHaris\nFAST-NUCES\nBS Computer Science | Class of 2027\nüîó Portfolio: zenvila.github.io\nüîó GitHub: github.com/Zenvila\nüîó LinkedIn: linkedin.com/in/haris-shahzad-7b8746291**\nüî¨ Member: COLAB (Research Lab)**\n","date":"8 July 2025","externalUrl":null,"permalink":"/posts/the-hidden-control-loop-of-modern-software/","section":"Posts","summary":"","title":"The Hidden Control Loop Of Modern Software","type":"posts"},{"content":"","date":"1 July 2025","externalUrl":null,"permalink":"/tags/computer-vision/","section":"Tags","summary":"","title":"Computer Vision","type":"tags"},{"content":" The Challenge # Large language models require internet connectivity and expensive cloud APIs. Users need offline, efficient document Q\u0026amp;A systems.\nThe Solution # Built a fully Dockerized RAG system using quantized LLaMA 3.1 8B (INT4) that runs entirely offline, answering questions from PDF documents.\nKey Achievement # Achieved 70% reduction in model size while maintaining accuracy, enabling deployment on modest hardware without internet dependency.\nTechnologies Used # Python, OpenVINO, NNCF, Docker, RAG, Vector Databases, LLMs\n","date":"1 July 2025","externalUrl":"","permalink":"/projects/lightweight-pdf-based-rag-system-with-llama-31-8b/","section":"Projects","summary":"Fully Dockerized Retrieval-Augmented Generation system using LLaMA 3.1 8B with INT4 quantization for offline PDF question answering.","title":"Lightweight PDF-based RAG System with LLaMA 3.1 8B","type":"projects"},{"content":"","date":"1 July 2025","externalUrl":null,"permalink":"/tags/llms/","section":"Tags","summary":"","title":"LLMs","type":"tags"},{"content":"","date":"1 July 2025","externalUrl":null,"permalink":"/tags/rag/","section":"Tags","summary":"","title":"RAG","type":"tags"},{"content":" The Challenge # Understanding images through natural language requires expensive cloud APIs. Local deployment of vision-language models is complex.\nThe Solution # Deployed Salesforce/blip-vqa-base model on CPU with a Gradio interface, enabling real-time visual question answering without GPU requirements.\nKey Achievement # Enabled multimodal AI capabilities on standard hardware, opening possibilities for AR/VR applications in EdTech and MedTech.\nTechnologies Used # Transformers, PyTorch, Gradio, Computer Vision, NLP\n","date":"1 July 2025","externalUrl":"","permalink":"/projects/vid-llm-powered-visual-question-answering-system/","section":"Projects","summary":"Local Visual Question Answering system using Salesforce/blip-vqa-base model deployed on CPU with Gradio interface for real-time multimodal reasoning.","title":"ViD-LLM Powered Visual Question Answering System","type":"projects"},{"content":"","date":"28 June 2025","externalUrl":null,"permalink":"/tags/networking/","section":"Tags","summary":"","title":"Networking","type":"tags"},{"content":" System Monitoring # Back to home\nMonitoring Systems with Monit Framework\nIn system administration, a common principle is that a good system administrator doesn\u0026rsquo;t need to frequently configure the system. Instead, they set it up once and then focus on monitoring and maintaining it. Monitoring tools like Monit help ensure that systems remain operational by continuously checking their status and reporting any issues.\nFor example, imagine your website provides services with 90% uptime (90/100 minutes). In technical terms, this is referred to as SLA (Service Level Agreement). While you could manually check your website periodically to ensure it\u0026rsquo;s running, this approach is inefficient. Monitoring tools automate this process by checking the system\u0026rsquo;s status at regular intervals.\nThis guide explains how to monitor an Apache web server using the Monit framework on Arch Linux.\nInstalling Monit and Apache Web Server # Note: The following commands are based on Arch Linux. You may need to adapt them for your own Linux distribution.\nInstall the Apache web server:\nsudo pacman -S apache Install Monit:\nsudo pacman -S monit Configuring Monit # Open the Monit configuration file:\nsudo nano /etc/monit/monitrc Configure the file according to your requirements. For example, you can set up Monit to monitor the Apache service. Ensure you include settings such as the port (default: 2812) and authentication if required.\nCheck the syntax of the configuration file:\nsudo monit -t Starting and Enabling Monit # Enable the Monit service to start at boot:\nsudo systemctl enable monit Start the Monit service:\nsudo systemctl start monit If you make changes to the configuration, restart Monit:\nsudo systemctl restart monit Configuring Apache Web Server # Ensure Apache is installed:\nsudo pacman -S apache Enable Apache to start at boot:\nsudo systemctl enable httpd Start the Apache service:\nsudo systemctl start httpd Verifying Monit Service # Open the Monit defaults file:\nsudo nano /etc/default/monit Ensure Monit is configured to monitor the desired services.\n![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgNf1_issZRrxlQNTC6IYvDoFHeTdZk9Xp1mYoKLMhfxkTzfYZmSvF-irFFzxwfnyZ-CnF03Em9iRArr8m_ZUroqBmWKLZwVZNrvvxl6FLojdVAxkvhliSU52_kXFkMnDiLXq_AYggpZspDqB6Dg1claDiZkR4pgSGG8Gld7zdAgD6qSBcilss8LnO2UJQ/w590-h57/Screenshot_20250114_071924.png align=\u0026ldquo;left\u0026rdquo;)\nInstall a text-based web browser to access Monit from the terminal:\nsudo pacman -S links Checking Port Status # To ensure Monit is listening on the configured port, use the following command:\nnetstat -nutlp Use links to check the Monit web interface on port 2812 (or the port you configured):\nlinks http://localhost:2812 Conclusion of Monit # Monit is a lightweight and efficient tool for automating system monitoring and ensuring reliability. It simplifies the management of services like Apache, detects issues proactively, and reduces manual intervention. By using Monit, system administrators can achieve high uptime and focus on other critical tasks, making it an essential tool for maintaining system health. P.S. If you spot any mistakes, please don\u0026rsquo;t hesitate to point them out. We\u0026rsquo;re all here to learn together!\nHaris\nFAST (NUCES)\nBS Computer Science | Class of 2027\nGitHub: https://github.com/Zenvila\nLinkedIn: linkedin.com/in/haris-shahzad786\nMember: COLAB (Research Lab)\n","date":"28 June 2025","externalUrl":null,"permalink":"/posts/system-monitoring/","section":"Posts","summary":"","title":"System Monitoring","type":"posts"},{"content":" SSH(Secure Shell ) # Secure Remote Access with SSH # SSH, or Secure Shell, is a method for securely connecting to another computer over a network. It enables you to remotely access and control another computer as if you were physically present.\nHow SSH Works # Secure Connection: SSH establishes an encrypted link between your computer and the remote one, ensuring that your data remains private and secure.\nRemote Control: Once connected, you can execute commands on the remote computer, transfer files, and perform various tasks just like you would on your own computer.\nSetting Up SSH # On Windows # If you\u0026rsquo;re using Windows, install PuTTY for SSH access. PuTTY provides a terminal where you can enter SSH commands. For mobile devices, Termux offers a Linux environment.\nOn Linux # Note : am on arch linux do run commands according to your linux version . # For a more secure experience, use the Linux terminal:\nInstall Net Tools: Use your package manager to install net-tools.\ncomm : (sudo pacman -Syu net-tools)\n![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg1LjOBSP5y2kv9l06DPfIkWpVr4u_I6Gxg4hg5ztIwt3nJowCgc9_C1dV1RXKQRk_Yut2I0DWVZ3GPeYsUjQspdqNJqkKCJ_ZVcCnXmiCPwqqemOuqsUPmzxsvxtLF7LhatNxglqXNYj9zAdZNxVorGzwbKrMKCTvi6egP6qmnPAWuL_CTu5_1B1Io3J4/w544-h83/Screenshot_20250128_161548.png align=\u0026ldquo;left\u0026rdquo;)\nInstall OpenSSH Server: Use your package manager to install openssh-server. comm : (sudo pacman¬†-Syu¬†openssh-server)\n![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg5SmQ5BePTgL1MT3b2h1IYCqOko76HeR7xrp9woK-5PVHyZtjNBJxW3H3_te-AtIPRQxL74kPQngTut7ZwsjSvXQzybbhgQzUuYSysfmh-UKcLKuf1Ub9R28LN7klVvF_4ye5ooJE9UjBIJ4VOqcY5UUHBeDDolL8GdfjOWxutath3Tb13sDMOmMke-Kc/w542-h98/Screenshot_20250128_160249.png align=\u0026ldquo;left\u0026rdquo;)\nNow up the server in both systems (own system or remote system):\ncomm : sudo systemctl enable sshd¬†# to up the ssh server\ncomm : sudo systemctl status sshd¬†# to check¬†the status of the server\n![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiKWTbF6gYkqCiEKqW_L1mp_Udn1IfZCznXeJesXxt2mCdiaI5Y4P2GeWklzOH1GnlL6xV_MKO4m-h2wt3YDhiEsBD-Edx7w3gij5I6xDlwFtQVcalpfox6PA8rb1WkyT_NHZSFq5xTKGbOlRCiSXCLhJEm4S4e6PmjlcPwaEbaNrSfc8yY8wAVEZsZzg0/w554-h98/Screenshot_20250128_161014.png align=\u0026ldquo;left\u0026rdquo;)\nyou can also restart the service if you want :\ncomm : sudo systemctl restart sshd\n![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjmxYMUNjO6YE9giNGSF-4IPG8SPqpKCVLpULVg-fSODSDyquSxbciJM_83LLBefiw2funigPDv2G7ei27cO6kFGwaHZNsKSqS0z494vMl_yA-VqSdcRIt_fXwLfNT4vY-YY6opmeKAMe19omWGEsY06LzRJGz6h9xJuxsK6vQocVyb2YvmZZYifl8Y58o/w515-h82/Screenshot_20250128_161410.png align=\u0026ldquo;left\u0026rdquo;)\nCheck Remote Server IP: Use¬†comm : ifconfig to find the server\u0026rsquo;s IP address (look for inet followed by the IP address).\n![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjWYUEavug5gX7EblqmapxAZYh7EX6zYPzD7iHCJ-HbePQjJ1KadauRpOQH91bT6IC2j42mmhiDn8ND6Qaxpxsw0kcoG8YW6n5pC1KrErSZoYcYf0mV3CD09KhdmM9aHHWnBKATRO5UGQs-xKbVBH7RTWMKFZWIguSSAHwg5_yBIcsYyPfJStFQMQLrV54/w604-h68/Screenshot_20250128_161803.png align=\u0026ldquo;left\u0026rdquo;)\nNow you can connect to your remote machine securely without relying on external services.\nNow here am making connection with my own system : (known as lopback connection):\n**Do remember it is the password connection(for this type of ssh you need to know to password of remote system):\n**\nFirst you need to check the ip and make sure if you connecting two differnet system¬†so internet¬†should be same .\ncomm¬†: ifconfig\ncomm : ssh username@ip\n![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiEi2WMddbdDhTTtZC2L7vlh6ZUsY8TFgyILp7Q60541MGBP8orZtKMBJnTk5W7dCCI-RvJrKsvI4sF5CFBnLSTfcgM_VP6kL1okOmBaqtqECMot1HKtYF-UNG4EoW1ATnhkqs-oyj-p1Am4yg4U9orFuY-D5yDqGlhGawzsofNiubXopY8MFYMN2ldV1I/w580-h118/Screenshot_20250128_162336.png align=\u0026ldquo;left\u0026rdquo;)\nNow you can check you have made the connection successfully .\nIf you wannt to logout the connection¬†:\ncomm : exit\n![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj5FmKjpUHjiv1eXEdAjZ4hm7QK7Y0ASkgLM-lO8QJQwVO_L4eRxMbnl-aRC5NGwZt94NBhMXdusd82tKcPlA32UqMHu-_xmi-QBQ-47nzazodoen21U9T58aMFHS6EB5GWymkK4QIexCT_MYHtZTq2N1WKs8ywvWpbBOismLeTiQRiLBg8wsAnDkhIE20/w542-h90/Screenshot_20250128_162950.png align=\u0026ldquo;left\u0026rdquo;)\nPasswordless Login to Remote Machine # Enhance security by using SSH keys instead of passwords:\nGenerate SSH Key: Use ssh-keygen to generate an SSH key pair. Press Enter to skip passphrase.\nssh-keygen ![](https://cdn.hashnode.com/res/hashnode/image/upload/v1741349001864/b8147715-1429-46cb-a6f5-9bb6acbb0bd4.png align=\u0026ldquo;center\u0026rdquo;)\nCopy Key to Remote Machine: Use ssh-copy-id to copy the public key to the remote server. comm :\nssh-copy-id -i filename remoteip ![](https://cdn.hashnode.com/res/hashnode/image/upload/v1741349067067/c46d5b6f-e143-49ef-bd33-1a948bdbe1d9.png align=\u0026ldquo;center\u0026rdquo;)\nLogin Without Password: Use ssh -i to login to the remote machine without entering a password.\nssh -i filename remoteip ![](https://cdn.hashnode.com/res/hashnode/image/upload/v1741349138934/faae1b2c-4a6b-4401-a22e-261023a46c3e.png align=\u0026ldquo;center\u0026rdquo;)\nTerminate Remote Session: To disconnect, find the process ID and terminate it using sudo kill.\nps aux | grep sshd sudo kill process id You can use test experiments¬†on remote machine\nConclusion # Following these steps ensures a secure and efficient connection between your computers. Every mistake is a chance to learn and improve your SSH skills.\nAdditional Resources # For more SSH tips and tricks, consider watching tutorial videos and reading blogs. Check out this YouTube video (https://www.youtube.com/watch?v=kdiz66KZrBg\u0026amp;list=LL\u0026amp;index=9) for detailed guidance.\nP.S. # If you spot any mistakes, please don\u0026rsquo;t hesitate to point them out. We\u0026rsquo;re all here to learn together! üòä\nHaris\nFAST (NUCES)\nBS Computer Science | Class of 2027\nüìå GitHub: https://github.com/Zenvila\nüìå LinkedIn: https://www.linkedin.com/in/haris-shahzad-7b8746291/\nüìå Member: COLAB (Research Lab)\n","date":"26 June 2025","externalUrl":null,"permalink":"/posts/sshsecure-shell/","section":"Posts","summary":"","title":"Sshsecure Shell","type":"posts"},{"content":"","date":"1 June 2025","externalUrl":null,"permalink":"/tags/ansible/","section":"Tags","summary":"","title":"Ansible","type":"tags"},{"content":" The Challenge # Manual deployment processes are error-prone and slow down development cycles. Teams need automated testing and deployment.\nThe Solution # Implemented a complete CI/CD pipeline with GitHub Actions and Docker, automating testing and containerized deployment on every push.\nKey Achievement # Reduced deployment time from 30 minutes to 2 minutes, ensuring consistent and reliable releases with zero-downtime deployments.\nTechnologies Used # GitHub Actions, Docker, Jest, CI/CD, Ubuntu\n","date":"1 June 2025","externalUrl":"","permalink":"/projects/ci-cd-pipeline-with-github-actions-and-docker/","section":"Projects","summary":"Complete CI/CD pipeline using GitHub Actions, Docker, and Ubuntu runners with automated testing and containerized deployment.","title":"CI/CD Pipeline with GitHub Actions and Docker","type":"projects"},{"content":"","date":"1 June 2025","externalUrl":null,"permalink":"/tags/flask/","section":"Tags","summary":"","title":"Flask","type":"tags"},{"content":"","date":"1 June 2025","externalUrl":null,"permalink":"/tags/github-actions/","section":"Tags","summary":"","title":"GitHub Actions","type":"tags"},{"content":"","date":"1 June 2025","externalUrl":null,"permalink":"/tags/infrastructure/","section":"Tags","summary":"","title":"Infrastructure","type":"tags"},{"content":" The Challenge # Manual infrastructure provisioning and configuration is time-consuming and inconsistent across environments.\nThe Solution # Combined Terraform for infrastructure provisioning with Ansible for configuration management, creating a fully automated deployment pipeline.\nKey Achievement # Eliminated manual setup steps, reducing infrastructure deployment time from days to minutes with Infrastructure as Code (IaC) practices.\nTechnologies Used # Terraform, Ansible, Docker, Infrastructure as Code (IaC)\n","date":"1 June 2025","externalUrl":"","permalink":"/projects/infrastructure-automation-with-terraform-and-ansible/","section":"Projects","summary":"Provisioned Docker-based Ubuntu environment using Terraform and configured Apache web server with Ansible for end-to-end infrastructure automation.","title":"Infrastructure Automation with Terraform and Ansible","type":"projects"},{"content":"","date":"1 June 2025","externalUrl":null,"permalink":"/tags/kubernetes/","section":"Tags","summary":"","title":"Kubernetes","type":"tags"},{"content":"","date":"1 June 2025","externalUrl":null,"permalink":"/tags/mongodb/","section":"Tags","summary":"","title":"MongoDB","type":"tags"},{"content":" The Challenge # Testing Kubernetes workflows requires expensive cloud infrastructure or complex multi-machine setups.\nThe Solution # Set up a multi-node Kubernetes cluster using Kind on Arch Linux, simulating real-world DevOps workflows with Deployments, Services, Ingress, and Helm.\nKey Achievement # Created a production-like environment locally, enabling rapid testing of container orchestration and microservices architecture.\nTechnologies Used # Kubernetes, Helm Charts, Kind, Docker, Linux\n","date":"1 June 2025","externalUrl":"","permalink":"/projects/multi-node-kubernetes-cluster-using-kind/","section":"Projects","summary":"Set up and configured a multi-node Kubernetes cluster using Kind with Deployments, Services, Ingress, ConfigMaps, Secrets, and Helm charts.","title":"Multi-Node Kubernetes Cluster Using Kind","type":"projects"},{"content":"","date":"1 June 2025","externalUrl":null,"permalink":"/tags/node.js/","section":"Tags","summary":"","title":"Node.js","type":"tags"},{"content":"","date":"1 June 2025","externalUrl":null,"permalink":"/tags/react/","section":"Tags","summary":"","title":"React","type":"tags"},{"content":"","date":"1 June 2025","externalUrl":null,"permalink":"/tags/terraform/","section":"Tags","summary":"","title":"Terraform","type":"tags"},{"content":"","date":"1 June 2025","externalUrl":null,"permalink":"/tags/web-development/","section":"Tags","summary":"","title":"Web Development","type":"tags"},{"content":" The Challenge # Building portable, scalable chat applications requires complex setup and dependency management across different environments.\nThe Solution # Developed a lightweight Flask chat app with SQLite3, fully containerized with Docker for easy deployment and portability.\nKey Achievement # Created a production-ready chat system that can be deployed anywhere Docker runs, demonstrating microservices architecture principles.\nTechnologies Used # Flask, SQLite3, Docker, Python\n","date":"1 June 2025","externalUrl":"","permalink":"/projects/zenchat---fully-dockerized-flask-chat-application/","section":"Projects","summary":"Lightweight real-time chat application using Flask and SQLite3, fully containerized with Docker for portable deployment.","title":"ZenChat ‚Äì Fully Dockerized Flask Chat Application","type":"projects"},{"content":" The Challenge # Full-stack applications require complex environment setup and dependency management across frontend, backend, and database services.\nThe Solution # Built a MERN stack wallet tracker with Docker Compose, orchestrating React, Node.js, Express, MongoDB, and all services in isolated containers.\nKey Achievement # Achieved seamless multi-service orchestration, enabling developers to run the entire stack with a single command.\nTechnologies Used # React.js, Node.js, Express, MongoDB, Docker Compose\n","date":"1 June 2025","externalUrl":"","permalink":"/projects/zenwallet---dockerized-mern-stack-wallet-tracker/","section":"Projects","summary":"Full-stack wallet tracking application built with MERN stack and fully containerized with Docker Compose for multi-service orchestration.","title":"ZenWallet ‚Äì Dockerized MERN Stack Wallet Tracker","type":"projects"},{"content":"","date":"31 May 2025","externalUrl":null,"permalink":"/tags/git/","section":"Tags","summary":"","title":"Git","type":"tags"},{"content":" Git and GitHub # Introduction:¬†Embarking on the journey of learning Git and GitHub can be a challenging but essential step for any aspiring developer. In this blog, we\u0026rsquo;ll break down the fundamental concepts and provide a step-by-step guide to help you navigate through the process of pushing and pulling code.\nUnderstanding the Basics:\nGit: Git is a powerful version control system designed to track changes in your codebase. Its key features include:\nTracking the history of changes.\nFacilitating collaboration among developers.\nGitHub: GitHub, on the other hand, is a web-based platform that leverages Git for code management. It allows developers to store, collaborate, and manage their code repositories. Think of GitHub as a user-friendly interface built on top of Git.\nSetting Up Your Environment:\nInstall Git: Depending on your operating system (Windows or Linux), install Git to your local machine.\nDownload Visual Studio Code (VS Code): A widely used code editor that integrates seamlessly with Git.\nOpen VS Code Terminal: Access the terminal within VS Code to execute Git commands.\nConfiguring Git:\ngit --version git config --global user.name \u0026#34;your GitHub account name\u0026#34; git config --global user.email \u0026#34;your GitHub email account\u0026#34; Working with Repositories:\nClone a Repository:\nCopy the repository URL from GitHub.\nIn the terminal, execute: git clone [repository_url].\nMaking Changes:\nOpen the cloned repository in VS Code.\nWrite or modify your code.\nCommitting Changes:\ngit add . git commit -m \u0026#34;Write your simple message here\u0026#34; git push origin main Pulling Changes:\ngit pull origin main Creating a New Repository:\ngit init git checkout -b [branch_name] # Create a new branch git branch -d [branch_name] # Delete a branch Conclusion:¬†Mastering Git and GitHub is crucial for efficient collaboration and version control in software development. By following these steps and commands, you can confidently navigate the basics of Git and GitHub. Apologies for any errors, and feel free to explore further to enhance your understanding. Happy coding!\nHaris\nFAST (NUCES)\nBS Computer Science | Class of 2027\nGitHub: https://github.com/Zenvila\nLinkedIn: linkedin.com/in/haris-shahzad786\nMember: COLAB (Research Lab)\n","date":"31 May 2025","externalUrl":null,"permalink":"/posts/git-and-github/","section":"Posts","summary":"","title":"Git And Github","type":"posts"},{"content":" The Challenge # Students waste hours searching through PDFs for class schedules and exam timetables. Manual distribution is inefficient.\nThe Solution # Created a web-based system that instantly provides personalized timetables, exam schedules, and instructor details by simply entering a roll number.\nKey Achievement # Eliminated PDF searching entirely, saving students 2+ hours per week while providing admins with efficient record management.\nTechnologies Used # Python, SQLite, Web Development\n","date":"1 May 2025","externalUrl":"","permalink":"/projects/automated-timetable-management-system/","section":"Projects","summary":"Web-based system to eliminate manual PDF searching for class or exam timetables with personalized access and admin management panel.","title":"Automated Timetable Management System","type":"projects"},{"content":"","date":"1 May 2025","externalUrl":null,"permalink":"/tags/sqlite/","section":"Tags","summary":"","title":"SQLite","type":"tags"},{"content":" Running ARM (Raspberry Pi) on Your PC Without Hardware # Why This Matters # ARM processors power some of the most exciting technology of our time‚Äîfrom smartphones and IoT devices to robotics and Raspberry Pi projects. If you‚Äôre into embedded AI, robotics, or system development, you‚Äôll often need an ARM environment for testing and optimization.\nBut here‚Äôs the challenge: what if you don‚Äôt own a Raspberry Pi or ARM board? What if you want to simulate and test ARM-specific tools and software directly on your x86 PC?\nThat‚Äôs exactly what we‚Äôll do here: build a fully working ARM64 (aarch64) Debian environment inside an x86 PC running Arch Linux. It feels almost magical‚Äîyou‚Äôre essentially pretending your PC is a Raspberry Pi.\nThis is especially powerful for embedded AI development, where optimization meets intelligence. You can test how your AI models behave in constrained ARM environments before deploying them to real hardware.\nSetting Up the ARM Lab on Arch Linux # We‚Äôll use QEMU (Quick Emulator) with binfmt_misc support to run ARM binaries transparently on an x86 host.\nStep 1. Install the required packages # sudo pacman -S qemu-user-static debootstrap These packages give you the ability to emulate ARM instructions and bootstrap a Debian ARM root filesystem.\nStep 2. Verify binfmt support # Start systemd-binfmt (it manages interpreters for foreign architectures):\nsudo systemctl start systemd-binfmt Check registered emulators:\nls /proc/sys/fs/binfmt_misc/ You should see entries like qemu-aarch64 and qemu-arm, meaning your system is ready to run ARM binaries.\nStep 3. Create a working directory # mkdir ~/arm-lab cd ~/arm-lab This is where your ARM environment will live.\nStep 4. Bootstrap an ARM Debian root filesystem # Use debootstrap to install a minimal Debian system for ARM64 inside your directory:\nsudo debootstrap --arch=arm64 bookworm debian-arm64 http://deb.debian.org/debian This downloads and extracts a fresh Debian ARM rootfs into debian-arm64.\nStep 5. Enter the ARM environment # Now, use chroot to enter it:\nsudo chroot debian-arm64 /bin/bash Congratulations! You‚Äôre inside a Debian ARM64 environment, running on your x86 PC.\nPretend Your PC is a Raspberry Pi # Inside your ARM Debian shell, install neofetch:\napt update apt install neofetch -y neofetch It will proudly report aarch64 architecture‚Äîjust like a real Raspberry Pi.\nEven better, you can now install ARM64-only packages that won‚Äôt work on x86.\nTrick your friends:\n‚ÄúI‚Äôve got a Raspberry Pi running‚Ä¶ inside my Arch PC!‚Äù\nWhy This Matters for Embedded AI # When working on robotics, drones, or aerospace software, you often cannot run heavy x86 code. You need:\nCross-compilation skills (build on x86, deploy on ARM).\nTesting environments (validate on ARM before touching hardware).\nOptimization awareness (SIMD instructions, low memory usage).\nBy setting up this ARM lab, you gain practical knowledge that bridges system programming and embedded AI.\nYou don‚Äôt just learn theory‚Äîyou actually see how software behaves differently on ARM. This is exactly the mindset required to build intelligent systems where optimization meets intelligence.\nFinal Thoughts # With just a few commands, you‚Äôve unlocked the ability to simulate Raspberry Pi on your PC:\nNo hardware required.\nFull ARM environment.\nAbility to install ARM64-only packages.\nPerfect playground for embedded AI.\nSo next time you hear someone say, ‚ÄúI wish I had a Raspberry Pi for testing‚Äù, you‚Äôll know the answer:\nYou can run it directly on your PC.\nHaris\nFAST-NUCES\nBS Computer Science | Class of 2027\nüîó Portfolio: zenvila.github.io\nüîó GitHub: github.com/Zenvila\nüîó LinkedIn: linkedin.com/in/haris-shahzad-7b8746291\nüî¨ Member: COLAB (Research Lab)\n","date":"24 April 2025","externalUrl":null,"permalink":"/posts/running-arm-raspberry-pi-on-your-pc-without-hardware/","section":"Posts","summary":"","title":"Running Arm Raspberry Pi On Your Pc Without Hardware","type":"posts"},{"content":" The Building Blocks of Modern Computing # What is a Thread? # Before diving into the concept of threads, let‚Äôs consider a common example. Imagine a company named X with four employees. Each employee is assigned their own task, and they upload their completed work to a shared database. These tasks might range from creating an entire application to completing a small part of it. At the end of the day, all their contributions combine to form the final application. In this analogy, each employee represents a thread, while the company represents the process.\n![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgZEylfCI_oZbyb5d0fQzQVbQSxy6ILbktlqRJsPfOH1CNVvy6Y2MjLNwSP4qeOhH0ul3ThOYR7g_yrHsC06Kf6Nrzkf8YUMwEUE_8m4XhULTXZGL8Q8XH6ZPAcHhBBf932ikaxQiC-sAEKtJHfyccRHGdPG1h8Y87WgQ7Hkw_1QFEjkIjXod6TC_U-VlDc/w431-h243/Thread%20exp.png align=\u0026ldquo;left\u0026rdquo;)\nKey Terms in Threading # Single-threaded Process:\nIn a single-threaded process, one thread executes the entire sequence of operations sequentially.\nMulti-threaded Process:\nIn a multi-threaded process, multiple threads work together, with each thread handling a specific part of the sequence of operations. The combined effort of these threads results in the completion of the task.\nShared Resources in Threads # All threads within the same process share certain resources, such as code or heap memory. This concept is referred to as shared resources.\nProgram vs. Process # Program: A program is a set of instructions written to perform a task.\nProcess: A process is the actual execution of a program. Multiple processes can be associated with the same program.\n![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjcZ36rJJHjMCClndN6pw5WNQU15kK4EjAVvGLnIxeAay9eIScsBHwHTlUFlHOHEOheFSBE6x4Pz-YQ6BfNp2-UqPfsOqgr6v7zhKko9ppgO07bdqQ8MXHi6qYtATGXZY76zHLbeyMgj7wD8TIoiwv28Jsmwbhfofez84h3IBNJMII2LKstlEsAXIeLrEBc/w362-h299/Screenshot_20250105_171435.png align=\u0026ldquo;left\u0026rdquo;)\nUnderstanding Parallelism # You may have heard terms like quad-core or octa-core processors. These terms indicate the number of threads a processor can handle simultaneously:\nA quad-core processor can run four threads in parallel.\nAn octa-core processor can run eight threads in parallel.\nAlthough it seems like multiple threads are running at the same time, in reality, the processor activates only a few threads at any given moment. This creates the illusion of parallel execution.\nParallel vs. Concurrent Threads # Parallel Threads: These threads run simultaneously on separate CPU cores.\nConcurrent Threads: These threads share resources, such as memory or files. When one thread needs a shared resource that another thread is using, it must wait for the resource to become available. This waiting is referred to as concurrency.\n![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj60LlEF7yWplA5y8FzM1WtW5lrsU1z6IqVAuzPtnvuz7kUmcTJCUhjv7tp-7ECM7T8q0HBZQAVMyjmRt_YGu2dm-vPXKvu22gN7Q4G5NY8wIJpsuTw0zTIXZ1unbNM_ogagO1lwVykl-MVs0F6CI9Z5J-Dq8qTE7BGSgQPgzR400jO4YOVJRUR1UvwjFa7/w361-h374/Screenshot_20250105_171621.png align=\u0026ldquo;left\u0026rdquo;)\nRole of the Scheduler # To minimize the waiting time in concurrent threads, the operating system scheduler ensures efficient distribution of processing time among threads. The scheduler assigns priority levels and manages the execution of threads according to a set of predefined rules, thereby optimizing performance.\nLet\u0026rsquo;s move how to check¬†in manjaro linux:- # Check Total Threads (CPU Threads): # This shows the number of threads per core. # The total number of CPU threads available: # # Check active threads:\nUsing htop: Install htop if not already installed: # # Look at the CPU meters to see active threads.\n![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjJjRlC9WmmAo0Gt8CcXEsc5hXWKUwfwsCFxLLpn736XAynfxs-V3lYEONYQAwkWOZ2PtQlbyMYU7Qt1aN2NLmRtfyaHccVg80lUNXoloKkRXhM0jIHUSAJKeLc6OqVOdJueVvtab-ye12Ur4M03aKW0bYVQOgLBAFpUVxrjRl-d0Vb9vXRE4Jj97E1nFoV/w475-h347/Screenshot_20250105_165544.png align=\u0026ldquo;left\u0026rdquo;)\nTo see the load on each thread: # ![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgpd4yJgHdy9tH2gT-d0ymvzfWrnMOZHs1lqOH1qXIhRE15VvTlyjuW32RdcMc-EoMIJbEUPBNCPpSqUP4OXczU0PKGFWimnjFMGtJUTqySAbynk24p2_smg2eZiFfkK6bOEdsdbY9XFqM-d56iXH-NLRP0FS6Ko-Lc8VygnP5cOBkD1G86ZuourJ_uDIjQ/w406-h297/Screenshot_20250105_170134.png align=\u0026ldquo;left\u0026rdquo;)\nCounts the number of threads running across all processes:\n![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjH05yYwsd37zj6-1k1mG0VGUHs2U5fVVcdudLJ98F_kkrFXdIKcx7sbbYHaU9EszMDP82lTmm0nO1-eTJhQIrNoLy4VSWyc_Y3spnePN-fbFQriJr-43OIE0_6fmQ-P1FOxfM0IRHU5RNdYegLo0_JM_dRoCA70yMyCb7sBOCuLhz1C3-zxjaamChjtNnO/w694-h93/Screenshot_20250105_170315.png align=\u0026ldquo;left\u0026rdquo;)\nConclusion # A thread is the smallest sequence of instructions that can be managed independently by a scheduler.\nMulti-threading allows multiple threads within a single process to share resources such as memory, while processes do not share resources.\nThreads communicate through inter-thread communication, while processes use inter-process communication.\nThreads generally execute faster than processes because they share the same resources, reducing overhead.\nBy understanding the basics of threads and their management, you can better appreciate how modern computing achieves efficiency and speed.\nP.S. If you spot any mistakes, please don\u0026rsquo;t hesitate to point them out. We\u0026rsquo;re all here to learn together!\nHaris\nFAST (NUCES)\nBS Computer Science | Class of 2027\nGitHub: https://github.com/Zenvila\nLinkedIn: linkedin.com/in/haris-shahzad786\nMember: COLAB (Research Lab)\n","date":"11 April 2025","externalUrl":null,"permalink":"/posts/the-building-blocks-of-modern-computing/","section":"Posts","summary":"","title":"The Building Blocks Of Modern Computing","type":"posts"},{"content":" Ubuntu Server Cloud: Secure, Automated, and Monitored # Setting Up a Cloud on Ubuntu Server # In this guide, we will walk through the process of setting up a cloud environment on Ubuntu Server. This includes downloading the ISO image, partitioning storage, automating user creation, monitoring the system, and ensuring proper user authorization.\nStep 1: Downloading Ubuntu Server # To begin, download the latest LTS (Long-Term Support) version of Ubuntu Server from the official website:\nüîó Download Ubuntu Server \\[**[https://ubuntu.com/download/server#manual-install](https://ubuntu.com/download/server#manual-install)**\\]\nAfter downloading, create a bootable USB using Rufus (for Windows) or the dd command (for Linux/macOS). Once the bootable media is ready, proceed with the installation on your server machine.\nStep 2: Storage and Partitioning # During installation, you may need to configure storage and partitioning:\nUsing a Single Disk? The default partitioning should be sufficient.\nAdding an Extra Disk? You need to manually configure mounting.\nüìå Optional: Mounting an Extra Disk\nIf you\u0026rsquo;re adding an extra disk, follow these steps:\nIdentify available disks:\nlsblk Create a new partition and format it as ext4:\nsudo mkfs.ext4 /dev/sdb Mount the disk to a directory (e.g., /mnt/storage):\nsudo mkdir -p /mnt/storage sudo mount /dev/sdb /mnt/storage To make the mount persistent, add this entry to /etc/fstab:\n/dev/sdb /mnt/storage ext4 defaults 0 2 If you\u0026rsquo;re not using an extra disk, this step can be skipped.\nStep 3: User Creation and Automation # After storage configuration, the next step is to create **multiple users automatically** using an automation script. Instead of manually adding users, we will use a **pre-configured script** to streamline the process. üëâ [**Automation Script for User Creation**](https://github.com/Zenvila/ubuntu-cloud) **\\[**[https://github.com/Zenvila/ubuntu-cloud](https://github.com/Zenvila/ubuntu-cloud)**\\]** ### sudo pacman -S monit**What this script does:** ‚úî Creates multiple users with predefined settings. ‚úî Assigns appropriate permissions. ‚úî Sets up SSH access for secure remote login. Run the script and ensure all users are correctly created. Step 4: Server Monitoring with Monit # Now that the cloud setup is complete, **system monitoring** is crucial to ensure stability and prevent resource overuse. We will use **Monit** for real-time performance tracking and automated alerts. üîó [**Complete Guide to Monit**](https://systemadmin-insights.hashnode.dev/system-monitoring) **\\[**[https://systemadmin-insights.hashnode.dev/system-monitoring](https://systemadmin-insights.hashnode.dev/system-monitoring)**\\]** ### **Why Monit?** ‚úî Tracks CPU, RAM, and disk usage in real time. ‚úî Sends email alerts if resource usage exceeds limits. ‚úî Monitors processes and automatically restarts failed services. üìå **Monit on Ubuntu Server vs Arch Linux** * If you are using **Arch Linux**, install Monit with `pacman`: ```bash sudo pacman -S monit ``` If you are using **Ubuntu Server**, install Monit with `apt`: ```bash sudo apt install monit ``` ## **Step 5: Security \u0026amp; User Authorization** To ensure a **secure cloud environment**, we have implemented **strict user authorization** and access control: ‚úÖ Each user **cannot** access another user's data. ‚úÖ **Admins** have separate privileges for system management. ‚úÖ Proper group permissions are enforced to restrict unauthorized access. With **Monit configured**, if **CPU usage exceeds a specific threshold**, an **email alert** will be sent to the administrator‚Äôs mobile for immediate action. Conclusion # In this guide, we have: ‚úÖ Installed Ubuntu Server and configured storage. ‚úÖ Set up automated user creation for efficiency. ‚úÖ Implemented Monit for real-time monitoring and alerts. ‚úÖ Enforced security through user access control. With these steps, your **Ubuntu Server cloud** is now **fully optimized** for performance and security. üöÄ P.S. # If you spot any mistakes, please don't hesitate to point them out. We're all here to learn together! üòä **Haris** FAST (NUCES) BS Computer Science | Class of 2027 üìå **GitHub**: [https://github.com/Zenvila](https://github.com/Zenvila) üìå **LinkedIn**: [https://www.linkedin.com/in/haris-shahzad-7b8746291/](https://www.linkedin.com/in/haris-shahzad-7b8746291/) üìå **Member**: COLAB (Research Lab) ","date":"8 April 2025","externalUrl":null,"permalink":"/posts/ubuntu-server-cloud-secure-automated-and-monitored/","section":"Posts","summary":"","title":"Ubuntu Server Cloud Secure Automated And Monitored","type":"posts"},{"content":"","date":"1 April 2025","externalUrl":null,"permalink":"/tags/container-orchestration/","section":"Tags","summary":"","title":"Container Orchestration","type":"tags"},{"content":" The Challenge # Single-server deployments lack scalability and fault tolerance. Production systems need distributed container orchestration.\nThe Solution # Built a Docker Swarm cluster across two physical machines running Arch Linux, with manager and worker nodes for container orchestration.\nKey Achievement # Achieved high availability and load distribution, demonstrating enterprise-grade container deployment on bare metal infrastructure.\nTechnologies Used # Docker, Docker Compose, Linux, Container Orchestration\n","date":"1 April 2025","externalUrl":"","permalink":"/projects/docker-swarm-cluster-deployment-on-multi-node-architecture/","section":"Projects","summary":"Built Docker Swarm cluster using two physical machines running Arch Linux with manager and worker nodes for container orchestration.","title":"Docker Swarm Cluster Deployment on Multi-Node Architecture","type":"projects"},{"content":" Kubernetes (Multi-node + Real-World Workflows) # Kubernetes (Multi-node + Real-World Workflows) # What we are going to learn and implement in a multi-node cluster in Kubernetes:\nIn our previous implementation, we set up a single-node cluster using Minikube (Minikube is best suited for single-node clusters).\nCheck out the blog:\nNow, let‚Äôs go one step ahead and look at what we‚Äôre going to learn and implement:\n| Topic | What is it? | | | | | kubectl | CLI tool to control Kubernetes | | Deployment | A controller for managing replicas of your app | | Service | Exposes your pods over network | | Ingress | Exposes HTTP(S) traffic to services | | ConfigMap | Store non-sensitive config data | | Secret | Store sensitive data (e.g., passwords) | | Volumes | Store persistent data (e.g., DB files) | | Helm | Package manager for Kubernetes | | k3s / kind | Lightweight Kubernetes for multi-node setup |\nNote: We\u0026rsquo;re implementing this on Arch Linux. If you\u0026rsquo;re using Ubuntu or another distro, adjust accordingly.\nInstall Required Tools on Arch # Install kubectl # sudo pacman -S kubectl ![](https://cdn.hashnode.com/res/hashnode/image/upload/v1750383139671/c0652384-9ddc-4474-a18c-2f667f5e41c2.png align=\u0026ldquo;center\u0026rdquo;)\nkubectl is the CLI tool to interact with your Kubernetes cluster.\nInstall Docker (Required for kind) # Because Kubernetes creates pods and nodes on containers, Docker is essential.\nsudo pacman -S docker sudo systemctl start docker sudo systemctl enable docker sudo usermod -aG docker $USER ![](https://cdn.hashnode.com/res/hashnode/image/upload/v1750383150114/fac27a47-b04b-4eba-8f6b-d6fd077c6d72.png align=\u0026ldquo;center\u0026rdquo;)\nRe-login to apply group changes.\nInstall kind (Multi-node Kubernetes using Docker) # k3s or kind are lightweight options for setting up multi-node clusters.\ncurl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.20.0/kind-linux-amd64 chmod +x ./kind sudo mv ./kind /usr/local/bin/kind ![](https://cdn.hashnode.com/res/hashnode/image/upload/v1750383183430/f7d8d9ab-430d-40ab-b305-fa76b96d8259.png align=\u0026ldquo;center\u0026rdquo;)\nCreate a Multi-Node Cluster (with kind) # So far, we‚Äôve installed the necessary tools. Now we‚Äôll implement a multi-node cluster with multiple pods running inside.\nCreate a kind-config.yaml file:\nkind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 nodes: - role: control-plane - role: worker - role: worker Explanation:\nkind: defines the resource type as a Kubernetes cluster.\napiVersion: specifies the version of the kind API.\nnodes: lists the machines in the cluster.\nA control-plane node manages the cluster (scheduling, API, etc.).\nWorker nodes run actual application containers.\nThis setup creates a cluster with 1 control-plane and 2 workers.\nCreate the Cluster: # kind create cluster --config kind-config.yaml --name haris-cluster ![](https://cdn.hashnode.com/res/hashnode/image/upload/v1750383213838/f3791cec-ede4-4db8-b21c-d3d6436ae89d.png align=\u0026ldquo;center\u0026rdquo;)\nExpected Output:\nCreating cluster \u0026ldquo;haris-cluster\u0026rdquo;\u0026hellip;\n‚úÖ Ensuring node image (kindest/node:v1.27.3)\n‚úÖ Preparing nodes\n‚úÖ Writing configuration\n‚úÖ Starting control-plane\n‚úÖ Installing CNI (networking)\n‚úÖ Installing StorageClass\n‚úÖ Joining worker nodes\n‚úÖ Set kubectl context to \u0026ldquo;kind-haris-cluster\u0026rdquo;\nYou can now check the cluster:\nkubectl cluster-info --context kind-haris-cluster ![](https://cdn.hashnode.com/res/hashnode/image/upload/v1750383246328/68640732-404e-4f6a-b211-66070ae8965b.png align=\u0026ldquo;center\u0026rdquo;)\nTo check the created nodes:\nkubectl get nodes ![](https://cdn.hashnode.com/res/hashnode/image/upload/v1750383255634/85c164dc-3eb7-42c7-b5ee-e9db352db054.png align=\u0026ldquo;center\u0026rdquo;)\nYou should see:\nOne control-plane\nTwo worker nodes\nThe control node handles all services like scheduling, pod management, and DNS.\nThe worker nodes are responsible for running the actual services (apps).\nCore Kubernetes Components (With Examples) # Deployments # üí° What is it?\nManages app replicas. If a pod fails, Deployment restarts it.\nüõ† Example: NGINX Deployment\nkubectl create deployment nginx-deploy --image=nginx kubectl get deployments kubectl get pods ![](https://cdn.hashnode.com/res/hashnode/image/upload/v1750383316578/20f226cd-6fcf-4074-bc93-aa3e14de7bd4.png align=\u0026ldquo;center\u0026rdquo;)\nDeployment helps automate service availability. If some pods fail due to error, Kubernetes automatically restarts or replicates them. This is how real-world websites stay resilient.\nServices # üí° What is it?\nExposes your pod as a network service within the cluster. You specify a port for communication.\nüõ† Expose NGINX as ClusterIP:\nkubectl expose deployment nginx-deploy --port=80 --target-port=80 kubectl get services ![](https://cdn.hashnode.com/res/hashnode/image/upload/v1750383391554/2d8bdf25-c3d0-4f23-87a1-183852898036.png align=\u0026ldquo;center\u0026rdquo;)\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1750383413703/a4f19464-eadc-450a-ae66-bffe4b48b80b.png align=\u0026ldquo;center\u0026rdquo;)\nIngress # üí° What is it?\nIngress exposes services to external HTTP(S) traffic.\nWithout Ingress, you‚Äôd need NodePort or LoadBalancer for each service.\nIngress centralizes HTTP routing and SSL handling for all your apps.\nInstall Ingress Controller (Nginx): # kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.8.1/deploy/static/provider/kind/deploy.yaml ![](https://cdn.hashnode.com/res/hashnode/image/upload/v1750383438365/4136b238-81cc-4dd6-9da2-12ddae93438f.png align=\u0026ldquo;center\u0026rdquo;)\nWait for pods:\nkubectl get pods -n ingress-nginx ![](https://cdn.hashnode.com/res/hashnode/image/upload/v1750383448327/3b8e9ea5-628e-4777-974b-b6ceefbef104.png align=\u0026ldquo;center\u0026rdquo;)\nCreate ingress.yaml # apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: example-ingress spec: rules: - host: example.local http: paths: - path: / pathType: Prefix backend: service: name: nginx-deploy port: number: 80 This rule tells NGINX: \u0026ldquo;Route http://example.local to nginx-deploy service\u0026rdquo;\nApply it:\nkubectl apply -f ingress.yaml üìå If you get an error, delete the controller pod and it will auto-restart:\nkubectl delete pod -n ingress-nginx -l app.kubernetes.io/component=controller Watch in real time:\nkubectl get pods -n ingress-nginx -w ![](https://cdn.hashnode.com/res/hashnode/image/upload/v1750383477191/b47cd602-7063-472f-a98b-183f633f744a.png align=\u0026ldquo;center\u0026rdquo;)\nAdd to /etc/hosts:\nsudo vim /etc/hosts # Add: 127.0.0.1 example.local Then open: http://example.local\nSecrets # üí° What is it?\nSecrets store sensitive data like:\nPasswords\nAPI keys\nTLS/SSL certs\nkubectl create secret generic db-secret --from-literal=password=SuperSecret kubectl get secrets kubectl describe secret db-secret ![](https://cdn.hashnode.com/res/hashnode/image/upload/v1750383559288/a2b1f978-e3b3-4b82-9465-6c4a37a97800.png align=\u0026ldquo;center\u0026rdquo;)\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1750383579615/c89d855e-875b-4adc-b332-4cacac98436f.png align=\u0026ldquo;center\u0026rdquo;)\nSecrets are base64-encoded (not encrypted by default). Access them via environment variables or volume mounts.\nConfigMaps # üí° What is it?\nStores non-sensitive config like ENV values, URLs, etc.\nkubectl create configmap app-config --from-literal=APP_ENV=production kubectl get configmap kubectl describe configmap app-config ![](https://cdn.hashnode.com/res/hashnode/image/upload/v1750383603836/952a7084-ff1f-47de-b7a3-7f0e8f2f42aa.png align=\u0026ldquo;center\u0026rdquo;)\nAdvantages:\nKeep code separate from config\nSwitch between dev, staging, prod easily\nVolumes # üí° What is it?\nPersistent storage for data ‚Äî survives container restarts.\nExample: # apiVersion: v1 kind: Pod metadata: name: volume-pod spec: containers: - name: app image: nginx volumeMounts: - mountPath: /data name: host-storage volumes: - name: host-storage hostPath: path: /tmp/data type: DirectoryOrCreate ![](https://cdn.hashnode.com/res/hashnode/image/upload/v1750383691709/4ff4054a-cd00-4cfa-b7c3-07501ff0689d.png align=\u0026ldquo;center\u0026rdquo;)\nExplanation:\nvolumeMounts: Inside container, /data maps to your host‚Äôs /tmp/data\nhostPath: Uses your system path and creates it if not present\nUseful for storing logs, uploads, configs, etc.\nHelm (Kubernetes Package Manager) # Helm is like apt, pacman, or npm ‚Äî but for Kubernetes apps (called charts).\nInstall Helm:\ncurl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash ![](https://cdn.hashnode.com/res/hashnode/image/upload/v1750383664128/586d1421-ed40-45fe-973a-de8848823c30.png align=\u0026ldquo;center\u0026rdquo;)\nAdd Repo and Install Chart # helm repo add bitnami https://charts.bitnami.com/bitnami helm repo update helm search repo bitnami/nginx helm install my-nginx bitnami/nginx ![](https://cdn.hashnode.com/res/hashnode/image/upload/v1750383629716/9766c163-7760-424e-94fd-ed1224de2623.png align=\u0026ldquo;center\u0026rdquo;)\nWhat is Bitnami? # A trusted source of pre-packaged Helm charts:\nSecure\nUpdated regularly\nEasy to customize\nIt\u0026rsquo;s like the \u0026ldquo;official app store\u0026rdquo; for Kubernetes.\nUseful Commands: # kubectl get all helm list | Term | Meaning | | | | | Helm | Tool to manage Kubernetes apps | | Chart | Pre-built package of Kubernetes files | | Repo | Source of Helm charts | | Install | Deploy apps in one command |\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1750383743527/1f6f72aa-5669-41b6-8bff-f0779b29781a.png align=\u0026ldquo;center\u0026rdquo;)\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1750383765301/d181d153-cefa-4b0d-9928-a82a935f9b24.png align=\u0026ldquo;center\u0026rdquo;)\nConclusion # In this blog, we explored how to set up a multi-node Kubernetes cluster using kind on Arch Linux. From setting up deployments, services, ingress, config maps, and secrets, to working with persistent storage and Helm charts ‚Äî you‚Äôve now got a solid real-world workflow under your belt. üß†üöÄ\nKeep experimenting, and soon, Kubernetes will feel like second nature!\nP.S. # If you spot any mistakes, feel free to point them out ‚Äî we‚Äôre all here to learn together! üòä\nHaris\nFAST-NUCES\nBS Computer Science | Class of 2027\nüîó Portfolio: zenvila.github.io\nüîó GitHub: github.com/Zenvila\nüîó LinkedIn: linkedin.com/in/haris-shahzad-7b8746291\nüî¨ Member: COLAB (Research Lab)\n","date":"24 March 2025","externalUrl":null,"permalink":"/posts/kubernetes-multi-node-real-world-workflows/","section":"Posts","summary":"","title":"Kubernetes Multi Node Real World Workflows","type":"posts"},{"content":"","date":"15 March 2025","externalUrl":null,"permalink":"/tags/c/","section":"Tags","summary":"","title":"C","type":"tags"},{"content":"","date":"15 March 2025","externalUrl":null,"permalink":"/tags/linux-kernel/","section":"Tags","summary":"","title":"Linux Kernel","type":"tags"},{"content":" The Challenge # Kernel updates require system reboots, causing service downtime and disrupting critical operations.\nThe Solution # Implemented Kexec for seamless kernel switching and Live Kernel Patching for dynamic bug fixes, eliminating the need for full reboots.\nKey Achievement # Reduced boot times by 50% and achieved zero-downtime kernel updates, ensuring continuous service availability.\nTechnologies Used # Linux Kernel, C, Kexec, Live Kernel Patching, System Administration\n","date":"15 March 2025","externalUrl":"","permalink":"/projects/preserving-services-with-faster-kernel-reboots-using-kexec/","section":"Projects","summary":"Updated kernel without rebooting using Kexec for seamless kernel switching and Live Kernel Patching for dynamic bug fixes, reducing boot times by 50%.","title":"Preserving Services with Faster Kernel Reboots Using Kexec","type":"projects"},{"content":" Voice over Internet Protocol (VOIP) # Voice over Internet Protocol (VOIP) # VOIP (Voice over Internet Protocol) enables voice and multimedia communication over the internet instead of traditional phone lines. It\u0026rsquo;s widely used for cheaper long-distance calls, video conferencing, messaging, business communications, mobile apps, and IoT devices. Despite its benefits, VOIP relies on internet quality and faces security challenges like hacking.\nAdvantages of VOIP: # Cost Savings:\nVOIP typically offers lower costs for long-distance and international calls compared to traditional telephone services. Flexibility:\nUsers can make calls from any internet-connected device, enhancing mobility and remote work capabilities. Integration:\nVOIP easily integrates with other digital services and business applications, improving productivity and efficiency. Scalability:\nVOIP systems can scale easily to accommodate growing business needs or fluctuations in call volumes. Disadvantages of VOIP: # Dependence on Internet Quality:\nCall quality and reliability can be affected by internet connection stability and bandwidth availability. Security Concerns:\nVOIP calls may be susceptible to hacking, eavesdropping, and fraud without robust security measures in place. Emergency Calls:\nVOIP may have limitations in accurately identifying and routing emergency calls (911 in the US), compared to traditional landline services. Power Dependence:\nVOIP requires power and internet access to function, which may be a limitation during power outages or network disruptions. Setting Up VOIP with Asterisk on Linux # To set up VOIP using Asterisk on Linux, follow these steps:\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1741347124166/bad8d6fd-c1f2-4bc4-bda7-30431f525b36.png align=\u0026ldquo;center\u0026rdquo;)\nInstall Asterisk:\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1741347011770/6ee54cd6-20d9-44b4-9aa1-5c942ca4cc6d.png align=\u0026ldquo;center\u0026rdquo;)\nConfigure Asterisk Files: Navigate to the Asterisk configuration directory:\ncd /etc/asterisk\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1741347160320/231a3809-30fc-4f80-88a4-2b84114659ac.png align=\u0026ldquo;center\u0026rdquo;)\nBackup Configuration Files: Backup the default configuration files:\nEdit Configuration Files: Edit the configuration files and paste the contents from the provided GitHub repository:\nsudo nano sip.conf sudo nano voicemail.conf sudo nano extensions.conf GitHub Repository:\nhttps://github.com/Zenvila/Voice-over-IP Reload Asterisk: Enter the Asterisk command line interface and reload the configuration:\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1741347686515/38166a9c-0529-4567-9932-09e8bf29e7c4.png align=\u0026ldquo;center\u0026rdquo;)\nsudo asterisk -r reload ![](https://cdn.hashnode.com/res/hashnode/image/upload/v1741347739697/5b6a034a-05c2-4b50-9992-0c56ff64b3a0.png align=\u0026ldquo;center\u0026rdquo;)\nCheck Client Connections: Verify client connections in Asterisk:\nsip show peers ![](https://cdn.hashnode.com/res/hashnode/image/upload/v1741347764036/597052ae-88e2-45ea-acf0-52dc9d746899.png align=\u0026ldquo;center\u0026rdquo;)\nHere is the command where you can check ip:\nifconfig ![](https://cdn.hashnode.com/res/hashnode/image/upload/v1741347807279/9f7f8d57-e003-40a6-958e-a098f9643e9b.png align=\u0026ldquo;center\u0026rdquo;)\n![you have to choose the inet ip ](https://cdn.hashnode.com/res/hashnode/image/upload/v1741347826111/3239055b-5036-44e7-a1fb-d11a7e68dada.png align=\u0026ldquo;center\u0026rdquo;)\nThis command should display connected SIP clients.\nTesting: Download a SIP softphone like MizuDroid on your mobile device and configure it using the provided settings to make test calls. Conclusion # Setting up VOIP with Asterisk on Linux allows for flexible and cost-effective voice communication solutions. By following these steps and configuring Asterisk correctly, you can leverage the benefits of VOIP for your communication needs.\nIf you have any misconceptions or queries, please mention me. For additional resources, here is the video link:\nhttps://www.youtube.com/watch?v=rtHFdhCm434\nP.S. # If you spot any mistakes, please don\u0026rsquo;t hesitate to point them out. We\u0026rsquo;re all here to learn together! üòä\nHaris\nFAST (NUCES)\nBS Computer Science | Class of 2027\nüìå GitHub: https://github.com/Zenvila\nüìå LinkedIn: https://www.linkedin.com/in/haris-shahzad-7b8746291/\nüìå Member: COLAB (Research Lab)\n","date":"3 March 2025","externalUrl":null,"permalink":"/posts/voice-over-internet-protocol-voip/","section":"Posts","summary":"","title":"Voice Over Internet Protocol Voip","type":"posts"},{"content":" The Challenge # Managing personal finances manually is tedious and error-prone. Users need simple tools to track income and expenses.\nThe Solution # Developed a web application with secure authentication, transaction tracking, and graphical data visualization for financial management.\nKey Achievement # Enabled users to visualize spending patterns and make informed financial decisions through intuitive dashboards and analytics.\nTechnologies Used # Python, SQLite, Web Development, Data Visualization\n","date":"1 March 2025","externalUrl":"","permalink":"/projects/expense-tracker/","section":"Projects","summary":"Web application for managing income and expenses with secure login, transaction tracking, and graphical data visualization.","title":"Expense Tracker","type":"projects"},{"content":"","date":"15 February 2025","externalUrl":null,"permalink":"/tags/cloud-computing/","section":"Tags","summary":"","title":"Cloud Computing","type":"tags"},{"content":" The Challenge # Cloud services are expensive and lock users into vendor ecosystems. Organizations need self-hosted, scalable cloud infrastructure.\nThe Solution # Built a fully functional, self-hosted cloud environment on Ubuntu Server 22.04 LTS with Monit for real-time monitoring and alerts.\nKey Achievement # Created a production-ready cloud system for all COLAB members, reducing infrastructure costs by 80% compared to commercial cloud providers.\nTechnologies Used # Cloud Computing, Cloud Security, Ubuntu Server, Monit, Linux Administration\n","date":"15 February 2025","externalUrl":"","permalink":"/projects/custom-cloud-deployment/","section":"Projects","summary":"Built fully functional, self-hosted cloud environment powered by Ubuntu Server 22.04 LTS with Monit for real-time monitoring and alerts.","title":"Custom Cloud Deployment","type":"projects"},{"content":"","date":"15 February 2025","externalUrl":null,"permalink":"/tags/monitoring/","section":"Tags","summary":"","title":"Monitoring","type":"tags"},{"content":" Understanding NLP ‚Äî From Basics to DroneLogNLP # What is NLP (Natural Language Processing)? # Natural Language Processing (NLP) is a branch of Artificial Intelligence that helps computers understand, interpret, and generate human language.\nYou can think of NLP as the bridge between human communication (text/speech) and machine understanding (code/numbers).\nThe Core Idea # When humans talk or write, we use words.\nWhen computers ‚Äúthink,‚Äù they use numbers.\nSo NLP converts words ‚Üí numbers in a meaningful way so that algorithms can find patterns, relationships, and meanings.\nThe Building Blocks of NLP # 1. Text Preprocessing # Before a computer can understand text, it must be cleaned.\nThat includes:\nTokenization: Breaking text into smaller parts (words, phrases, or sentences).\n‚Üí \u0026quot;Drone launched at Sector A\u0026quot; ‚Üí [\u0026quot;Drone\u0026quot;, \u0026quot;launched\u0026quot;, \u0026quot;at\u0026quot;, \u0026quot;Sector\u0026quot;, \u0026quot;A\u0026quot;]\nLowercasing: Convert all words to lowercase.\nRemoving Stopwords: Words like ‚Äúthe‚Äù, ‚Äúis‚Äù, ‚Äúat‚Äù which don‚Äôt add much meaning.\nLemmatization/Stemming: Reducing words to their root form.\n‚Üí ‚ÄúFlying‚Äù, ‚Äúflies‚Äù ‚Üí ‚Äúfly‚Äù\n2. Feature Extraction # Once text is clean, we must turn it into numbers (vectors).\nCommon methods:\nBag of Words (BoW): Counts how many times each word appears.\nTF-IDF: Gives more importance to rare, informative words.\nWord Embeddings (like Word2Vec, GloVe): Represent words as dense vectors showing meaning and relationships.\n‚Üí e.g., vector(\u0026quot;king\u0026quot;) - vector(\u0026quot;man\u0026quot;) + vector(\u0026quot;woman\u0026quot;) ‚âà vector(\u0026quot;queen\u0026quot;)\n3. Sentence Embeddings # Words alone aren‚Äôt enough.\nSometimes, we need to represent entire sentences or paragraphs as single vectors.\nThat‚Äôs where Sentence Transformers come in.\nExample model:\nall-MiniLM-L6-v2 (used in your project).\nIt creates 384-dimensional embeddings for sentences that capture context and meaning.\nWhat Are Embeddings? # Think of embeddings as compressed meanings of words or sentences.\nEach word or sentence becomes a long list of numbers that describe its meaning.\nFor example:\n‚ÄúAttack initiated at base.‚Äù ‚Üí [0.23, -0.11, 0.09, ...]\n‚ÄúStrike started near base.‚Äù ‚Üí [0.21, -0.10, 0.08, ...]\nThese two vectors are close to each other ‚Äî meaning the model ‚Äúunderstands‚Äù they‚Äôre similar.\nYour Project: DroneLogNLP # Now let‚Äôs connect all that to what you built.\nYou made a system that can analyze drone operation logs using NLP and answer questions intelligently.\nStep-by-step Breakdown # 1. Synthetic Drone Logs # You had a dataset ‚Äî synthetic_drone_logs.csv\nExample entries:\nTimestamp, Summary 2025-10-01 08:00, Drone launched from base Alpha. 2025-10-01 08:05, Drone detected unusual heat signatures in Sector B2. 2025-10-01 08:10, Strike executed on target in Sector B2. 2. Text Embedding # You used SentenceTransformer to convert each summary into a vector.\nThis created a numerical map of meanings for every log entry.\nFile: embedding_model.py\nResult:\nsummary_embeddings.npy ‚Äî stores all embeddings.\n3. Query System # You allowed users to type natural queries like:\n‚ÄúShow me the strike details in Sector B2.‚Äù\nThe system:\nConverts the query to an embedding.\nFinds which drone log embedding is closest in meaning.\nReturns that log as the best answer.\nThis is semantic search, not keyword search.\nIt understands meaning, not just words.\n4. GUI Interface # You built a Tkinter-based GUI ‚Äî gui_tkinter.py\nSo instead of typing in the terminal, users get:\nA graphical window\nInput box for queries\nResult area for matched log\nBackground image for interface enhancement\nThis makes it user-friendly, especially for defense or control operators.\nReal-World Motivation # In defense systems or drone missions, huge log files are generated every hour.\nOperators have to manually scroll through logs to find critical details.\nFor example:\n‚ÄúWhen was the drone launched?‚Äù\n‚ÄúWhich sector had a strike?‚Äù\n‚ÄúWhat anomalies were detected?‚Äù\nYour project automates this process:\nAsk naturally, get instant results.\nIt saves time, improves accuracy, and assists in mission intelligence.\nTools \u0026amp; Technologies Used # | Tool | Purpose | | | | | Python | Core programming | | Sentence Transformers | Generate embeddings | | NumPy | Vector math and similarity calculation | | Tkinter | GUI development | | Pandas | Data handling and CSV processing | | Cosine Similarity | Measure semantic closeness | | Matplotlib (optional) | Visualization support |\nSkills You Demonstrated # Natural Language Processing (NLP)\nSemantic Search\nMachine Learning Fundamentals\nPython Development\nGUI Programming\nProblem-Solving for Defense Systems\nEnd-to-End Project Workflow\nUse Cases Beyond Defense # Cybersecurity Logs Analysis\nServer Logs Monitoring\nIncident Report Systems\nAutonomous Vehicle Event Tracking\nIndustrial Fault Diagnosis\nBasically ‚Äî anywhere logs are produced and humans need quick insight.\nFinal Thoughts # ‚ÄúAI doesn‚Äôt just make machines smarter ‚Äî it makes human decisions faster and more informed.‚Äù\nYour project, DroneLogNLP, shows how simple NLP ideas like embeddings and semantic search can turn raw data into something that responds intelligently.\nIt‚Äôs not just a student project ‚Äî it‚Äôs a prototype for future command-and-control intelligence systems.\n","date":"1 February 2025","externalUrl":null,"permalink":"/posts/understanding-nlp-from-basics-to-dronelognlp/","section":"Posts","summary":"","title":"Understanding Nlp From Basics To Dronelognlp","type":"posts"},{"content":" Step-by-Step Arch Linux Installation \u0026amp; Post-Setup # Complete Arch Linux Installation and Configuration Guide # Introduction # This guide provides a step-by-step process for installing Arch Linux and setting up essential configurations, including networking, user management, and software installation. This is a minimal setup guide without any graphical user interface (GUI) installation.\nStep 1: Boot Into Arch Linux Live Environment # Boot from the Arch Linux installation media and verify the internet connection using:\nping -c 3 archlinux.org If it fails, follow network setup steps below.\nStep 2: Set Up Network Connection # List available network interfaces:\nip link Connect to Wi-Fi using iwctl:\niwctl Inside iwctl, run the following commands:\ndevice list # List wireless devices station wlan0 scan # Scan for available networks station wlan0 get-networks # Show available networks station wlan0 connect \u0026#34;SSID\u0026#34; # Connect to a network (replace SSID with actual network name) Exit iwctl and verify the connection:\nping -c 3 archlinux.org For NetworkManager users:\npacman -S networkmanager --noconfirm systemctl enable --now NetworkManager Verify connection again:\nping -c 3 archlinux.org Step 3: Partition and Format the Disk # Check available disks:\nfdisk -l Partition the disk using:\ncfdisk /dev/sdX # Replace X with your drive letter Create partitions and format them:\nmkfs.ext4 /dev/sdX1 # Format root partition mkfs.vfat -F32 /dev/sdX2 # Format EFI partition (if using UEFI) Step 4: Mount and Install Arch Linux # mount /dev/sdX1 /mnt pacstrap /mnt base linux linux-firmware Generate fstab:\ngenfstab -U /mnt \u0026gt;\u0026gt; /mnt/etc/fstab Step 5: System Configuration # Chroot into the new system:\narch-chroot /mnt Set time zone:\nln -sf /usr/share/zoneinfo/Region/City /etc/localtime hwclock --systohc Enable localization:\necho \u0026#34;en_US.UTF-8 UTF-8\u0026#34; \u0026gt;\u0026gt; /etc/locale.gen locale-gen echo \u0026#34;LANG=en_US.UTF-8\u0026#34; \u0026gt; /etc/locale.conf Set hostname:\necho \u0026#34;archlinux\u0026#34; \u0026gt; /etc/hostname Step 6: Set Root Password and Create User # passwd Create a new user:\nuseradd -m -G wheel -s /bin/bash dawood passwd dawood Grant sudo privileges:\nEDITOR=nano visudo Uncomment:\n%dawood ALL=(ALL:ALL) ALL Step 7: Install and Configure Bootloader # For systemd-boot (UEFI):\nbootctl install echo \u0026#34;title Arch Linux\u0026#34; \u0026gt; /boot/loader/entries/arch.conf echo \u0026#34;linux /vmlinuz-linux\u0026#34; \u0026gt;\u0026gt; /boot/loader/entries/arch.conf echo \u0026#34;initrd /initramfs-linux.img\u0026#34; \u0026gt;\u0026gt; /boot/loader/entries/arch.conf echo \u0026#34;options root=/dev/sdX1 rw\u0026#34; \u0026gt;\u0026gt; /boot/loader/entries/arch.conf For GRUB (BIOS/UEFI):\npacman -S grub os-prober --noconfirm grub-install --target=x86_64-efi --efi-directory=/boot --bootloader-id=GRUB Generate GRUB config:\ngrub-mkconfig -o /boot/grub/grub.cfg Exit chroot and reboot:\nexit reboot Step 8: Post-Installation Configurations # Login as the new user:\nsu - dawood Enable networking:\nsystemctl enable --now NetworkManager Check internet:\nping -c 3 archlinux.org Step 9: Install Essential Packages # sudo pacman -Syu sudo pacman -S base-devel neofetch git wget curl tree xdg-user-dirs --noconfirm Set Neofetch to run on terminal start:\necho \u0026#34;neofetch\u0026#34; \u0026gt;\u0026gt; ~/.bashrc Step 10: Utilities Installation # sudo pacman -S os-prober xdg-user-dirs tree ranger thunar wezterm dmenu nitrogen bluez bluez-utils --noconfirm Install Yay for AUR package management:\ngit clone https://aur.archlinux.org/yay.git cd yay makepkg -si Install Google Chrome and Visual Studio Code:\nyay -S google-chrome visual-studio-code-bin --noconfirm Verify Yay installation:\nyay --version yay -Syu Step 11: Install and Configure SSH (Optional) # sudo pacman -S openssh --noconfirm sudo systemctl enable --now sshd Check SSH status:\nsystemctl status sshd Connect via SSH:\nssh dawood@your-ip-address Conclusion # With these steps, you\u0026rsquo;ve successfully installed Arch Linux, set up networking, created a user, installed X11, configured WezTerm, installed essential software, and enabled SSH for remote access.\nEnjoy your minimal and efficient Arch Linux setup! Haris\nFAST (NUCES)\nBS Computer Science | Class of 2027\nüìå GitHub: https://github.com/Zenvila\nüìå LinkedIn: https://www.linkedin.com/in/haris-shahzad-7b8746291/\nüìå Member: COLAB (Research Lab)\n","date":"13 January 2025","externalUrl":null,"permalink":"/posts/step-by-step-arch-linux-installation-and-post-setup/","section":"Posts","summary":"","title":"Step By Step Arch Linux Installation And Post Setup","type":"posts"},{"content":"","date":"10 January 2025","externalUrl":null,"permalink":"/tags/image-processing/","section":"Tags","summary":"","title":"Image Processing","type":"tags"},{"content":" Overview # A web-based simulation tool designed to bridge the gap between theoretical equations and visual understanding in Computer Vision. Unlike standard image editors, this dashboard breaks down algorithms into their mathematical stages, offering a \u0026ldquo;glass-box\u0026rdquo; view of how pixels are manipulated.\nKey Features # Step-by-Step Simulation # Visualizes the internal stages of algorithms (e.g., Canny Edge Detection: Gaussian Blur ‚Üí Sobel Gradient ‚Üí Hysteresis Thresholding).\nReal-Time Parameter Tuning # Interactive sliders to adjust Gaussian Sigma, Kernel sizes, and Threshold values with immediate visual feedback.\nAdvanced Processing # Includes modules for:\nHistogram Equalization/Matching Morphological Operations (Erosion/Dilation) SIFT/Harris Feature Detection Side-by-Side Comparison # Split-screen view to compare original vs. processed imagery for pixel-perfect analysis.\nTechnologies Used # Language: Python UI Framework: Streamlit Core Libraries: OpenCV (Computer Vision) NumPy (Matrix Operations) Matplotlib (Data Visualization) ","date":"10 January 2025","externalUrl":"","permalink":"/projects/interactive-dip-lab/","section":"Projects","summary":"A web-based simulation tool designed to bridge the gap between theoretical equations and visual understanding in Computer Vision, offering a ‚Äòglass-box‚Äô view of how pixels are manipulated.","title":"Interactive DIP Lab","type":"projects"},{"content":"","date":"10 January 2025","externalUrl":null,"permalink":"/tags/opencv/","section":"Tags","summary":"","title":"OpenCV","type":"tags"},{"content":"","date":"10 January 2025","externalUrl":null,"permalink":"/tags/streamlit/","section":"Tags","summary":"","title":"Streamlit","type":"tags"},{"content":"","date":"10 January 2025","externalUrl":null,"permalink":"/tags/web-application/","section":"Tags","summary":"","title":"Web Application","type":"tags"},{"content":"","date":"9 January 2025","externalUrl":null,"permalink":"/tags/asterisk/","section":"Tags","summary":"","title":"Asterisk","type":"tags"},{"content":"","date":"9 January 2025","externalUrl":null,"permalink":"/tags/freepbx/","section":"Tags","summary":"","title":"FreePBX","type":"tags"},{"content":"","date":"9 January 2025","externalUrl":null,"permalink":"/tags/twilio/","section":"Tags","summary":"","title":"Twilio","type":"tags"},{"content":"","date":"9 January 2025","externalUrl":null,"permalink":"/tags/voip/","section":"Tags","summary":"","title":"VoIP","type":"tags"},{"content":" The Challenge # Traditional telephony systems are expensive and rigid. I needed to build a communication system that was cost-effective, programmable, and capable of handling complex routing without physical hardware constraints. The goal was to eliminate traditional hardware landlines in favor of a flexible, cloud-integrated solution.\nThe Solution # Designed and deployed a scalable Voice over IP (VoIP) telecommunication system bridging local infrastructure with the global telephone network (PSTN). I engineered a hybrid solution using FreePBX (on top of Asterisk) as the internal Private Branch Exchange (PBX) and Twilio Elastic SIP as the SIP Trunking provider.\nKey Features # Twilio SIP Trunking:\nConfigured Twilio Elastic SIP Trunks to handle inbound/outbound calls, enabling global connectivity with low latency. PBX Management:\nDeployed FreePBX (on top of Asterisk) to manage internal extensions, call routing, and IVR (Interactive Voice Response) menus. Softphone Integration:\nConfigured endpoint connectivity for desktop and mobile clients (Zoiper/MicroSIP) allowing users to make secure calls from anywhere. Security Hardening:\nImplemented fail2ban and configured firewall rules (iptables) to prevent SIP brute-force attacks and unauthorized toll fraud. Technologies Used # Core: FreePBX, Asterisk Cloud Provider: Twilio (Elastic SIP) OS: Linux (CentOS/Debian) Protocols: SIP, RTP, TCP/UDP Softphones: Zoiper, MicroSIP Security: fail2ban, iptables, TLS, SRTP ","date":"9 January 2025","externalUrl":"","permalink":"/projects/voip-freepbx-twilio-vapi/","section":"Projects","summary":"Designed and deployed a scalable Voice over IP (VoIP) telecommunication system bridging local infrastructure with the global telephone network (PSTN).","title":"VoIP (FreePBX+Twilio+Vapi)","type":"projects"},{"content":" The Challenge # Traditional telephony systems are expensive and rigid. I needed to build a communication system that was cost-effective, programmable, and capable of handling complex routing without physical hardware constraints. The goal was to eliminate traditional hardware landlines in favor of a flexible, cloud-integrated solution.\nThe Solution # Designed and deployed a scalable Voice over IP (VoIP) telecommunication system bridging local infrastructure with the global telephone network (PSTN). I engineered a hybrid solution using FreePBX (on top of Asterisk) as the internal Private Branch Exchange (PBX) and Twilio Elastic SIP as the SIP Trunking provider.\nKey Features # Twilio SIP Trunking:\nConfigured Twilio Elastic SIP Trunks to handle inbound/outbound calls, enabling global connectivity with low latency. Configured the Termination and Origination URIs to route calls dynamically. Implemented IP Access Control Lists (ACLs) for security. PBX Management:\nDeployed FreePBX (on top of Asterisk) to manage internal extensions, call routing, and IVR (Interactive Voice Response) menus. Set up inbound routes to direct customer calls to specific departments via IVR (\u0026ldquo;Press 1 for Support\u0026rdquo;). Configured outbound routes with failover logic to ensure call reliability. Softphone Integration:\nConfigured endpoint connectivity for desktop and mobile clients (Zoiper/MicroSIP) allowing users to make secure calls from anywhere. Security Hardening:\nImplemented fail2ban and configured firewall rules (iptables) to prevent SIP brute-force attacks and unauthorized toll fraud. Optimized SIP packets (UDP/TCP) for low latency voice transmission. Secured the transmission using TLS and SRTP encryption to prevent eavesdropping. Technologies Used # Core: FreePBX, Asterisk Cloud Provider: Twilio (Elastic SIP) OS: Linux (CentOS/Debian) Protocols: SIP, RTP, TCP/UDP Softphones: Zoiper, MicroSIP Security: fail2ban, iptables, TLS, SRTP ","date":"8 January 2025","externalUrl":"","permalink":"/projects/voip-infrastructure/","section":"Projects","summary":"Designed and deployed a scalable Voice over IP (VoIP) telecommunication system bridging local infrastructure with the global telephone network (PSTN).","title":"Cloud-Integrated Enterprise VoIP Infrastructure","type":"projects"},{"content":"Welcome to my new digital garden! üöÄ\nI built this site using Hugo and the Blowfish theme. Stay tuned for updates about my projects in Automation and Robotics.\n","date":"1 January 2025","externalUrl":null,"permalink":"/posts/hello-world/","section":"Posts","summary":"","title":"Hello World","type":"posts"},{"content":" The Challenge # Server failures go unnoticed until users report issues. System administrators need proactive monitoring and automated error detection.\nThe Solution # Implemented Monit for comprehensive server monitoring with fine-tuned alert mechanisms, ensuring system stability and efficiency.\nKey Achievement # Achieved 99.9% system uptime with instant notifications, enabling rapid response to potential issues before they impact users.\nTechnologies Used # Linux System Administration, Monit\n","date":"1 January 2025","externalUrl":"","permalink":"/projects/monit-deployment/","section":"Projects","summary":"Implemented Monit for server monitoring and automation of error detection with fine-tuned alert mechanisms for system stability.","title":"Monit Deployment","type":"projects"},{"content":"","date":"1 January 2025","externalUrl":null,"permalink":"/tags/welcome/","section":"Tags","summary":"","title":"Welcome","type":"tags"},{"content":" Docker Evolution in the AI Era # From Simple Containers to Modern AI Workflows # Containers have revolutionized the way software is developed, deployed, and scaled. Among them, Docker has emerged as a cornerstone technology, bridging software and hardware seamlessly. In this blog, we‚Äôll explore Docker‚Äôs journey, how it‚Äôs evolving in the AI era, and how I experimented with it in a mini-project.\nWhat is Docker? # Docker is a platform that allows developers to package applications along with all their dependencies into a container. Containers are lightweight, isolated environments that ensure the app runs consistently across different machines. Think of it as a ‚Äúportable app box‚Äù that contains everything your program needs: code, libraries, and system tools.\nWhy it matters: # No more ‚Äúworks on my machine‚Äù issues.\nFaster deployment and testing.\nEasier scaling across servers and cloud infrastructure.\nHow Docker Has Changed # Originally, Docker was mostly used for web apps or backend services, where developers could pre-install libraries and deploy quickly. Today, with the explosion of AI and ML, Docker is evolving into a critical tool for AI workflows, including:\nML Model Deployment: Pre-trained models can be packaged into containers and deployed anywhere.\nReproducible Experiments: AI experiments often require complex libraries (TensorFlow, PyTorch, etc.), which Docker isolates perfectly.\nHardware Optimization: Containers now support GPU acceleration, making AI computations faster and more efficient.\nMicroservices \u0026amp; Scalability: Modern AI systems rely on multiple services (data ingestion, model inference, visualization), which Docker orchestrates efficiently.\nSoftware \u0026amp; Hardware Perspective # Software Perspective # Packages dependencies in a single environment.\nSupports Python, R, Java, and other languages used in AI.\nWorks with orchestration tools like Kubernetes to manage clusters.\nHardware Perspective # Efficiently shares system resources without the overhead of full virtual machines.\nSupports GPU, TPU, and other AI accelerators.\nEnables portable AI solutions that run on laptops, servers, or cloud GPUs.\nMini-Project: Testing Docker for AI # To explore this, I created a Docker container for a Python AI project:\nBuilt a Docker image with Python, PyTorch, and required libraries.\nTested a sample CNN model inside the container.\nVerified reproducibility by running the container on multiple systems.\nObserved faster deployment and zero dependency issues.\nThis experiment gave me practical insight into how Docker simplifies both software setup and hardware utilization for AI applications.\nWhy This Matters Today # In the modern AI era:\nContainers allow teams to focus on model innovation rather than environment issues.\nEnterprises can deploy AI at scale without worrying about system compatibility.\nDocker combined with AI is shaping next-gen architecture, where software and hardware are tightly integrated for performance.\nConclusion # Docker has grown from a simple container tool into a vital AI enabler, bridging software development, hardware optimization, and deployment. My mini-project experience reinforced how containers simplify testing, scaling, and reproducing AI workflows. In a world moving toward AI-driven solutions, Docker is not just a tool‚Äîit‚Äôs a foundation for innovation.\nHaris\nFAST-NUCES\nBS Computer Science | Class of 2027\nüîó Portfolio: zenvila.github.io\nüîó GitHub: github.com/Zenvila\nüîó LinkedIn: linkedin.com/in/haris-shahzad-7b8746291\nüî¨ Member: COLAB (Research Lab)\n","date":"10 December 2024","externalUrl":null,"permalink":"/posts/docker-evolution-in-the-ai-era/","section":"Posts","summary":"","title":"Docker Evolution In The Ai Era","type":"posts"},{"content":" The Challenge # Voice assistants lack visual understanding. Users need AI systems that can identify objects and answer questions about what they see.\nThe Solution # Integrated OpenAI API and Google API for real-time object recognition using webcam, enabling multimodal AI interactions.\nKey Achievement # Enabled natural language queries about visual content, demonstrating the potential of AI-powered augmented reality applications.\nTechnologies Used # Artificial Intelligence (AI), OpenAI API, Google API, Python, Computer Vision\n","date":"1 December 2024","externalUrl":"","permalink":"/projects/llm-voice-assistant/","section":"Projects","summary":"LLM-powered voice assistant with object recognition using webcam, integrating OpenAI API and Google API for real-time analysis.","title":"LLM Voice Assistant","type":"projects"},{"content":"","date":"1 December 2024","externalUrl":null,"permalink":"/tags/openai/","section":"Tags","summary":"","title":"OpenAI","type":"tags"},{"content":" Puppet ‚Äì Automating Infrastructure Configuration # What is Puppet? # Puppet is an open-source configuration management tool that automates the provisioning, configuration, and management of infrastructure. It ensures that your systems are configured consistently and correctly according to predefined policies.\nWhy is Puppet a Configuration Management Tool? # Automates Setup: Ensures systems are consistently configured.\nInfrastructure as Code: Uses a declarative language to define configurations.\nConsistency: Enforces policies across systems, reducing configuration drift.\nUse Cases # Provisioning: Automate setup of new servers.\nConfiguration: Maintain consistent system configurations.\nDeployment: Automate software installations and updates.\nCompliance: Ensure systems meet regulatory standards.\nDependencies # Puppet Agent: Installed on managed nodes.\nPuppet Master: Central server distributing configurations.\nModules/Manifests: Code defining resource states.\nSetting Up Puppet # To set up Puppet, you need a master server (Puppet Master) and one or more agent servers (Puppet Agents). Here‚Äôs a simplified guide:\nInitial Setup¬†# Update your repositories: # sudo apt update¬†sudo apt upgrade Install Docker:¬†sudo apt install docker.io Pull the Puppet image:\nsudo docker pull puppet Create the Puppet Master container:\nsudo docker run --name puppet-master -it puppet7 Start the Docker service:\nsudo systemctl start docker Run the Puppet Master container:\nsudo docker start puppet-master Check if the container is running:\nsudo docker ps -a Installing Puppet Server:¬†apt install puppetserver Enter the container and install necessary utilities:\napt install vim net-tools If you encounter issues with broken packages, run the following commands:\napt update \u0026amp;\u0026amp; sudo apt upgrade comm : apt \u0026ndash;fix-broken install\napt clean apt autoremove \u0026amp;\u0026amp; sudo apt-get autoremove apt-get install puppet-agent apt-get install puppet-module-puppetlabs-mailalias-core apt-get install puppetserver¬†else¬†Check the Java version:\njava -version¬†(Note : Adjust Java configuration if necessary to manage system memory usage)\nCheck Puppet service:\nnetstat -ntulp (If you see port 8140, it means Puppet is listening to all ports.)\nSet up the Certificate Authority:\n/opt/puppetlabs/bin/puppetserver ca setup **\nCheck system information:**\nfacter -p Find the Fully Qualified Domain Name (FQDN):\nfacter -p | grep fqdn¬†Create a .pp file for Puppet configurations:\npuppet apply filename.pp¬†Creating the Puppet Agent\nCreate the Puppet Agent container using the same image:\nsudo docker run --name puppet-agent01 -it puppet (Note : Start and run the Puppet Agent container as described earlier)\nInstall Puppet Agent in the container:\napt install puppet-agent¬†Establishing Communication¬†Update /etc/hosts with IP and FQDN:\nvi /etc/hosts¬†(Note: Add the IP and FQDN at the end of the file)¬†Run commands in the Puppet Agent container to establish communication with the master:¬†/opt/puppetlabs/bin/puppetserver ca setup¬†In the Puppet Master container, configure autosign in /etc/puppetlabs/puppet/puppet.conf:¬†autosign = true¬†In the Puppet Agent container, test the connection to the Puppet Master:¬†puppet agent --test puppet_masterfqdn Managing Puppet Policies for both(master \u0026amp; agent )\nIn the Puppet Master container, navigate to:\ncd /etc/puppetlabs/code/environments/production/manifests/¬†(Note : In this directory create this file )\nCreate init.pp and site.pp files to define policies and configurations.\nIn the Puppet Agent container, run the following command to apply policies from the master:\nTo run again¬†you have to run this command again and again :¬†puppet agent --test puppet_masterfqdn P.S.\nIf you spot any mistakes, please don\u0026rsquo;t hesitate to point them out. We\u0026rsquo;re all here to learn together! üòä\nHaris\nFAST (NUCES)\nBS Computer Science | Class of 2027\nüìå GitHub: https://github.com/Zenvila\nüìå LinkedIn: https://www.linkedin.com/in/haris-shahzad-7b8746291/\nüìå Member: COLAB (Research Lab)\n","date":"17 November 2024","externalUrl":null,"permalink":"/posts/puppet-automating-infrastructure-configuration/","section":"Posts","summary":"","title":"Puppet Automating Infrastructure Configuration","type":"posts"},{"content":" Probability and Statistics for AI # Math for AI: Probability and Statistics with a Computer Science Perspective (Using Python)\nIntroduction # After exploring Linear Algebra for AI, the next logical step in building a strong foundation for Artificial Intelligence is understanding Probability and Statistics. These two branches of mathematics are essential for reasoning under uncertainty, which is at the heart of most AI applications.\nIn real-world problem solving, we often start with qualitative ideas, but when we express these in numbers ‚Äî quantitative data ‚Äî we begin to apply statistics. In short, statistics is the science of making decisions based on data.\nWhy Probability \u0026amp; Statistics Matter in Computer Science # In pure math, we usually deal with small-scale values (e.g., 10‚Äì20 values), which is manageable. But in computer science, especially in AI, we work with thousands to millions of data points. This creates scalability challenges, making statistics and probability crucial for:\nData Science (large-scale statistics)\nMachine Learning (glorified probability)\nAlgorithm analysis, cryptography, and system modeling\nThey help us model and analyze uncertainty, a core concern in most computing problems.\nGetting Started with Python for Stats \u0026amp; Probability # We‚Äôll use Python as it\u0026rsquo;s general-purpose and well-supported for scientific computing.\nWe‚Äôll also use a virtual environment (via Conda) to isolate packages and ensure compatibility across systems. This environment can be easily ported to another machine, similar to a virtual machine.\nBefore setting up the environment, it\u0026rsquo;s helpful to explore an open-source GitHub repository that provides foundational code and material:\ngit clone https://github.com/recluze/stats-prob-cs This repository includes course notebooks and examples for deeper understanding. Browsing the code will help you get comfortable with the basics.\nNote: I\u0026rsquo;m using Arch Linux. Some commands may vary slightly on other distros. For best results, use Linux or macOS over Windows.\nInstall Python: # sudo pacman -Syu python Set up a virtual environment: # wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh bash Miniconda3-latest-Linux-x86_64.sh conda create -n stat-env python=3.11 conda activate stat-env Install required libraries: # conda install jupyterlab pandas matplotlib jupyterlab: Web-based IDE for notebooks.\npandas: For structured data manipulation.\nmatplotlib: For plotting graphs and visualizations.\nUnderstanding Data Types # In this course, we deal with two types of data:\nArtificial Data ‚Äì Collected through surveys (manual input).\nOrganic Data ‚Äì Generated by systems and logs (e.g., website user activity).\nOrganic data is especially relevant in computing as it reflects real-world system behavior.\nIn machine learning, data must often be i.i.d. (independent and identically distributed) for modeling.\nOne-Hot Encoding # When comparing categorical variables (e.g., male vs. female), one-hot vectors are used to represent them numerically without implying order or magnitude:\nExample:\n[male, female] = [1, 0], [0, 1] This avoids incorrect assumptions like \u0026ldquo;greater = male\u0026rdquo; and \u0026ldquo;smaller = female\u0026rdquo; which may happen if scalar values are used.\nIf we represent unordered categorical data as scalars instead of arrays, we distort the structure and meaning of that data.\nFoundations of Probability # Probability is axiomatic ‚Äî based on agreed-upon rules that help us model uncertainty. It‚Äôs not about proving what\u0026rsquo;s true, but about agreeing on how to handle unknowns.\nTopics and Their Computer Science, Hardware \u0026amp; Software Perspectives # Probability and statistics touch almost every aspect of computing. Below is a brief overview of key topics in this domain, described from three perspectives: computer science use cases, hardware impact, and software support.\nThese concepts help build intelligent systems that make decisions, process uncertainty, and handle large-scale data.\n| Topic | Description | Computer Science Perspective | Hardware Perspective | Software Perspective | | | | | | | | Data Types | Different forms of data: structured, unstructured | Organize and clean datasets | Efficient memory access | pandas, NumPy | | One-Hot Vectors | Binary representation for categories | Preprocessing for ML models | Stored efficiently in RAM | Used in embeddings, TensorFlow | | Histograms \u0026amp; Visualizations | Graphical summary of data distribution | Detect patterns and anomalies | GPU acceleration for big data | matplotlib, seaborn | | Central Tendency | Mean, Median, Mode | Summarize large datasets | Requires optimized storage | numpy.mean(), pandas functions | | Variance \u0026amp; Standard Deviation | Measure of data spread | Feature scaling and normalization | Computed using parallel float ops | scikit-learn, NumPy | | Entropy \u0026amp; Information | Measure of uncertainty | Decision trees, NLP models | GPU-based entropy calculation | sklearn, scipy.stats | | Probability \u0026amp; Events | Likelihood of an event | Foundation of model prediction | RNG hardware/simulation | Python‚Äôs random, numpy.random | | Conditional Probability | Probability given a condition | Bayesian models, filters | Efficient table lookups | scikit-learn, PyMC3 | | Bayes‚Äô Rule | Update belief with evidence | Spam filters, medical diagnosis | Uses logarithmic transformations | Naive Bayes classifiers | | Random Variables | Variables with probabilistic outcomes | Modeling random processes | Supported by dedicated chips | NumPy, PyMC3, TensorFlow | | Distributions (PMF/PDF) | Probability models for data | Classification, noise estimation | SIMD instructions for speed | scipy.stats, torch.distributions | | Joint \u0026amp; Marginal Probability | Modeling multivariate scenarios | NLP, image classification | Parallel computation across cores | pandas, statsmodels | | Expected Values | Long-run average outcomes | Policy evaluation in RL | Summation using vector units | NumPy, torch.mean | | KL Divergence | Measure of distribution difference | Regularization, GANs | Log-sum-exp ops on GPU | PyTorch, TensorFlow | | Bayesian Inference | Probabilistic belief update | Uncertainty-aware learning | Monte Carlo simulations | PyMC3, NumPyro, Edward |\nSummary # Probability and statistics are crucial for making decisions under uncertainty, which is common in all computing systems.\nThey form the mathematical backbone of machine learning and data science.\nFrom a hardware view, modern CPUs/GPUs accelerate statistical computation.\nOn the software side, Python libraries like NumPy, Pandas, SciPy, and PyTorch simplify implementation.\nWhether you\u0026rsquo;re analyzing logs, training models, or building systems like Jetson Nano for robotics ‚Äî understanding probability and statistics is essential to designing intelligent, efficient, and scalable solutions.\n**P.S:**if you spot any mistakes, feel free to point them out ‚Äî we‚Äôre all here to learn together! üòä\nHaris\nFAST-NUCES\nBS Computer Science | Class of 2027\nüîó Portfolio: zenvila.github.io\nüîó GitHub: github.com/Zenvila\nüîó LinkedIn: linkedin.com/in/haris-shahzad-7b8746291**\nüî¨ Member: COLAB (Research Lab)**\n","date":"25 October 2024","externalUrl":null,"permalink":"/posts/probability-and-statistics-for-ai/","section":"Posts","summary":"","title":"Probability And Statistics For Ai","type":"posts"},{"content":" HTTP/1.1 vs. HTTP/2 vs. HTTP/3 # A Comprehensive Comparison¬†Web protocols have evolved significantly over the years, and the three most commonly used versions today are HTTP/1.1, HTTP/2, and HTTP/3. These protocols govern how data is transmitted over the web, and each version brings improvements to speed, security, and reliability.\n![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhxarRMLmLBJuFRI5dvZM2zwU4f7FViSV2R0kLLHf_oJ5nDjxNDhUM9KlUNZiyBpVi0kMevZ6PpyRe0usjz5Su6u4lJaf1AyzRcEb9FGKh_7_ntifAd9oAyGO3_jUvDvEfFQNjW5EjabVdL5CqGWVXduOouGDYRzJ4wrsWcqMKXmK0jt2uHgGbahRqYjxw/w504-h188/Untitled.png align=\u0026ldquo;left\u0026rdquo;)\nHTTP/1.1: The Classic Protocol\nTransport Protocol: TCP (Transmission Control Protocol)\nKey Features:\nPersistent Connections: HTTP/1.1 introduced persistent connections, allowing multiple requests to be sent over a single TCP connection. However, it still opens new connections for each request-response pair.\nPipelining: Multiple requests can be sent before receiving responses, but the responses must be processed in the order they were sent.\nText-Based: HTTP/1.1 uses a text-based format, which is human-readable but less efficient for machine parsing.\nHead-of-Line Blocking: HTTP/1.1 suffers from head-of-line blocking, where one delayed or lost packet can block the entire stream of requests.\nFirst, you need to install curl to test these protocols: HTTP/1.1, HTTP/2, and HTTP/3.\n![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjFLWI6T17l3WB_HEc2FxQ93zlX4A1pMRyvWS8OLNisPsPRZxWmABIEr_ENKPRJ_F5cGBSCvZhDlmpWGZeRRDLYTQoT_RdE4BMeV6rbKWl2CIFDZB5Z6oRZyCvXkcxWP4rfYIi2JYHkJg8OAw9iqXPr0WcTQfIucb629_bifrthXqkhiKuxOEIyJ_SDmOA/w355-h211/Screenshot_20250126_154818.png align=\u0026ldquo;left\u0026rdquo;)\nTesting HTTP/1.1 on GitHub (Arch-based Linux)\nResults: Here are the testing results for HTTP/1.1.\n![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjxM8lcuv4KBIviwjsroYI-qeMF3jwOtJYiAm1qqWCDc2Uh2NmsKVco2vz0IxYBV7p1ZIj-gFA4j8C_9aqtl1t8GDD63prrbKfWQClUzHgDPkHq9udBRV7boBwwErhMI6gBjMGQCSmqUh4X03b9fOWSPfQPLEPRUFq4DAVUUs2YaV90U0SB0iPQSdyzrv0/w663-h220/Screenshot_20250126_154557.png align=\u0026ldquo;left\u0026rdquo;)\nHTTP/2: Major Improvements\nTransport Protocol: TCP (Transmission Control Protocol)\nKey Features:\nMultiplexing: Multiple requests and responses can be sent over a single connection, reducing latency and eliminating head-of-line blocking.\nHeader Compression: HTTP/2 uses HPACK for header compression, reducing overhead and speeding up data transfer.\nStream Prioritization: Allows clients to specify the priority of streams (requests) so that more important resources can be sent first.\nServer Push: Servers can proactively send resources to the client before they are requested, improving performance for critical resources.\nBinary Protocol: HTTP/2 is binary rather than text-based, which makes it more efficient for parsing and reduces errors.\nTesting HTTP/2 on GitHub (Arch-based Linux)\nResults: Here are the testing results for HTTP/2.\n![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhoOAoXxKNELts54-wcqdwLObxYJyShkcNeeyHp_kEdnFumhQ718YVcLrChGgZsl4dFpA4ZKARcIp8sV930sDE_L6zMeeg8uHmj-8vkQ3X8uyUlCUbdjyBSBDztOK64xv2GuLb4OpLs9Y6BSAvJ7Km8-TQnMIZcoR9sa1-WOTzQ6CcmkrttZaVZfN2BPjE/w465-h153/Screenshot_20250126_154947.png align=\u0026ldquo;left\u0026rdquo;)\nHTTP/3: The New Standard\nTransport Protocol: QUIC (Quick UDP Internet Connections), based on UDP (User Datagram Protocol)\nKey Features:\nReduced Latency: QUIC reduces connection setup time and allows faster data transfer, especially in mobile and high-latency networks.\nMultiplexing with No Head-of-Line Blocking: Unlike TCP, UDP (used in QUIC) doesn‚Äôt block the entire connection when a packet is lost. This significantly improves performance, especially in real-time applications like video streaming and gaming.\nBuilt-in Encryption: HTTP/3 integrates TLS (Transport Layer Security) directly into the protocol, enhancing security without additional handshake delays.\nConnection Migration: QUIC allows connections to migrate between networks (e.g., from Wi-Fi to cellular) without interrupting the connection, providing a smoother user experience on mobile devices.\nTesting HTTP/3 on GitHub (Arch-based Linux)\nResults: Here are the testing results for HTTP/3.\n![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh6CrOavze5wPUm-POL3AsgC_PHwRsbsYGccdG2Vru_jxgKlsru6JNCJ3CyKdBq2c3dXjy-kmPSPpwVz1UMw65vbxG0IICv5VA42Twyfi4b3SXxZYnA5O2eF6YCfiDBxTP6x3CkZNjS9hcO8QXyJ5AffmgI8DYp5KalHXBvMhFZzk8rtUXoQkIUtIu0Nd4/w557-h184/Screenshot_20250126_155045.png align=\u0026ldquo;left\u0026rdquo;)\nPractical Comparison\nHTTP/1.1: Although HTTP/1.1 is still widely used, it is becoming outdated due to its inefficiency in handling modern web traffic. The protocol suffers from significant delays, especially with multiple resources being requested.\nHTTP/2: HTTP/2 improves web performance significantly, especially for websites with many resources (images, scripts, etc.). Its multiplexing and header compression reduce latency, and it is more widely supported than HTTP/3. However, it still relies on TCP, which can be less efficient in some high-latency scenarios.\nHTTP/3: HTTP/3 is the future of web protocols, offering the lowest latency and best performance, especially for mobile users and in environments with high packet loss. However, it is still in the process of adoption and requires both server and client support. As seen in the case of GitHub, the transition to HTTP/3 is ongoing, and it may take time before it becomes the default for many services.\nDo check the screenshots for HTTP/2 and HTTP/3. While I tested HTTP/3, it does not support it.\nWhy GitHub is Still Using HTTP/2 (TCP) Over HTTP/3 (UDP)\nAfter checking the headers for GitHub\u0026rsquo;s website, it is clear that GitHub is still using HTTP/2 over TCP and not HTTP/3, which uses UDP. This choice is likely due to several factors:\nStability: TCP is more reliable and widely supported. It guarantees data integrity and error correction, making it a safer choice for critical services like GitHub.\nCompatibility: Not all browsers, clients, and networks fully support HTTP/3. GitHub prioritizes ensuring compatibility across a wide range of devices and network environments.\nGradual Rollout of HTTP/3: Although HTTP/3 promises significant performance improvements, its adoption is still in progress. GitHub, along with many other major websites, is taking a gradual approach to adopting the new protocol to avoid potential issues.\nAdditionally, remember that HTTPS is more secure than HTTP because HTTPS uses an SSL/TLS certificate to encrypt the communication between the client and the server. This encryption ensures that data transferred over HTTPS is protected from interception or tampering. In contrast, HTTP does not use encryption, making it more vulnerable to security risks.\n![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhckwQe2PH0ir4Iuq7v9ZAiPB-oELwJMR6FhnlOZSLcVtIWY6Ym91y-lPAGoeuggYSyOmVGaezyn2bvhjls7OgB4cy47aie_MQFY6ySE3S0lNOslJeSCNkEImC1gEH_G5Fw68jBrmILaOOHhAdVakzv4SK6APlHItqbxPQp2gCHs0wLmsJn3bpJ-EkR64A/w411-h149/images.png align=\u0026ldquo;left\u0026rdquo;)\nHere are the testing results on Gtihub server:\n![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjk-d7c-FdnK36kFc5nC1s-HP0pntdirnpZ1626YLHgtzwOPwCO_SphCJN4Gc9mMPinMRPfXmuRsP0Ag9b1ErGulHWuQlINfeQAp0SFFO7PHQ41pEO_ajsoTFJVaMu9YbS7EO_8SdeLDOKD90E4-iD6rZCXMkUoFDwcSIJpgjGSyRO144t20rDpx-B1yj0/w482-h92/Screenshot_20250126_160114.png align=\u0026ldquo;left\u0026rdquo;)\nAs you can see :\nWhen you made the curl request with http://github.com/ZenTeknik, GitHub automatically redirected the request to https://github.com/ZenTeknik. This is because GitHub (and many modern websites) enforces HTTPS to ensure secure communication between the server and clients. HTTP is considered insecure, and they prefer to direct users to the secure HTTPS version.\nThe 301 Moved Permanently response code indicates that the resource has permanently moved to a new URL (the HTTPS version).\nConclusion\nThe evolution from HTTP/1.1 to HTTP/2 and now to HTTP/3 shows a significant improvement in web performance and security. While HTTP/1.1 was the standard for many years, HTTP/2 brought major improvements in handling multiple requests efficiently. HTTP/3, with its use of QUIC and UDP, offers even lower latency and faster performance, but it is still in the process of widespread adoption.\nFor now, HTTP/2 over TCP remains the default for many services, including GitHub, due to its stability and compatibility. As HTTP/3 continues to mature, it will likely become the standard for most modern websites in the near future.\nHaris\nFAST (NUCES)\nBS Computer Science | Class of 2027\nGitHub: https://github.com/Zenvila\nLinkedIn: linkedin.com/in/haris-shahzad786\nMember: COLAB (Research Lab)\n","date":"24 October 2024","externalUrl":null,"permalink":"/posts/http11-vs-http2-vs-http3/","section":"Posts","summary":"","title":"Http11 Vs Http2 Vs Http3","type":"posts"},{"content":" Numerical Computing for AI # Introduction # Numerical computing in computer science involves using computers to perform calculations on real numbers, which are often approximated due to the limitations of computer representation. This field is crucial for various scientific and engineering applications where analytical solutions are difficult or impossible to obtain.\nIn this course, we mainly focus on what numerical computing actually is and why it\u0026rsquo;s called \u0026ldquo;numerical computing.\u0026rdquo;\nAnalytical vs. Numerical Mathematics # The math that we have done so far (like calculus and multivariable calculus) is analytical math. In a sense, we use numeric values and insert them into equations to simplify and get a precise solution. Analytical math is good when we have limited variables or objects and need a simplified solution.\nTake Newton\u0026rsquo;s Second Law:\nF = ma\nThis works well with two variables. But if we add a third object and try to calculate a radius or interaction, it becomes difficult to solve analytically. That‚Äôs where numerical methods come in. These methods use approximations to solve problems that are too complex for traditional analytical solutions. These approximations are very close to the actual answers.\nSo why do we need approximations? Because analytical methods are only feasible for limited objects. For large-scale problems, mathematicians use numerical methods, and when these methods are implemented via computers, it\u0026rsquo;s called numerical computation.\nScalars and Vectors # To begin understanding numerical computing, we start with the concepts of scalars and vectors.\nScalars: A single value, with no direction. In machine learning, if we consider an equation like area = length * width, the result (area) is a scalar.\nVectors: A collection of scalars, often having direction. In machine learning, even if we are using length and width together, they are treated as a vector.\nMatrices # A matrix is not just rows and columns; it acts as a transformer of vectors.\nWhen a matrix is multiplied by a vector, the result is a new vector that represents a transformation ‚Äî involving direction, magnitude, or both. Entire fields like ML and computer vision are built upon these transformations.\nLinearity # What is linearity? Suppose we have two parallel lines. If, after applying a matrix transformation, the lines remain parallel and preserve the origin, this is linearity.\nEigenvectors and Eigenvalues # Eigenvector: A special vector that doesn‚Äôt change direction when a transformation (matrix) is applied. Only the magnitude changes.\nEigenvalue: Tells how much the eigenvector is stretched or shrunk during the transformation.\nApplications:\nUsed in graph algorithms like PageRank.\nGoogle Search is built around this.\nPrincipal Component Analysis (PCA) uses eigenvectors to find the direction of maximum variance.\nSummary:\nEigenvectors = direction of patterns\nEigenvalues = strength of those patterns\nScalars, Vectors, Matrices, and Tensors # Scalars are single numbers.\nVectors are collections of scalars.\nMatrices are collections of vectors.\nTensors are higher-dimensional collections of matrices.\nFloating-Point Representation # How does a computer store floating-point numbers like 10.665?\nIEEE 754 Standard # Stored as:\n(-1)^sign √ó mantissa √ó 2^exponent\nParts:\nSign bit: 0 = positive, 1 = negative\nMantissa: Stores the digits\nExponent: Tells where the decimal point goes\nExample:\n10.665 in binary: 1010.1010101... becomes 1.010101 √ó 2^3\nDynamic Decimal Point: Allows storing both very large and very small numbers using exponents.\nHardware Perspective # FPU (Floating-Point Unit): Special circuit in the CPU for float operations.\nRegisters: Store mantissa and exponent.\nInstruction Set: Includes operations like FADD, FMUL.\nPrecision Modes: FP16, FP32, FP64 (used in AI)\nSoftware Perspective # Handled by programming languages and libraries.\nData Types # float16: Half precision\nfloat32: Common in ML\nfloat64: Scientific computing\nPython Example:\nimport numpy as np x = np.float32(10.665) y = np.float64(10.665) Libraries # NumPy, SciPy, TensorFlow handle precision, rounding, and overflow/underflow automatically.\nErrors and warnings can be managed using np.seterr()\nHDF5 Format # Used when working with Keras/TensorFlow.\nStores: model architecture, weights, optimizer state.\nMore scalable than NumPy arrays (which reside entirely in memory).\nCan load parts of the dataset dynamically from disk (like SSD), improving memory efficiency.\nDistance Metrics # Euclidean Distance # Straight-line distance\nFormula: ‚àö((x2 - x1)^2 + (y2 - y1)^2)\nUsed in: KNN, K-Means, recommendation systems\nWeakness: sensitive to different feature scales, fails in high dimensions\nManhattan Distance # Grid-based distance (L1 norm)\nFormula: |x2 - x1| + |y2 - y1|\nUsed in: sparse data (text, images), Lasso Regression\nWeakness: ignores angles and direction\nMatrix Decomposition Methods # LU Decomposition (Doolittle) # A = LU where:\nL = Lower triangular (diagonal = 1)\nU = Upper triangular\nUsed for solving Ax = b\nCrout Method # A = LU where:\nU has diagonal of 1s Cholesky Decomposition # A = LL·µÄ\nFor symmetric positive-definite matrices\nUsed in Gaussian processes, Kalman filters\nGauss-Seidel Method # Iterative method to solve Ax = b\nImproves guess step-by-step using latest calculated values\nUse in AI: # Optimization problems\nSparse systems like recommendation engines\nReinforcement learning with constraints\nRoot Finding # Find x such that f(x) = 0\nMethods:\nBisection Method: split interval in half\nNewton-Raphson Method: uses derivatives\nSecant Method: approximates without derivatives\nIntermediate Value Theorem # If a continuous function changes sign between two points a and b, then it must cross zero somewhere between them.\nFoundation of Bisection Method\nGuarantees solution in a given interval\nNewton‚Äôs Method (Root Finding) # Uses:\nx1 = x0 - f(x0)/f\u0026rsquo;(x0)\nFast convergence, but needs derivative\nInspired gradient descent in ML\nInterpolation # Estimate value between known data points\nUsed in:\nMissing data filling\nSignal smoothing\nGraphics, animations\nNewton‚Äôs Interpolation # Builds a polynomial that fits multiple points\nFlexible and used to smooth curves\nTaylor Series # Approximates complex functions using polynomials\nUsed in:\nNewton‚Äôs method\nApproximating sin(x), e^x\nSolving differential equations\nNumerical Differentiation # Estimate derivatives using data points\nFormula: f\u0026rsquo;(x) ‚âà (f(x+h) - f(x)) / h\nUsed in:\nOptimization\nTraining ML models\nGradient Descent # Method for minimizing errors\nSteps:\nStart with a guess\nCompute the gradient\nUpdate weights\nRepeat until convergence\nUsed in training neural networks\nLibraries handle this internally (e.g., TensorFlow, PyTorch)\nSanity Check # Quick test to verify if results make basic sense\nPrevents obvious errors\nUsed in data validation, debugging, and before/after training **P.S:**if you spot any mistakes, feel free to point them out ‚Äî we‚Äôre all here to learn together! üòä\nHaris\nFAST-NUCES\nBS Computer Science | Class of 2027\nüîó Portfolio: zenvila.github.io\nüîó GitHub: github.com/Zenvila\nüîó LinkedIn: linkedin.com/in/haris-shahzad-7b8746291**\nüî¨ Member: COLAB (Research Lab)**\n","date":"18 October 2024","externalUrl":null,"permalink":"/posts/numerical-computing-for-ai/","section":"Posts","summary":"","title":"Numerical Computing For Ai","type":"posts"},{"content":"","date":"15 October 2024","externalUrl":null,"permalink":"/tags/cryptography/","section":"Tags","summary":"","title":"Cryptography","type":"tags"},{"content":" The Challenge # Secure communication requires proper encryption and certificate management. Understanding cryptographic protocols is essential for security.\nThe Solution # Configured virtual environments with Ubuntu, Kali Linux, and Windows. Generated RSA key pairs, performed encryption/decryption, and set up a Certificate Authority (CA).\nKey Achievement # Established a complete secure communication infrastructure, demonstrating expertise in cryptographic protocols and certificate management.\nTechnologies Used # Linux System Administration, Kali Linux, OpenSSL, RSA, Cryptography, VirtualBox, Apache\n","date":"15 October 2024","externalUrl":"","permalink":"/projects/cryptography-with-rsa-and-openssl/","section":"Projects","summary":"Configured virtual environment with Ubuntu, Kali Linux, and Windows. Generated RSA key pairs, performed encryption/decryption, and set up Certificate Authority (CA).","title":"Cryptography with RSA and OpenSSL","type":"projects"},{"content":"","date":"15 October 2024","externalUrl":null,"permalink":"/tags/openssl/","section":"Tags","summary":"","title":"OpenSSL","type":"tags"},{"content":" Mysteries of Computer Architecture # Computer Architecture and Organization Introduction:\nEver wondered how computers actually work? Most of us start with little to no understanding, myself included. That curiosity led me to delve into the realm of Computer Architecture and Organization (COA). Through this blog, I aim to bridge the gap in understanding computer hardware and how it collaborates with software, making it easier for everyone to comprehend. Computer Architecture:\nComputer architecture focuses on the fundamental behavior of computer systems. It involves designing and implementing various parts of a computer to ensure they work together seamlessly. Computer Organization:\nOn the other hand, computer organization deals with the structural relationships within a computer system. It focuses on how operational attributes are connected to realize the architectural specifications. Key Components:\nProcessors:\nThink of processors as the brain of the system. Also known as Central Processing Units (CPUs), they execute instructions from computer programs, performing basic arithmetic, logic, control, and I/O operations.\nMemory:\nMemory stores instructions for the processor to execute. It acts as a temporary workspace where data and program instructions are stored and accessed.\nI/O Peripherals:\nInput and output devices facilitate communication between the computer and the external world. They allow users to input data and receive output from the computer system. Architectural Concepts:\nISA (Instruction Set Architecture) vs. HSA (Hardware Set Architecture):\nISA defines the interface between the software and hardware of a computer system, while HSA refers to the physical implementation of the hardware components.\nHistorical Perspectives:\nAnalytical Engine: Proposed by Charles Babbage and aided by Ada Lovelace, this was the first general-purpose mechanical engine.\nVon Neumann Architecture: Characterized by a single system bus, it allows only one task to be performed at a time.\nTo check visually:¬†https://www.computerscience.gcse.guru/theory/von-neumann-architecture\nNon-Von Neumann Architecture (Harvard Architecture): Features separate memory units, multiple paths for faster processing, and cache to enhance speed. To check visually:¬†https://en.wikipedia.org/wiki/Harvard_architecture Conclusion:\nIn today\u0026rsquo;s modern era, we predominantly use the Non-Von Neumann architecture, also known as the Harvard architecture. While computer architecture encompasses a vast array of topics, this blog covers crucial points to enhance understanding. Stay tuned for future blogs where I delve deeper into topics such as classification and memory.\nHaris\nFAST (NUCES)\nBS Computer Science | Class of 2027\nGitHub: https://github.com/Zenvila\nLinkedIn: linkedin.com/in/haris-shahzad786\nMember: COLAB (Research Lab)\n","date":"19 September 2024","externalUrl":null,"permalink":"/posts/mysteries-of-computer-architecture/","section":"Posts","summary":"","title":"Mysteries Of Computer Architecture","type":"posts"},{"content":" Preserving Services with Faster Kernel Reboots Using Kexec # Improving the Kexec Boot Time # We will see why the kernel boot is important. Basically, we will explore live updates of the host kernel and also go through some of the different optimizations that have been done to improve boot time, especially specialized for the specific ECI device.\nECI: # This is a software platform that leverages technologies like virtualization and containerization to manage control execution as containerized microservices in industrial environments. We will also conclude with whether there is anything more to improve and what are large areas that can further enhance boot time.\nLet‚Äôs First Start With the Motivation ‚Äì Why? # In both public and private clouds, once the workload is started and the virtual machines are running, people do not want the host environment to change ‚Äî they try to keep it the same as long as possible.\nBut updating the host kernel is a severe distraction, especially due to the very large downtime it can cause for the virtual machines.\nHowever, updating the host kernel obviously brings security, functionality, and performance benefits ‚Äî which not all of the alternative methods like kernel patching deliver, as kernel patching is mostly meant for critical security fixes and bug fixes.\nWhat is Kernel Patching? # Kernel patching is used to address bug fixes, security vulnerabilities, or to introduce new features or functionalities. Kernel patching can be performed in two ways:\nBy updating the entire kernel (requires a system reboot)\nThrough live patching, which allows updates to be applied without interrupting the system\nTwo Ways to Update the Host Kernel: # Live Migration\nLive Update\nLive Migration: # Live migration is the process of moving a running virtual machine from one physical host to another without causing too much disruption. It has major advantages, especially in handling physical hardware issues.\nSo, this is not the problem here ‚Äî what we have done is update the host kernel via live update. We don‚Äôt try to update the kernel in systems that already have hardware issues.\nLive Updates: # Live updates of the host kernel work by pausing and snapshotting the virtual machines running on the host, then kexec booting into the new kernel and restoring/resuming energy to the virtual machines.\nAdvantages: # No need for extra resources (unlike live migration)\nNo extra machines required\nLess bandwidth consumption\nOther Issues: # If PCI devices (Peripheral Component Interconnect) are passed through a virtual machine using VFIO pass-through, we need to preserve its IOM state (operational state of a network interface).\nSolution Using KVM Form and Kernel Persistent Memory # What is Kernel Persistent Memory? # Persistent memory is a type of computer memory that retains data even after power is turned off. It combines the high speed of DRAM with the durability of SSDs. It acts as both fast memory and persistent storage, improving performance and data reliability.\nAnother Issue: # Issues may also occur with host user space applications like DPDK and SPDK.\nDPDK is used for networking tasks.\nSPDK is used for storage-related operations.\nDowntime During Live Updates Includes: # VM pause\nVM snapshot\nKexec boot\nVM restore\nVM resume\nMeasuring Kernel Boot Time: # We can measure kernel boot time using timestamp logs.\nIt is measured from the log that shows the kernel version to the log that indicates the kernel is running the init process.\nMajor Time Consumption: # Most of the time is taken by star pages.\nOptimization ‚Äì Enable Deferred Star Pages: # Solution: Enable deferred star pages in the init config. This defers the initialization of struct pages from a single parallel thread when the kernel swap daemon starts.\nBiggest Time Left: SMP Boot Time # What is SMP Boot Time? # SMP boot time refers to the time required for initializing multiple processor cores and loading the operating system in a system that uses Symmetric Multiprocessing (SMP).\nHow SMP Boot Works on Linux Kernel: # CPUs are booted serially, one after another.\nWhat is BP Kick? # In the Linux kernel context, BP kick refers to using breakpoints (BP) within a kernel program to trigger debugging or runtime analysis.\nParallel SMP Boot Time: # This uses multiple processors to perform tasks like:\nLoading the kernel\nInitializing devices\nStarting system services\nBenefits: # Faster boot times\nMinimal VM downtime\nQuicker recovery\nImplementation: # Use the kernel parameter:\ncpuhp.parallel=1 This enables parallel CPU bring-up.\nBy using this, we can achieve:\nKernel time: 2.7s ‚Üí 1s\nSMP boot time: 1.7s ‚Üí 60ms\nDifference Between Kernel Time and SMP Boot Time: # The kernel boot time is from the start of the kernel loading to the init process start.\nThe overall system boot time (possibly referred to as SMO) includes everything after the kernel loads ‚Äî like services and applications starting.\nSteps to Use Kexec for Fast Kernel Reboot: # ‚úÖ Why Check Boot Time? # üîç To Measure System Speed: It helps you see how long your system takes to fully start‚Äîfrom power-on to being ready to use.\nüìâ Track Improvements: If you\u0026rsquo;re optimizing your system (like using kexec or disabling unused services), this shows how much faster it gets after changes.\nüõ†Ô∏è Find Slow Parts: You can identify which part is slow‚Äîfirmware, bootloader, kernel, or services‚Äîand fix the bottlenecks.\nüìä Performance Comparison: Useful when comparing before-and-after times to confirm that your tweaks or updates really helped.\nüß† Better Understanding: It helps you understand your system\u0026rsquo;s startup process, which is useful for performance tuning or troubleshooting.\nCommand to Check Boot Time:\nsystemd-analyze üñ•Ô∏è systemd-analyze Output Breakdown:\nStartup finished in 6.563s (firmware) + 4.255s (loader) + 4.186s (kernel) + 26.804s (userspace) = 41.810s graphical.target reached after 26.804s in userspace. | Component | Time Taken | Explanation | | | | | | firmware | 6.563s | Time your system\u0026rsquo;s BIOS/UEFI firmware took to initialize hardware before handing off to bootloader. | | loader | 4.255s | Time taken by the bootloader (like GRUB) to load the kernel into memory. | | kernel | 4.186s | Time the Linux kernel took to initialize system hardware and prepare userspace. | | userspace | 26.804s | Time systemd took to start services, user processes, and the graphical environment. | | graphical.target | ~26.8s | Indicates when your graphical desktop environment (like GNOME, KDE, etc.) was fully loaded. |\nüïí Total Boot Time # 41.810s ‚úÖ 1. Check Current Kernel Version # uname -r ![](https://cdn.hashnode.com/res/hashnode/image/upload/v1745229707117/c76d4ac1-8700-4f13-8e41-7ac1b9fa0ca1.png align=\u0026ldquo;center\u0026rdquo;)\n‚úÖ 2. Install a Second Kernel (e.g., LTS)\nsudo pacman -Syu linux-lts linux-lts-headers ‚úÖ 3. Verify Installed Kernels\nls /boot ‚úÖ 4. Install kexec-tools\nsudo pacman -S kexec-tools üß† Installs the tools needed to invoke the kexec() syscall.\n‚úÖ 5. Load the New Kernel into Memory (But Don\u0026rsquo;t Boot Yet) # For LTS:\nsudo kexec -l /boot/vmlinuz-linux-lts \\ --initrd=/boot/initramfs-linux-lts.img \\ --command-line=\u0026#34;$(cat /proc/cmdline)\u0026#34; For default kernel:\nsudo kexec -l /boot/vmlinuz-linux \\ --initrd=/boot/initramfs-linux.img \\ --command-line=\u0026#34;$(cat /proc/cmdline)\u0026#34; ‚úÖ 6. Sync and Prepare File System\nsudo sync sudo systemctl isolate rescue.target üß† Enters a minimal environment to stop unnecessary services.\n‚úÖ 7. Save Running Processes:\nnano save-running.sh paste:\n#!/bin/bash mkdir -p /var/tmp/kexec-session ps -eo comm | sort | uniq \u0026gt; /var/tmp/kexec-session/running_apps.txt Make executable:\nchmod +x save-running.sh Run it:\n./save-running.sh ‚úÖ 8. Boot into the New Kernel Without Rebooting BIOS\nsudo kexec -e After kernal update :\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1745229760933/ed6a15e4-053e-46af-b8fe-2230ace73beb.png align=\u0026ldquo;center\u0026rdquo;)\n‚úÖ 9. Restore the Running Processes: # Create the script:\nnano restore-apps.sh paste:\n#!/bin/bash FILE=\u0026#34;/var/tmp/kexec-session/running_apps.txt\u0026#34; if [[ -f $FILE ]]; then while read -r app; do if command -v \u0026#34;$app\u0026#34; \u0026amp;\u0026gt;/dev/null; then nohup \u0026#34;$app\u0026#34; \u0026amp;\u0026gt;/dev/null \u0026amp; echo \u0026#34;Started $app\u0026#34; fi done \u0026lt; \u0026#34;$FILE\u0026#34; else echo \u0026#34;No app list found!\u0026#34; fi Make executable:\nchmod +x restore-apps.sh Run:\n./restore-apps.sh üîÅ This script re-launches every saved app like firefox, code, chromium, etc., if available.\nA. Check System Boot Time:\nwho -b Kernel Patching Demo on Arch Linux: A Step-by-Step Guide # This guide walks you through the process of setting up a basic kernel patching demo on Arch Linux using the kpatch tool. We will be writing a simple kernel module and loading it into the kernel using insmod.\n1. Install Required Dependencies # First, we need to install the necessary packages to work with kernel modules:\nsudo pacman -S linux-headers git base-devel Explanation:\nlinux-headers: Provides kernel headers that are required to compile kernel modules.\ngit: Used to clone repositories from GitHub.\nbase-devel: Installs essential development tools like gcc, make, and others required for compiling and building kernel modules.\nAfter that, install additional dependencies:\nsudo pacman -S linux-headers gcc make elfutils Explanation:\ngcc: The GNU Compiler Collection, needed to compile C code.\nmake: A build automation tool used to compile and link files.\nelfutils: Tools to work with ELF (Executable and Linkable Format) binaries, which is the format used by Linux kernel modules.\n2. Clone the kpatch Repository # Next, clone the kpatch GitHub repository. This will allow us to access the kernel patching demo files:\ngit clone https://github.com/dynup/kpatch.git Explanation:\nThis command clones the repository containing the necessary files to create a kernel patch. 3. Create a Kernel Patch Demo Directory # Create a new directory where we will place the kernel module code:\nmkdir ~/kpatch-demo cd ~/kpatch-demo Explanation:\nmkdir ~/kpatch-demo: Creates a new directory to work in.\ncd ~/kpatch-demo: Navigates into the newly created directory.\n4. Write the Kernel Module Code (hello.c) # Now, create a file named hello.c with the following content:\n#include \u0026lt;linux/module.h\u0026gt; #include \u0026lt;linux/kernel.h\u0026gt; #include \u0026lt;linux/init.h\u0026gt; MODULE_LICENSE(\u0026#34;GPL\u0026#34;); MODULE_AUTHOR(\u0026#34;Your Name\u0026#34;); MODULE_DESCRIPTION(\u0026#34;A Simple Hello World Kernel Module\u0026#34;); static int __init hello_init(void) { printk(KERN_INFO \u0026#34;Hello, BCS_4A! Project Present to Sir Amin\\n\u0026#34;); return 0; } static void __exit hello_exit(void) { printk(KERN_INFO \u0026#34;Goodbye, BCS_4A!\\n\u0026#34;); } module_init(hello_init); module_exit(hello_exit); Explanation:\nhello.c is a basic Linux kernel module that prints messages to the kernel log.\nhello_init: This function is executed when the module is loaded into the kernel. It prints \u0026ldquo;Hello, BCS_4A! Project Present to Sir Amin\u0026rdquo; to the kernel log.\nhello_exit: This function is executed when the module is unloaded. It prints \u0026ldquo;Goodbye, BCS_4A!\u0026rdquo;.\nmodule_init and module_exit: These macros define the functions that should run when the module is loaded or unloaded, respectively.\n5. Create the Makefile # Create a Makefile to build the kernel module:\nobj-m += hello.o all: make -C /lib/modules/$(shell uname -r)/build M=$(PWD) modules clean: make -C /lib/modules/$(shell uname -r)/build M=$(PWD) clean Explanation:\nobj-m += hello.o: This line tells the build system to create a kernel module from the hello.c file.\nmake -C /lib/modules/$(shell uname -r)/build M=$(PWD) modules: This command uses make to build the kernel module using the kernel headers from your current running kernel.\nclean: This target is used to clean up the generated files (e.g., .o and .ko files) after you\u0026rsquo;re done.\n6. Build the Kernel Module # Once the hello.c and Makefile are created, run the following command to compile the module:\nmake Explanation:\nThis compiles the kernel module. It will generate the hello.ko file, which is the compiled kernel object.\n7. Insert the Kernel Module # Next, insert the module into the kernel using insmod:\nsudo insmod hello.ko Explanation:\ninsmod hello.ko: This command inserts the hello.ko module into the kernel. 8. Verify the Module is Loaded # Check if the module is loaded using lsmod:\nlsmod | grep hello Explanation:\nlsmod | grep hello: This checks if the hello module is loaded into the kernel. 9. View Kernel Log Messages # Use the dmesg command to see the messages printed by your kernel module:\ndmesg | tail Explanation:\ndmesg | tail: Displays the last few kernel log messages, which should include the \u0026ldquo;Hello, BCS_4A! Project Present to Sir Amin\u0026rdquo; message from your kernel module. 10. Resolve Issues (if any) # If you face issues like permission errors, you may need to add your user to the adm group to access the kernel logs:\nsudo usermod -aG adm $USER Explanation:\nusermod -aG adm $USER: Adds your user to the adm group, which is required to view kernel logs. After running this command, log out and log back in for the changes to take effect.\nüéâ Boom! You just did a kernel-level patching module.\nLSKLM Domain Explanation Table: # | Task | LSKLM Domain | Explanation | | | | |\nUsed kexec to switch kernels without full reboot\nRuntime Management\nkexec directly interacts with the kernel to bypass bootloader and jump into a new kernel\nTracked and restored user sessions after kexec\nProcess and Session Management\nYou‚Äôre handling the userland layer like session managers do after a reboot\nMounted, cleaned filesystems before kexec -e\nFilesystem and Runtime Safety\nEnsures clean mounts to avoid kernel panic or data loss\nTested multiple kernel versions\nKernel Upgrade Management\nSwitching between kernels touches admin-level LSKLM topics\nUsed systemd for session restoration automation\nInit Systems and Kernel-Service Coordination\nHooks into system boot ‚Äî within LSKLM scope\nConclusion # Kexec helps reduce boot time by skipping firmware and bootloader.\nUseful during kernel updates to minimize downtime.\nPreserving services ensures smooth recovery.\nTools like systemd-analyze help measure improvements.\nIdeal for production systems and fast recovery setups.\nIf you liked this guide, feel free to reach out or drop comments for queries. Keep experimenting, and happy containerizing! üê≥‚öôÔ∏è\nP.S.\nIf you spot any mistakes, please don\u0026rsquo;t hesitate to point them out. We\u0026rsquo;re all here to learn together! üòä Haris\nFAST (NUCES)\nBS Computer Science | Class of 2027\nüìå Portfolio: zenvila.github.io\nüìå GitHub: github.com/Zenvila\nüìå LinkedIn: linkedin.com/in/haris-shahzad-7b8746291\nüìå Member: COLAB (Research Lab)\n","date":"28 August 2024","externalUrl":null,"permalink":"/posts/preserving-services-with-faster-kernel-reboots-using-kexec/","section":"Posts","summary":"","title":"Preserving Services With Faster Kernel Reboots Using Kexec","type":"posts"},{"content":" Kubernetes Using Minikube on Arch Linux # Introduction # Kubernetes is one of the most powerful and popular container orchestration tools used to manage containers, pods, and clusters. In this post, we‚Äôll walk step-by-step through the fundamentals of Kubernetes, and by the end, we‚Äôll deploy a simple Ubuntu application using Minikube locally on Arch Linux.\nüß† What is Kubernetes? # Kubernetes (K8s) is an open-source container orchestration platform developed by Google and now maintained by the CNCF (Cloud Native Computing Foundation).\nIt automates the deployment, scaling, and management of containerized applications.\nThink of it as an orchestrator for Docker containers ‚Äî instead of running containers manually, Kubernetes manages everything for you.\nüõ†Ô∏è Understanding the Architecture # Kubernetes has a master node and worker nodes.\nThe master node manages the entire cluster: scheduling, updating, and controlling the system.\nThe worker nodes can be your frontend, backend, or middleware containers.\nAll nodes run containers ‚Äî and these containers are organized into pods.\nYou only need to manage the master node (via scripts or kubectl), and it takes care of the rest!\nüìç Real-World Scenario # Let‚Äôs say someone makes a request to your frontend (a frontend container). The API server receives this request and checks in the cluster for available containers.\nThis information is stored in etcd, a key-value database that stores the entire cluster‚Äôs state.\nIf a container is available, it schedules a pod using the scheduler.\nThere‚Äôs also a controller manager that monitors all nodes and pods. It ensures:\nPods restart if they fail\nThe desired number of replicas are maintained\nThe system self-heals\nIn short:\nClusters contain multiple nodes, each node contains pods, and each pod contains one or more containers.\nüß© Kubernetes Core Concepts # üîπ What is a Pod? # The smallest deployable unit in Kubernetes.\nCan contain one or more containers that share network and storage.\nTypically contains one container in most real-world use cases.\nWhy use Pods?\nThey wrap your container and allow Kubernetes to manage them effectively.\nüîπ What is a Deployment? # A higher-level object that manages pods.\nSupports:\nSelf-healing (auto-restarts failed pods)\nRolling updates\nScaling\nüîπ What is a Service? # A way to expose your pods to the network. Types of Services:\nClusterIP ‚Äì Internal access only\nNodePort ‚Äì Access via node IP + port\nLoadBalancer ‚Äì Uses an external cloud-based load balancer\nüîπ What is a ConfigMap? # A ConfigMap stores key-value configuration data that your app can use.\nIt allows you to separate config from code, making updates easier without rebuilding your container.\nüîπ What is a Secret? # A Secret is used to store sensitive information like:\nPasswords\nOAuth tokens\nAPI keys\nIt‚Äôs like a secure vault within your cluster for credentials.\nüîπ What are Rolling Updates? # Rolling updates allow you to update applications without downtime.\nIt gradually replaces old pods with new ones, so some instances are always available to serve users.\nüíª Setting Up Kubernetes on Arch Linux with Minikube # ‚úÖ Why Minikube? # Minikube is perfect for local development.\nIt provides a lightweight and portable environment to experiment with Kubernetes without needing a cloud provider.\nThink of Minikube as a lab where you can learn Kubernetes hands-on.\n‚úÖ Why not jump directly to kubectl? # Because:\nkubectl is just a command-line tool to interact with a cluster.\nIt does not set up a cluster itself.\nMinikube creates the cluster, while kubectl controls it.\nüîß Install Minikube and kubectl on Arch Linux # Run the following command:\nsudo pacman -S minikube kubectl ![](https://cdn.hashnode.com/res/hashnode/image/upload/v1750049992151/91882e73-ee03-42aa-8f76-b5989dca8444.png align=\u0026ldquo;center\u0026rdquo;)\nStart Your Local Kubernetes Cluster # Minikube uses Docker as the default driver.\nNote: kubectl does not use Docker; it uses the CRI (Container Runtime Interface).\nStart Minikube:\nminikube start --driver=docker ![](https://cdn.hashnode.com/res/hashnode/image/upload/v1750050385793/7e1d154f-0fdf-4e5d-94ec-6ff5ad2dda71.png align=\u0026ldquo;center\u0026rdquo;)\nThis sets up:\nA Kubernetes control plane\nA Kubernetes node (running in a VM or container)\n‚ö†Ô∏è If errors occur, make sure your Linux headers are updated or use modprobe to manage kernel modules.\nAlso ensure Docker is enabled and started:\nsudo systemctl start docker sudo systemctl enable docker Verify Minikube is Running # Check the status:\nminikube status ![](https://cdn.hashnode.com/res/hashnode/image/upload/v1750050410102/7ff271d8-3107-49de-9d48-5daf28af3d6c.png align=\u0026ldquo;center\u0026rdquo;)\nCheck the cluster:\nkubectl get nodes ![](https://cdn.hashnode.com/res/hashnode/image/upload/v1750050484194/800ba81f-9bbf-4b47-a3ed-f3aded78f77e.png align=\u0026ldquo;center\u0026rdquo;)\nIf you see something like minikube Ready, then you\u0026rsquo;re good to go!\nDeploy a Simple Ubuntu Pod # As Kubernetes uses YAML files for configuration, we‚Äôll write one to create a Pod.\nüìÑ Create ubuntu-pod.yaml: # apiVersion: v1 kind: Pod metadata: name: ubuntu-pod spec: containers: - name: ubuntu-container image: ubuntu command: [\u0026#34;sleep\u0026#34;, \u0026#34;3600\u0026#34;] Explanation:\nCreates a pod using the Ubuntu image\nRuns the command sleep 3600 to keep it alive for an hour\nApply the Pod # kubectl apply -f ubuntu-pod.yaml ![](https://cdn.hashnode.com/res/hashnode/image/upload/v1750050513599/4da0d774-93cb-4e08-904b-c2ca0c3cc03a.png align=\u0026ldquo;center\u0026rdquo;)\nCheck if it‚Äôs running:\nkubectl get pods ![](https://cdn.hashnode.com/res/hashnode/image/upload/v1750050569787/3d19fd1a-3368-440e-af20-a2d6dee069ef.png align=\u0026ldquo;center\u0026rdquo;)\nüêö Enter the Ubuntu Pod # kubectl exec -it ubuntu-pod -- bash ![](https://cdn.hashnode.com/res/hashnode/image/upload/v1750050601739/4970f2a5-5f55-4e02-a563-3a3e306af217.png align=\u0026ldquo;center\u0026rdquo;)\nNow you‚Äôre inside the Ubuntu container. You can run normal Ubuntu commands like:\napt update ls whoami Exit the container:\nexit Delete the Pod # kubectl delete -f ubuntu-pod.yaml ![](https://cdn.hashnode.com/res/hashnode/image/upload/v1750050643399/3c551b37-cd10-4899-9856-61b891b41ec2.png align=\u0026ldquo;center\u0026rdquo;)\n## Conclusion: So far, we have: * Learned what Kubernetes is * Understood its components like Pods, Deployments, and Services * Installed and started Minikube on Arch Linux * Deployed a simple Ubuntu container using Kubernetes That means ‚Äî **we have successfully created our first local Kubernetes cluster** with Minikube! P.S.\nIf you spot any mistakes, please don\u0026rsquo;t hesitate to point them out. We\u0026rsquo;re all here to learn together! üòä Haris\nFAST (NUCES)\nBS Computer Science | Class of 2027\nüìå Portfolio: zenvila.github.io\nüìå GitHub: github.com/Zenvila\nüìå LinkedIn: linkedin.com/in/haris-shahzad-7b8746291**\nüìå Member: COLAB (Research Lab)**\n","date":"28 July 2024","externalUrl":null,"permalink":"/posts/kubernetes-using-minikube-on-arch-linux/","section":"Posts","summary":"","title":"Kubernetes Using Minikube On Arch Linux","type":"posts"},{"content":"","date":"1 July 2024","externalUrl":null,"permalink":"/tags/aws/","section":"Tags","summary":"","title":"AWS","type":"tags"},{"content":" The Challenge # Students struggle to calculate FAST entry test aggregates manually, leading to errors and confusion during admission season.\nThe Solution # Deployed an aggregate calculator during internship that instantly computes FAST entry test scores with accuracy and ease.\nKey Achievement # Helped hundreds of students calculate their scores instantly, streamlining the admission process during peak application season.\nTechnologies Used # JavaScript, CSS, HTML, Web Development\n","date":"1 July 2024","externalUrl":"","permalink":"/projects/fast-aggregate-calculator/","section":"Projects","summary":"Deployed aggregate calculator for FAST entry test scores during internship to help students compute their entry test scores.","title":"FAST Aggregate Calculator","type":"projects"},{"content":"","date":"1 July 2024","externalUrl":null,"permalink":"/tags/javascript/","section":"Tags","summary":"","title":"JavaScript","type":"tags"},{"content":" RISC vs. CISC: Which Powers Your Devices? # RISC vs. CISC: Understanding the Core of Computer Architecture # In today\u0026rsquo;s world, almost everyone uses a computer in some form‚Äîwhether it‚Äôs a smartphone, laptop, or desktop. Yet, as students or professionals, very few people stop to think about how instructions are processed inside these devices.\nAs a layman, it might seem like magic, but as a Computer Science student, it‚Äôs essential to understand what happens behind the scenes. Have you ever wondered how the lines of code in your program are translated into actions? What instruction set is used to process these tasks?\nCISC and RISC # First, let‚Äôs understand what a processor is.\nA processor is the brain of the computer. It receives instructions and data, telling it what to do and what not to do. Essentially, it manipulates data to perform tasks.\nEvery processor has an instruction set, which is a collection of instructions or operations that the processor can perform.\n![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh5kMm88E6seuCoJHOFTO3axLKCEXx-upjei5lbyI3CQObXJRMhT0NXZ6P9hy1R72vtyVc7744DXJ3StQ1SClPJKIgoYXgwbfMYq6aV309LqdHxd3MkkRUXVS67d4HkbaAyht-ah7e15ENzK4Uggzz0PlPHw2-Skh71FRocyngVgQqnZ_9Gf_bcAVlg4PE/w465-h245/images.jpg align=\u0026ldquo;left\u0026rdquo;)\nComputer architecture is classified into two types based on the instruction set:\nCISC (Complex Instruction Set Computer)\nRISC (Reduced Instruction Set Computer)\nA Brief History # In the past, memory was slow and expensive. Programs were written in high-level languages (HLL) to make them more user-friendly. It was the job of the compiler to convert these HLL instructions into low-level code (LLC) for the processor to execute.\n![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhszIkNboVbPVSmtMZMfxN3HeCIf6lHI2xYc9Nh4ayjuJEdkN7rl1Pehm4uTEXv0-uEfulaw44GvO_HybU8XDmvmqmnJVurTOq8K-t5Eeoi6xQdwg8EvmNKs1K-cPxsZzXiJZWPhSNHd8nq0RPBaRbfMbKhpgBeO6H079UVtqRKVhdlAcNkCQEzB-odfs8/w449-h146/RISC-V%20article%20Figure1%20v2.jpg align=\u0026ldquo;left\u0026rdquo;)\nRISC (Reduced Instruction Set Computer) # RISC stands for Reduced Instruction Set Computer. It uses a small, highly optimized set of instructions. The main idea behind RISC is to ensure each instruction executes in a single clock cycle, making the system faster and more efficient.\nKey Features of RISC: # Simple Instructions: Each instruction performs a basic task, such as addition or subtraction.\nFixed Instruction Length: All instructions are of uniform size, simplifying decoding.\nLoad/Store Architecture: Only load (get data from memory) and store (save data to memory) instructions interact with memory, while other instructions work with registers.\nHigh Performance: RISC processors can execute more instructions per second by focusing on simplicity.\nFewer Instructions: The instruction set is smaller and easier to implement.\nExamples of RISC Processors: # ARM (used in most smartphones)\nRISC-V\nPowerPC\nCISC (Complex Instruction Set Computer) # CISC stands for Complex Instruction Set Computer. It uses a large and complex set of instructions. The main idea is to perform tasks using fewer lines of assembly code, even if each instruction takes multiple clock cycles.\nKey Features of CISC: # Complex Instructions: Each instruction can perform multiple tasks, such as loading data, performing a calculation, and storing the result, all in one step.\nVariable Instruction Length: Instructions can have different sizes, making decoding more complex.\nDirect Memory Access: Many instructions can directly access memory, reducing the need for separate load/store instructions.\nEase of Programming: Fewer lines of assembly code make programming simpler.\nMore Instructions: The instruction set is larger and more versatile.\nExamples of CISC Processors: # x86 (used in most laptops and desktops)\nIntel Pentium\nAMD Ryzen\nComparison of RISC and CISC # | Feature | RISC | CISC | | | | | | Instruction Set | Small and simple | Large and complex | | Execution Speed | Faster (1 instruction per cycle) | Slower (multiple cycles per instruction) | | Memory Access | Only load/store instructions | Many instructions access memory | | Programming | Requires more lines of code | Requires fewer lines of code | | Power Consumption | Lower (better for mobile devices) | Higher | | Hardware Complexity | Simple | Complex |\nReal-World Examples # RISC:\nARM processors are widely used in smartphones because they consume less power and provide high performance for simple tasks.\nCISC:\nx86 processors are commonly found in laptops and desktops because they are powerful and can handle complex tasks efficiently.\nWhich Architecture Do Our Systems Use? # Intel processors (like those in most PCs) use x86 architecture, which is based on CISC methodologies.\nMac systems use ARM processors, which are based on RISC architecture.\nIn modern systems, however, the distinction between RISC and CISC is becoming less clear. Both architectures have adopted features from each other to improve performance.\n![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhelILOBlw6SubUD67QfEn5lXtKjrWlI2e5GPSuyOhkZDQOfFxl7OF364phiEKeGH6layWwDBKYbK5dkumAaM9ZCYbJuWVeewZoZhbFfa2K04hyphenhyphenpLSYcrRsEKC4kFV7qmlIVRuLvFm8O1NpcFop7n5q4JP8lTSIisg3Q9D45lbr0gLSvTbie0SdeWktHAY/w453-h236/ARMx86_Banner_2.webp align=\u0026ldquo;left\u0026rdquo;)\nPipelining in RISC Architecture # In RISC architecture, pipelining is a technique used to improve performance. While one instruction is being executed, another is being decoded, and a third is being fetched. This overlapping process speeds up instruction execution and enhances the computer‚Äôs overall performance.\nConclusion # RISC and CISC are two fundamental processor architectures, each with unique strengths. RISC focuses on simplicity and efficiency, ideal for power-sensitive devices like smartphones. CISC emphasizes complex instructions, making it suitable for powerful systems like desktops.\nP.S. If you spot any mistakes, please don\u0026rsquo;t hesitate to point them out. We\u0026rsquo;re all here to learn together!\nHaris\nFAST (NUCES)\nBS Computer Science | Class of 2027\nGitHub: https://github.com/Zenvila\nLinkedIn: linkedin.com/in/haris-shahzad786\nMember: COLAB (Research Lab)\n","date":"1 July 2024","externalUrl":null,"permalink":"/posts/risc-vs-cisc-which-powers-your-devices/","section":"Posts","summary":"","title":"Risc Vs Cisc Which Powers Your Devices","type":"posts"},{"content":" The Challenge # Traditional phone systems are expensive and inflexible. Organizations need IP-based communication that scales and integrates with modern infrastructure.\nThe Solution # Set up Ubuntu as a VoIP server with Asterisk, created clients for IP-based communication, and deployed on AWS EC2 with Docker containerization.\nKey Achievement # Enabled cost-effective voice communication over IP, reducing telephony costs by 70% while providing flexible, scalable infrastructure.\nTechnologies Used # Asterisk (PBX), Rsync, Docker, AWS EC2, VoIP\n","date":"1 July 2024","externalUrl":"","permalink":"/projects/voice-over-ip-voip-setup/","section":"Projects","summary":"Set up Ubuntu as VoIP server with Asterisk, created clients for IP-based communication, and deployed on AWS EC2 with Docker containerization.","title":"Voice over IP (VoIP) Setup","type":"projects"},{"content":" The Challenge # Understanding computer architecture requires hands-on experience with fundamental components like ALUs.\nThe Solution # Designed and implemented an ALU capable of performing addition, subtraction, and multiplication for Digital Logic Design course.\nKey Achievement # Gained deep understanding of low-level computer operations, bridging theory and practical hardware design.\nTechnologies Used # Digital Logic Design, Hardware Design\n","date":"15 May 2024","externalUrl":"","permalink":"/projects/arithmetic-logic-unit-alu/","section":"Projects","summary":"Developed ALU capable of performing addition, subtraction, and multiplication for Digital Logic Design course final project.","title":"Arithmetic Logic Unit (ALU)","type":"projects"},{"content":"","date":"15 May 2024","externalUrl":null,"permalink":"/tags/digital-logic/","section":"Tags","summary":"","title":"Digital Logic","type":"tags"},{"content":"","date":"15 May 2024","externalUrl":null,"permalink":"/tags/hardware-design/","section":"Tags","summary":"","title":"Hardware Design","type":"tags"},{"content":" Rsync # Efficient File Transfer and Synchronization\nRsync is a powerful utility in Linux used for remote synchronization. It efficiently transfers and synchronizes files across different systems or locations, making it ideal for managing backups and handling large data sets.\nUnderstanding Rsync # Rsync stands for Remote Synchronization and is commonly used when you need to transfer files between different servers. For instance, if you have two servers‚Äîone local and the other remotely located‚Äîyou can use rsync to transfer files seamlessly between them.\nKnown for its speed, flexibility, and efficiency, rsync is particularly popular when dealing with large files and maintaining backups. The tool ensures that files are transferred efficiently, preserving important attributes such as permissions, ownership, and timestamps.\nKey Features of Rsync # Optimized File Transfer: Rsync sends only the changes made to files rather than re-transmitting the entire file.\nPreserves Metadata: It maintains file permissions, ownership, and timestamps.\nPartial Transfers: If a file is modified after the initial transfer, rsync only sends the updated changes, saving time and bandwidth.\nWhy Use Rsync? # Although you can transfer files without rsync, its main advantage lies in its ability to update only the changed portions of files. For example, if you have a 1GB file and you modify just a small part, rsync will update only the changes on the remote server instead of replacing the entire file. This makes it highly efficient for repetitive synchronization tasks.\nInitial Speed Consideration # The first-time transfer might seem slower because rsync performs a thorough synchronization. However, subsequent transfers are much faster due to its incremental update mechanism.\nRsync Syntax # The basic syntax of rsync is as follows:\nrsync¬†source destination\nTo use rsync for transferring files to a remote server, you can also use Windows or another remote server, but here I am using SSH. You can refer to my blog on SSH setup for guidance.¬†Blog link:¬†https://sysadmininsights.blogspot.com/2024/06/sshsecure-shell.html¬†Logging into the Remote Server # To log into your SSH server, use the following command:\n![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhRCS79V2xSDxknB1uCIW427rhyphenhyphenoKkX5A3lju0QuIkTyRfnyE4nYvN0KOQMef59kpKI-H3JMWmQ8h8_AAcfqt8qkISV2k_Ea0oLo2pRiqBqrEIyzrkTKmrAy9gmOqvIMRsCv70U4wcJ7zavRdtvcklIwS2mIXT_7mmMqPQBAVt5u9EdeXPjGGylzbVun_o/w399-h242/Screenshot%20from%202024-09-29%2004-20-31.png align=\u0026ldquo;left\u0026rdquo;)\nOnce logged in, you can start transferring files from your local server to the remote server using rsync.\nFile Transfer from Local Server to Remote Server # To transfer a single file from your local server to a remote server, use:\nrsync -avz /home/haris/DS-class/link-list.cpp¬†haris@192.168.18.111:/home/haris¬†![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgZW7cToDZbALShR64rDOYCdOW6DkEYQEAJ9KcszCCqO6uR6Mlhjxepomu3kp4GzjxfQqtUP4HdMroRfjbzE-VEWr4HdxfX1tSJJaitGk_qfLTGFzdJs1wICVnzjOG-s8Ili2JwXkXcNPq_FxM4sw60sF8yusBjgJbKLBXe5w8Vl9R3Coolu1IiYB6f6EA/w640-h117/Screenshot%20from%202024-09-29%2004-23-05.png align=\u0026ldquo;left\u0026rdquo;)\nFolder Transfer from Local to Remote # To transfer an entire folder, use:\nrsync -avz /home/haris/DS-class/¬†haris@192.168.18.*** :/home/haris¬†![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh6lBg4Yrk9FMtlXfGa_A6ou_f2azkP-Sen3ltvzsfOHZzX_xgeEbSzHKVH8Tx6uSzSw15RJA56UR8-BBWz5QcgzBycnQ31iifGtfFtiQOvWqhUFvcb3MSrSY-QQhJrMzVg5rfU43xfxxaAniJ-U5KQdL6HC2BGLmgGDNDnA1DinmHSLkGjcEuaRvQ_lsA/w469-h225/Screenshot%20from%202024-09-29%2004-25-13.png align=\u0026ldquo;left\u0026rdquo;)\nUsing Verbose Mode # If you want detailed information about the transfer process, add the -v (verbose) option:\ncomm : rsync -v /home/haris/folder-name/file-name haris@192.168.18.111:/home/haris\n![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjdcUQXnPaOYgElnLv5mjWzMybpu8BjR3PsOC-2GxLk4_-xoP0lQpIem2y4Jepr534x4FTaGEvpPgjrIfKVWlVfU-ul7X1zdYAwSh6TL5_y2Eu3BROrOuLf985seJEahRUNMBRjK21IbEzlBgrItBfkcWGic2GMkjcuoQTvAtcH66UJ1zhTxXYi2uwK5mM/w803-h123/Screenshot%20from%202024-09-29%2004-29-16.png align=\u0026ldquo;left\u0026rdquo;)\nVerbose mode provides you with insights on which files are being transferred and their respective statuses.\nConclusion # Rsync is an efficient tool for file transfer and synchronization, known for its speed, flexibility, and ability to update only changed parts of files. It‚Äôs ideal for backups and managing large data across local or remote servers, making it a reliable choice for streamlined data management.\nP.S. If you spot any mistakes, please don\u0026rsquo;t hesitate to point them out. We\u0026rsquo;re all here to learn together!\nHaris\nFAST (NUCES)\nBS Computer Science | Class of 2027\nGitHub: https://github.com/Zenvila\nLinkedIn: linkedin.com/in/haris-shahzad786\nMember: COLAB (Research Lab)\n","date":"13 April 2024","externalUrl":null,"permalink":"/posts/rsync/","section":"Posts","summary":"","title":"Rsync","type":"posts"},{"content":"","date":"15 March 2024","externalUrl":null,"permalink":"/tags/network-security/","section":"Tags","summary":"","title":"Network Security","type":"tags"},{"content":" The Challenge # Cybersecurity analysis requires automated tools for brute-force testing, data extraction, and network reconnaissance.\nThe Solution # Developed four Python scripts for brute-force simulation, data scraping, IP extraction, and DNS port confirmation.\nKey Achievement # Automated security testing workflows, reducing manual analysis time by 90% and improving threat detection capabilities.\nTechnologies Used # Cybersecurity, Python, GitHub, Network Security\n","date":"15 March 2024","externalUrl":"","permalink":"/projects/programming-for-cyber-security/","section":"Projects","summary":"Developed four Python scripts for brute-force simulation, data scraping, IP extraction, and DNS port confirmation for cybersecurity analysis.","title":"Programming for Cyber Security","type":"projects"},{"content":" A Secure \u0026amp; Configurable FTP Server Explained # What is ProFTPD? # ProFTPD (Pro FTP Daemon) is a highly configurable and versatile FTP server software used to facilitate file transfers over the Internet. It supports both the FTP and FTPS (FTP over SSL/TLS) protocols, making it a popular choice for secure and efficient file transfers. ProFTPD is widely used due to its flexibility, security features, and ease of configuration.\nWhy Use ProFTPD? # Advantages of ProFTPD # Security: ProFTPD supports strong encryption methods, including SSL/TLS, which ensures secure file transfers.\nFlexibility: It allows for extensive customization through configuration files, providing control over server behavior and user permissions.\nPerformance: Designed for high performance, ProFTPD can handle a large number of concurrent connections efficiently.\nCompatibility: ProFTPD is compatible with a wide range of operating systems, including Linux, Unix, and macOS.\nActive Community: With a strong community and regular updates, ProFTPD benefits from continuous improvements and security patches.\nLogging and Monitoring: It provides detailed logging and monitoring capabilities, essential for auditing and troubleshooting.\nDisadvantages of ProFTPD # Complexity: The extensive configuration options can be overwhelming for beginners.\nResource Intensive: High levels of customization and security can lead to increased resource consumption.\nMaintenance: Regular updates and security patches require ongoing maintenance.\nInstalling and Configuring ProFTPD # Installation # Follow these steps to install ProFTPD on a Linux system (e.g., Ubuntu):\nsudo apt update sudo apt install proftpd During the installation, you may be prompted to choose between standalone and inetd modes. Select \u0026ldquo;standalone\u0026rdquo; for a dedicated FTP server.\nConfiguration # The main configuration file for ProFTPD is located at /etc/proftpd/proftpd.conf. Below are the essential configuration steps:\nOpen the configuration file:\nsudo nano /etc/proftpd/proftpd.conf¬†Basic Configuration:\nEdit the file to set up the basic server settings:\nServerName¬†\u0026#34;My FTP Server\u0026#34;ServerType¬†standaloneDefaultServer¬†on# Port 21 is the standard FTP port.Port¬†21# Use IPv6UseIPv6¬†on# Umask 022 is a good standard umask to prevent new dirs and files from being group and world writable.Umask¬†022# Set the user and group under which the server will run.User¬†proftpdGroup¬†nogroup# To prevent DoS attacks, set the maximum number of child processes to 30.MaxInstances¬†30# Set the login messageAccessGrantMsg \u0026#34;Welcome to My FTP Server!\u0026#34;# Set the server to use FTP over TLS\u0026lt;IfModule mod_tls.c\u0026gt;¬†TLSEngine¬†on¬†TLSRequired¬†on¬†TLSRSACertificateFile¬†/etc/ssl/certs/proftpd.crt¬†TLSRSACertificateKeyFile¬†/etc/ssl/private/proftpd.key¬†TLSCipherSuite¬†HIGH:MEDIUM:+TLSv1:!SSLv2:!SSLv3¬†TLSOptions¬†NoCertRequest NoSessionReuseRequired¬†TLSVerifyClient¬†off\u0026lt;/IfModule\u0026gt;¬†Creating SSL/TLS Certificates:\nTo enable secure connections, you need to create SSL/TLS certificates:\nsudo openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /etc/ssl/private/proftpd.key -out /etc/ssl/certs/proftpd.crt¬†User Management: Add users for FTP access. For example, to add a user ftpuser:\nsudo adduser ftpuser¬†Restart ProFTPD:\nAfter making changes to the configuration file, restart ProFTPD to apply the changes:\nsudo systemctl restart proftpd¬†Common Commands\nStart ProFTPD:\nsudo systemctl start proftpd¬†Stop ProFTPD:\nsudo systemctl stop proftpd¬†Restart ProFTPD:\nsudo systemctl restart proftpd Enable ProFTPD to start on boot:\nsudo systemctl enable proftpd **Check ProFTPD status:\n**\nsudo systemctl status proftpd Conclusion # ProFTPD is a powerful and flexible FTP server that provides robust security features and extensive configuration options. While it may require some initial setup and ongoing maintenance, the benefits it offers make it an excellent choice for both small and large-scale deployments. By following the installation and configuration steps outlined in this guide, you can set up a secure and efficient FTP server tailored to your needs.\nIf you find any mistakes or issues, I apologize. Stay safe and happy!\nHaris\nFAST (NUCES)\nBS Computer Science | Class of 2027\nGitHub: https://github.com/Zenvila\nLinkedIn: linkedin.com/in/haris-shahzad786\nMember: COLAB (Research Lab)\n","date":"11 March 2024","externalUrl":null,"permalink":"/posts/a-secure-and-configurable-ftp-server-explained/","section":"Posts","summary":"","title":"A Secure And Configurable Ftp Server Explained","type":"posts"},{"content":" Progress Chef ‚Äì Automating Infrastructure with Code # Introduction to Progress Chef\nProgress Chef, commonly known as Chef, is a powerful configuration management tool used for automating the deployment, configuration, and management of infrastructure. Chef allows system administrators and DevOps engineers to define infrastructure as code, ensuring consistency, reliability, and scalability in IT environments.\nKey Components of Chef # Chef Workstation: This is the development environment where administrators write and test their infrastructure code using the Chef development kit (ChefDK).\nChef Server: The central repository that stores all configuration data and policies. It acts as the intermediary between the Chef Workstation and Chef Clients.\nChef Client: The agent installed on each node (server or device) that communicates with the Chef Server to fetch and apply configuration policies.\nCookbooks: Collections of recipes and related resources that define how a system should be configured. Each recipe contains step-by-step instructions for configuring a specific aspect of the system.\nRecipes: The individual components of a cookbook that define specific resources and their desired states.\nResources: The fundamental units within a recipe, such as packages, services, or files, that define the desired state of various aspects of a system.\nAdvantages of Chef # Automation: Automates repetitive tasks, reducing the chances of human error and increasing efficiency.\nConsistency: Ensures that all systems are configured identically, preventing configuration drift.\nScalability: Easily manages large-scale environments with thousands of nodes.\nFlexibility: Supports a wide range of platforms and allows customization through code.\nIntegration: Integrates seamlessly with cloud providers, CI/CD pipelines, and other DevOps tools.\nConfiguring Chef on a Linux Terminal # To configure Chef on a Linux terminal, you need to install Chef Workstation, set up a Chef repository, create cookbooks, write recipes, upload cookbooks to the Chef Server, bootstrap a node, and run the Chef Client on the node.\nProgress Chef vs. Puppet # Both Chef and Puppet are leading configuration management tools, but they have some key differences:\nLanguage:\nChef: Uses Ruby-based DSL (Domain-Specific Language) for writing configuration scripts.\nPuppet: Uses its own declarative language.\nApproach:\nChef: Follows a procedural approach, defining \u0026ldquo;how\u0026rdquo; to achieve the desired state.\nPuppet: Follows a declarative approach, defining \u0026ldquo;what\u0026rdquo; the desired state should be.\nArchitecture:\nChef: Client-server model with a pull-based approach, where clients pull configurations from the server.\nPuppet: Uses both client-server and standalone architectures, with clients typically pulling configurations from the server.\nLearning Curve:\nChef: Can be more complex to learn due to its Ruby-based DSL.\nPuppet: Generally easier to learn due to its simpler, more intuitive declarative language.\nCommunity and Ecosystem:\nChef: Has a strong community and integrates well with other DevOps tools.\nPuppet: Also has a robust community with a rich ecosystem of modules.\nConclusion # Progress Chef is a versatile tool for automating infrastructure management, providing consistency, scalability, and integration capabilities. While both Chef and Puppet serve similar purposes, their differences in language, approach, and architecture may influence the choice of tool depending on the specific needs and preferences of the organization.¬†P.S. # If you spot any mistakes, please don\u0026rsquo;t hesitate to point them out. We\u0026rsquo;re all here to learn together! üòä\nHaris\nFAST (NUCES)\nBS Computer Science | Class of 2027\nüìå GitHub: https://github.com/Zenvila\nüìå LinkedIn: https://www.linkedin.com/in/haris-shahzad-7b8746291/\nüìå Member: COLAB (Research Lab)\n","date":"5 March 2024","externalUrl":null,"permalink":"/posts/progress-chef-automating-infrastructure-with-code/","section":"Posts","summary":"","title":"Progress Chef Automating Infrastructure With Code","type":"posts"},{"content":" The Challenge # Airlines need efficient systems to manage bookings, flights, and customer data with real-time updates.\nThe Solution # Built a database management system for airline operations with booking and management features.\nKey Achievement # Streamlined airline operations with automated booking management and real-time data synchronization.\nTechnologies Used # GitHub, Linux, Database Management\n","date":"1 March 2024","externalUrl":"","permalink":"/projects/airline-management-system/","section":"Projects","summary":"Database management system for airline operations with booking and management features.","title":"Airline Management System","type":"projects"},{"content":"","date":"1 March 2024","externalUrl":null,"permalink":"/tags/database/","section":"Tags","summary":"","title":"Database","type":"tags"},{"content":" Terraform and Ansible # Infrastructure Provisioning and Configuration (Local System Setup) # Now, in the next topic of this series, we will explore Terraform.\nWhat is Terraform? # Terraform allows you to automate and manage your infrastructure and platforms that run on your servers. It is open-source and uses a declarative approach‚Äîyou define what end result you want, unlike imperative tools, which define how to get there step-by-step.\nTerraform is a tool used for infrastructure provisioning.\nFor example:\nYou‚Äôve started a project and want to deploy several servers to run a microservices application. You run Docker containers to bring the service up. Suppose you decide to build this on the AWS platform to host your entire infrastructure.\nIn this case, the DevOps engineer prepares and provisions the infrastructure, and the developer deploys the software on it. So, where does Terraform come in?\nTerraform is used in the first part‚Äîinfrastructure provisioning.\nWhat is the Difference Between Terraform and Ansible? # By definition, they may sound similar. Both are Infrastructure-as-Code (IaC) tools used for provisioning, configuring, and managing infrastructure.\nHowever:\nTerraform is mainly an infrastructure provisioning tool. It can also deploy tools, but its core strength lies in setting up infrastructure.\nAnsible is mainly a configuration tool. Once the infrastructure is provisioned, Ansible configures it: installs software, deploys apps, and handles updates.\nAnsible is more mature, while Terraform is relatively newer and more advanced in orchestration.\nIn best DevOps practices, both tools are used together to cover the entire setup end-to-end.\nProvisioning and Configuration with Terraform and Ansible (Locally) # We\u0026rsquo;ll now build and deploy a service locally using Terraform and Ansible.\nProvisioning: Terraform Creates Ubuntu Container with SSH # Step 1: Install Required Packages # sudo pacman -S terraform docker ansible openssh Make sure Docker is running:\nsudo systemctl start docker sudo systemctl enable docker Step 2: Generate SSH Key (if not already) # ssh-keygen -t rsa -f ~/.ssh/id_rsa -q -N \u0026#34;\u0026#34; This creates:\n~/.ssh/id_rsa ‚Üí private key\n~/.ssh/id_rsa.pub ‚Üí public key\nExplanation:\n| Part | Explanation | | | | | ssh-keygen | Creates SSH key pairs | | -t rsa | Uses RSA encryption | | -f ~/.ssh/id_rsa | Saves private key here | | -q | Quiet mode | | -N \u0026quot;\u0026quot; | No passphrase (empty password) |\nmain.tf # provider \u0026#34;docker\u0026#34; {} resource \u0026#34;docker_image\u0026#34; \u0026#34;ubuntu\u0026#34; { name = \u0026#34;ubuntu:20.04\u0026#34; } resource \u0026#34;docker_container\u0026#34; \u0026#34;ubuntu_container\u0026#34; { name = \u0026#34;ubuntu-terraform\u0026#34; image = docker_image.ubuntu.name ports { internal = 22 external = 2222 } network_mode = \u0026#34;bridge\u0026#34; command = [ \u0026#34;sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026lt;\u0026lt;-EOT apt update \u0026amp;\u0026amp; apt install -y openssh-server sudo \u0026amp;\u0026amp; mkdir -p /var/run/sshd \u0026amp;\u0026amp; echo \u0026#34;PermitRootLogin yes\u0026#34; \u0026gt;\u0026gt; /etc/ssh/sshd_config \u0026amp;\u0026amp; echo \u0026#34;root:root\u0026#34; | chpasswd \u0026amp;\u0026amp; service ssh start \u0026amp;\u0026amp; tail -f /dev/null EOT ] } versions.tf # terraform { required_providers { docker = { source = \u0026#34;kreuzwerker/docker\u0026#34; } } required_version = \u0026#34;\u0026gt;= 1.0.0\u0026#34; } Initialize and Apply Terraform # terraform init terraform apply -auto-approve Confirm Container and SSH Access # docker ps It should show:\nPORTS: 0.0.0.0:2222-\u0026gt;22/tcp Connect using:\nssh root@127.0.0.1 -p 2222 # Password: root If you get a shell, SSH works.\nCheck Internet in Container # docker exec -it ubuntu-terraform ping -c 4 8.8.8.8 If this fails, run:\ndocker exec -it ubuntu-terraform bash echo \u0026#34;nameserver 8.8.8.8\u0026#34; \u0026gt; /etc/resolv.conf apt update Now your internet inside the container should work.\nConfiguration: Ansible to Set Up Apache in Ubuntu Container # Now that we have provisioned the infrastructure locally, we move to the next step: configuring it using Ansible.\nCreate Ansible Directory # mkdir ansible \u0026amp;\u0026amp; cd ansible Create Inventory File ‚Äì inventory.ini # [web] localhost ansible_host=127.0.0.1 ansible_port=2222 ansible_user=root ansible_password=root ansible_connection=ssh ansible_ssh_common_args=\u0026#39;-o StrictHostKeyChecking=no\u0026#39; Explanation of Inventory # | Line | Description | | | | | [web] | Group name | | localhost | Logical hostname | | ansible_host=127.0.0.1 | IP address | | ansible_port=2222 | Docker SSH port | | ansible_user=root | Username | | ansible_password=root | Password | | ansible_connection=ssh | Connection type | | ansible_ssh_common_args=... | Skip manual key confirmation |\nCreate Playbook ‚Äì install_apache.yml # - name: Install and start Apache Web Server via raw shell commands hosts: web gather_facts: no tasks: - name: Update APT cache raw: apt update - name: Install Apache2 raw: apt install -y apache2 - name: Start Apache now raw: service apache2 start - name: Verify Apache is running raw: service apache2 status || true Run the Playbook # Make sure you\u0026rsquo;re inside the ansible/ directory:\nbashCopyEditansible-playbook -i inventory.ini install_apache.yml Test Apache # curl http://127.0.0.1:2222 You should see the default Apache welcome page.\nSummary: What You‚Äôve Achieved # Provisioned an Ubuntu container with SSH using Terraform\nInstalled and started Apache inside it using Ansible\nVerified that Apache is running and serving web pages\nConclusion # This blog demonstrated how to integrate Terraform and Ansible to provision and configure a local server environment. Terraform handled the creation of the containerized infrastructure, while Ansible managed the configuration tasks such as installing and starting Apache.\nThis approach is practical for learning DevOps workflows and testing deployments locally before scaling to cloud platforms. Using both tools together ensures full automation from infrastructure setup to software deployment. In upcoming sections, this setup can be extended to platforms like AWS, offering even more powerful automation capabilities.\nP.S.\nIf you spot any mistakes, feel free to point them out ‚Äî we‚Äôre all here to learn together! üòä\nHaris\nFAST-NUCES\nBS Computer Science | Class of 2027\nüîó Portfolio: zenvila.github.ioüîó GitHub: github.com/Zenvilaüîó LinkedIn: linkedin.com/in/haris-shahzad-7b8746291**\nüî¨ Member: COLAB (Research Lab)**\n","date":"15 January 2024","externalUrl":null,"permalink":"/posts/terraform-and-ansible/","section":"Posts","summary":"","title":"Terraform And Ansible","type":"posts"},{"content":"","date":"1 January 2024","externalUrl":null,"permalink":"/tags/arm/","section":"Tags","summary":"","title":"ARM","type":"tags"},{"content":" The Challenge # Testing embedded AI for drones and robotics requires expensive ARM hardware. Development cycles are slow without proper testing environments.\nThe Solution # Simulated ARM64 systems on PC using hypervisor, creating a safe testing environment for embedded AI without physical hardware.\nKey Achievement # Accelerated prototyping cycles by 10x, enabling rapid testing of AI pipelines before deploying on real embedded devices.\nTechnologies Used # ARM Architecture, Virtualization, KVM, QEMU, Embedded Systems\n","date":"1 January 2024","externalUrl":"","permalink":"/projects/arm-embedded-ai-lab-on-pc-using-hypervisor/","section":"Projects","summary":"Simulated ARM64 systems on PC using hypervisor for embedded AI testing in drones, robotics, and aerospace without physical hardware.","title":"ARM Embedded AI Lab on PC using Hypervisor","type":"projects"},{"content":"","date":"1 January 2024","externalUrl":null,"permalink":"/tags/embedded-systems/","section":"Tags","summary":"","title":"Embedded Systems","type":"tags"},{"content":"","date":"1 January 2024","externalUrl":null,"permalink":"/tags/virtualization/","section":"Tags","summary":"","title":"Virtualization","type":"tags"},{"content":" SSH Tunneling # New to SSH? Check out my intro blog:\nSSH Blog : https://hashnode.com/edit/cm7vvn7eg000209l1fr88hp1o\nWhat is SSH Tunneling? # SSH Tunneling is like building a secure underground pipe between two computers. Imagine two laptops ‚Äî one in Lahore and the other in Karachi ‚Äî and we want to connect them even if they are on different networks or behind NAT/firewalls. With SSH tunneling, you can securely forward a port from one machine to another over the internet without exposing ports directly to the world.\nWhy SSH Tunneling Instead of Simple SSH Port Forwarding? # Traditional SSH Port Forwarding only works if the other device is on the same network or has a public IP. But often:\nYou\u0026rsquo;re on different Wi-Fi or ISPs (like college vs home)\nThe remote device is behind NAT or firewall\nYou don‚Äôt know the public IP of the other device\nThat‚Äôs where SSH tunneling with third-party jump servers like serveo.net comes in ‚Äî no public IP needed!\nWhat is Serveo.net? # Serveo.net is a free reverse SSH tunneling service that acts like a middleman. It gives you a public-facing URL or port and forwards all the traffic to your local machine over SSH.\nNo need to install anything ‚Äî just use regular ssh command!\nScenario: Haris (Client) ‚¨ÖÔ∏è‚û°Ô∏è Dawood (Host) # We have two laptops:\nüíª Dawood‚Äôs Laptop ‚Äî the one we want to connect to (host)\nüíª Haris‚Äôs Laptop ‚Äî the one we connect from (client)\nWe‚Äôll build a tunnel so Haris can SSH into Dawood‚Äôs laptop, even if they‚Äôre on completely different networks.\nüõ†Ô∏è Step-by-Step Guide # üîç Step 1: Check SSH is Enabled on Dawood‚Äôs Laptop # On Dawood\u0026rsquo;s laptop, run:\nsudo systemctl status ssh If not active, start it:\nsudo systemctl start ssh You can also check if port 22 is listening:\nsudo netstat -tuln | grep :22 If it doesn‚Äôt show anything, install OpenSSH:\nsudo pacman -Syu openssh-server Step 2: Know Your IP (Optional) # If you want to check your public IP manually:\nifconfig But remember: Serveo removes the need to know public IPs altogether. This is just for knowledge.\nStep 3: Create SSH Tunnel from Dawood‚Äôs Laptop to Serveo # Now, on Dawood\u0026rsquo;s laptop, run this:\nssh -R 5678:localhost:22 serveo.net Explanation:\n-R 5678:localhost:22 ‚Üí This means: any connection made to serveo.net:5678 should be forwarded to localhost:22 on Dawood‚Äôs machine.\nserveo.net ‚Üí The middleman server doing the tunneling.\nOutput should look like:\nForwarding TCP connections from serveo.net:5678 Keep this terminal open ‚Äî the tunnel is now active.\nWhat Just Happened? # We told Serveo:\n‚ÄúHey, when anyone connects to port 5678 on your server, forward that request to Dawood‚Äôs port 22 on his local machine.‚Äù\nSo now the world has access to Dawood‚Äôs SSH (securely) ‚Äî only through this tunnel.\nStep 4: Connect from Haris‚Äôs Laptop # Now, on Haris‚Äôs laptop, run this:\nssh -p 5678 dawood@serveo.net It will ask for Dawood‚Äôs Linux password (the username must match his system).\nYou are now connected remotely to Dawood‚Äôs machine via SSH, using the tunnel!\nNow using rsync Over Tunnel (File Transfer) :\nTo copy files from Haris to Dawood:\nrsync -avz -e \u0026#34;ssh -p 5678\u0026#34; ~/my_folder dawood@serveo.net:~/destination_folder To pull files from Dawood to Haris:\nrsync -avz -e \u0026#34;ssh -p 5678\u0026#34; dawood@serveo.net:~/remote_folder ~/local_folder Warnings # Tunnel breaks if Dawood closes terminal\nNo authentication system on serveo.net, so only use for temporary or trusted access\nUse strong passwords or even better, set up SSH key authentication\nNote: It‚Äôs not just serveo.net that you can use‚Äîthere are many other services like ngrok and localXpose that help you bypass restrictions and create secure, encrypted connections. These tools are especially useful when dealing with NATs or firewalls, particularly if you need to connect across different types of network restrictions.\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1749231949407/d0212a9e-75b8-4bd3-9876-0f9d52dc747a.png align=\u0026ldquo;center\u0026rdquo;)\nP.S.\nIf you spot any mistakes, please don\u0026rsquo;t hesitate to point them out. We\u0026rsquo;re all here to learn together! üòä Haris\nFAST (NUCES)\nBS Computer Science | Class of 2027\nüìå Portfolio: zenvila.github.io\nüìå GitHub: github.com/Zenvila\nüìå LinkedIn: linkedin.com/in/haris-shahzad-7b8746291\nüìå Member: COLAB (Research Lab)\n","date":"16 November 2023","externalUrl":null,"permalink":"/posts/ssh-tunneling/","section":"Posts","summary":"","title":"Ssh Tunneling","type":"posts"},{"content":" The Challenge # Retail stores require efficient inventory and sales tracking systems to manage operations and analyze performance.\nThe Solution # Developed a web-based store management system for inventory and sales tracking.\nKey Achievement # Enabled real-time inventory management and sales analytics, improving operational efficiency and decision-making.\nTechnologies Used # GitHub, Linux, Web Development\n","date":"15 October 2023","externalUrl":"","permalink":"/projects/super-store-management-system/","section":"Projects","summary":"Web-based store management system for inventory and sales tracking.","title":"Super Store Management System","type":"projects"},{"content":" My Journey with Oracle Cloud Infrastructure (OCI) Foundations Associate 2025 # Recently, I completed the Oracle Cloud Infrastructure (OCI) 2025 Foundations Associate (1Z0-1085-25) certification ‚Äî a beginner-level course designed to help learners understand the fundamentals of Oracle Cloud and how it supports modern digital transformation.\nThrough a series of well-structured learning modules, I gained hands-on knowledge about OCI‚Äôs architecture, services, and the principles behind cloud computing. Here‚Äôs an overview of the key modules and what I learned from each.\n1. Cloud Concepts and OCI Overview # The course began with an introduction to cloud computing models ‚Äî including Infrastructure as a Service (IaaS), Platform as a Service (PaaS), and Software as a Service (SaaS).\nIt explained how Oracle Cloud Infrastructure delivers computing resources on demand through its global regions and availability domains. Each region represents a geographical location, and every availability domain acts as an independent data center with its own power, cooling, and network resources.\nThis module helped me understand the advantages of the cloud, such as elasticity, high availability, fault tolerance, and cost efficiency, and how OCI provides these through its well-planned infrastructure.\n2. Compute Services # The Compute module focused on OCI‚Äôs ability to run workloads on flexible computing resources. I learned how users can create and manage virtual machines (VMs) or bare metal instances depending on their performance needs.\nThe course explained key features such as:\nInstance Configuration and Pooling for creating scalable application environments.\nAutoscaling, which automatically adjusts resources based on traffic demand.\nLoad Balancing, which distributes incoming traffic across multiple servers to ensure reliability.\nThis module gave me a solid understanding of how OCI Compute helps build scalable and fault-tolerant applications.\n3. Storage Services # The Storage module introduced the different types of storage that OCI offers:\nObject Storage for unstructured data like backups, logs, and images.\nBlock Volume for high-performance storage attached to compute instances.\nFile Storage for shared file systems accessible across multiple instances.\nArchive Storage for long-term retention of infrequently accessed data.\nI also learned about data durability, redundancy, and lifecycle management, which ensure that data remains available and protected across regions. Understanding the balance between performance, cost, and durability was an essential takeaway from this section.\n4. Networking # The Networking module was one of the most important sections of the course. It explained the role of the Virtual Cloud Network (VCN) ‚Äî OCI‚Äôs private, customizable network environment that connects all deployed resources.\nKey concepts included:\nSubnets for organizing and isolating resources.\nGateways (Internet, NAT, Service, and Dynamic Routing Gateways) for connecting to external or private networks.\nSecurity Lists and Network Security Groups (NSGs) for controlling inbound and outbound traffic.\nLoad Balancers for distributing workloads evenly.\nThis module gave me a deeper understanding of how cloud networking works and how OCI ensures security and connectivity within cloud environments.\n5. Database Services # The Database module explored Oracle‚Äôs advanced database offerings. I learned about:\nAutonomous Database ‚Äî a self-managing, self-patching, and self-tuning database that eliminates manual administration.\nOracle Database Cloud Service, which provides full control over Oracle databases in the cloud.\nMySQL HeatWave, a high-performance, in-memory query accelerator that runs analytics directly on transactional data.\nThis section highlighted how Oracle leverages its decades of database expertise to provide fast, secure, and reliable database services for both traditional and modern workloads.\n6. Security and Identity # Security in the cloud is critical, and this module covered OCI‚Äôs approach to maintaining it through Identity and Access Management (IAM).\nI learned how policies, users, groups, compartments, and dynamic groups are used to manage access to resources. The concept of the Shared Responsibility Model clarified how both Oracle and the customer play roles in maintaining security.\nOther important features included encryption at rest and in transit, audit logs, and fault domains for protecting workloads from hardware-level failures.\n7. Pricing, SLAs, and Support # This module focused on OCI‚Äôs pricing model, Service Level Agreements (SLAs), and support tiers.\nIt explained how Oracle provides transparent and predictable pricing across services and offers multiple support levels based on customer needs.\nI also learned about the Always Free Tier, which allows users to experiment with key OCI services at no cost, and usage monitoring tools that help control and optimize spending.\nThe SLA discussion emphasized Oracle‚Äôs strong guarantees for availability, manageability, and performance, reflecting its commitment to enterprise-grade reliability.\n8. Shared Responsibility and Best Practices # The final module reinforced the Shared Responsibility Model, defining what Oracle secures (infrastructure and hardware) versus what customers secure (applications, data, and access).\nIt also introduced cloud governance best practices such as compartment design, policy management, tagging resources, and monitoring usage.\nThese concepts are essential for maintaining security, efficiency, and compliance in cloud operations.\nConclusion # Completing the OCI 2025 Foundations Associate certification gave me a well-rounded understanding of how Oracle Cloud Infrastructure operates ‚Äî from its global architecture to its compute, storage, networking, and security services.\nEach module built a deeper appreciation of how Oracle has designed its cloud to meet the demands of modern applications while maintaining flexibility, cost control, and performance.\nThis certification represents a strong first step toward mastering Oracle Cloud. Going forward, I plan to explore the OCI Architect Associate track to gain deeper, hands-on knowledge in designing and deploying enterprise-level solutions on OCI.\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1761897112304/bd42b284-cec2-41f0-a70e-6aba3cfe0fa4.png align=\u0026ldquo;center\u0026rdquo;)\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1761897136539/d6e995f9-5523-413d-acee-6650ba0b51f8.png align=\u0026ldquo;center\u0026rdquo;)\nHaris\nFAST-NUCES\nBS Computer Science | Class of 2027\nüîó Portfolio: zenvila.github.io\nüîó GitHub: github.com/Zenvila\nüîó LinkedIn: linkedin.com/in/haris-shahzad-7b8746291\nüî¨ Member: COLAB (Research Lab)\n","date":"27 September 2023","externalUrl":null,"permalink":"/posts/my-journey-with-oracle-cloud-infrastructure-oci-foundations-associate-2025/","section":"Posts","summary":"","title":"My Journey With Oracle Cloud Infrastructure Oci Foundations Associate 2025","type":"posts"},{"content":" Linear Algebra for AI # What Does Math Have to Do with Machine Learning? # All programming involves math at some level ‚Äî and machine learning is no exception.\nIn fact, machine learning is programming by optimization, and to understand optimization, we need mathematics.\nTo understand what optimization is, how it works, and how machines learn, we need a strong mathematical foundation. In this series, we will cover the first mathematical tool: Linear Algebra, which plays a major role in understanding optimization and its use in machine learning.\nLinear Algebra # Linear algebra helps us understand:\nThe object being optimized\nHow data is structured\nHow models are built and computed\nTakeaway for Today # Linear Algebra is important, and it is not the same as high-school algebra.\nWhy do we care about linear algebra?\nBecause it is the mathematics of arrays ‚Äî and in machine learning, everything is made of arrays:\nThe data, like an image, is an array\nThe models are collections of arrays\nEven the internal computations of these models are performed using arrays\nWhat Is the Role of Linear Algebra in GPUs? # GPUs (Graphics Processing Units) are designed to perform many small math operations in parallel. Most of these operations are linear algebra tasks, like:\nMatrix multiplication\nVector addition\nDot products\nThese operations are used in:\nGraphics (e.g., rotating 3D objects, lighting)\nAI/ML (e.g., neural networks where weights and inputs are matrices/vectors)\nGPUs are perfect for this because:\nThey can do the same math operation on thousands of numbers at once\nLinear algebra is highly parallel, making it a natural fit\nWhat Type of Linear Algebra Is Used in GPUs? # | Concept | Description | | | | | Vectors | Lists of numbers (1D) | | Matrices | Tables of numbers (2D) | | Matrix Multiplication | Core for transforming data and neural net calculations | | Dot Product | Used in graphics and ML to calculate similarity | | Transpose, Inverse | Used in transformations and solving equations |\nReal AI Example # In deep learning:\nInputs (like images or text) are represented as vectors/matrices\nWeights are also stored as matrices\nThe output is computed using matrix multiplications\nThe GPU is the engine that performs these operations quickly.\nWhat Is the Role of Linear Algebra in AI? # Linear algebra is the engine of AI.\nIt helps us represent and transform data ‚Äî whether it‚Äôs images, audio, or text ‚Äî into a mathematical format that machines can learn from.\nAI works with numbers, and linear algebra gives us a clean way to:\nStore data (as vectors and matrices)\nTransform data (rotate, scale, combine)\nLearn patterns (using matrix multiplications and updates)\nSimple Example # A black and white image is just a grid of numbers (pixel brightness) ‚Äî this is a matrix.\nTo process the image:\nAI uses matrix operations, like multiplying it with a weight matrix\nThis helps the model detect edges, corners, and patterns\nThe same applies to text and audio ‚Äî everything is turned into matrices or vectors.\nAlgorithms from Linear Algebra Used in AI # | Algorithm / Concept | Use in AI | Example | | | | | | Matrix Multiplication | Core of neural networks | Input √ó Weights = Output | | Dot Product | Measures similarity | Word embeddings similarity | | Eigenvalues/Eigenvectors | Dimensionality reduction (PCA) | Compress high-dimensional data | | Singular Value Decomposition (SVD) | Recommendation systems | Netflix/movie suggestions | | LU / QR Decomposition | Solving systems of equations | Optimization problems | | Transpose, Inverse | Reshaping and solving systems | Reversing transformations |\nComparison \u0026amp; Transformation # | Without Linear Algebra | With Linear Algebra | | | | | Raw data handled manually | Data represented as matrices | | Hard to find patterns | Easy to apply transformations | | Slow calculations | Fast parallel operations on GPUs | | Hard to scale to big data | Scales well using matrix operations |\nSummary # Linear Algebra is the core language of AI\nIt lets AI store, transform, and learn from data\nIt is used deeply in both hardware (GPU) and software (code)\nKey operations like dot products, matrix multiplication, decompositions power every AI model\nHardware Perspective (GPU \u0026amp; CPU) # GPUs love matrices: They can process thousands of matrix multiplications at once\nFaster matrix operations mean faster training\nAll deep learning frameworks (like TensorFlow and PyTorch) rely on GPU acceleration for this reason\nLinear algebra fits perfectly into hardware like GPUs, which are built for parallel math.\nGPUs contain thousands of small cores that can multiply numbers and add them ‚Äî ideal for matrix operations in AI.\nSo when your model runs input √ó weights, the GPU does this fast and in parallel, unlike a CPU that works step by step.\nExamples:\nTraining a neural network with 1 million weights may take hours on a CPU, but minutes on a GPU\nProcessing an image with 1,000,000 pixel values √ó 10,000 weights is handled much faster on a GPU\nSoftware Perspective (AI Libraries \u0026amp; Code) # AI frameworks like TensorFlow, PyTorch, and NumPy are built on top of linear algebra libraries such as:\nBLAS (Basic Linear Algebra Subprograms)\ncuBLAS (NVIDIA‚Äôs GPU-accelerated version)\nBLAS provides a standard interface, which means AI software written for it can run efficiently on many kinds of hardware.\nNVIDIA cuBLAS is a GPU-accelerated library optimized for AI and High-Performance Computing (HPC). It provides drop-in support for industry-standard linear algebra operations.\nWhen you write something like:\noutput = layer(input) That code actually performs matrix multiplication in the background.\nEven if you don‚Äôt see the math, it‚Äôs all vectors, matrices, and dot products under the hood.\nExample:\nWord2Vec converts words into vectors and finds similarity using the dot product ‚Äî powered entirely by linear algebra.\nConclusion # Linear Algebra is not optional ‚Äî it\u0026rsquo;s essential for understanding how machine learning models are built, trained, and deployed.\nFrom the way we store data, to how we train models, and the hardware/software we use ‚Äî linear algebra is at the center of it all.\nBy learning it, you\u0026rsquo;re not just learning math ‚Äî you\u0026rsquo;re learning the language of AI.\n**P.S:**if you spot any mistakes, feel free to point them out ‚Äî we‚Äôre all here to learn together! üòä\nHaris\nFAST-NUCES\nBS Computer Science | Class of 2027\nüîó Portfolio: zenvila.github.io\nüîó GitHub: github.com/Zenvila\nüîó LinkedIn: linkedin.com/in/haris-shahzad-7b8746291**\nüî¨ Member: COLAB (Research Lab)**\n","date":"8 September 2023","externalUrl":null,"permalink":"/posts/linear-algebra-for-ai/","section":"Posts","summary":"","title":"Linear Algebra For Ai","type":"posts"},{"content":" The Builder\u0026rsquo;s Mindset # I\u0026rsquo;m a Computer Science student who prefers the terminal over a GUI. I use Arch Linux not because it\u0026rsquo;s hard, but because it gives me control. I don\u0026rsquo;t just study theory; I build real systems. Whether it\u0026rsquo;s Automation, Robotics, or AI, I\u0026rsquo;m driven by the challenge of making things work in the real world.\nThe command line is my playground. Every script I write, every system I configure, every problem I solve‚Äîit all happens through code and configuration. There\u0026rsquo;s something deeply satisfying about understanding how things work at a fundamental level, and that\u0026rsquo;s what drives me.\nThe Modern World # \u0026ldquo;The illiterate of the 21st century will not be those who cannot read and write, but those who cannot learn, unlearn, and relearn.\u0026rdquo; ‚Äì Alvin Toffler\nThis quote defines my approach to technology. The field moves fast, and what worked yesterday might be obsolete tomorrow. I\u0026rsquo;m constantly learning new tools, unlearning outdated practices, and relearning better ways to solve problems. This isn\u0026rsquo;t just about staying current‚Äîit\u0026rsquo;s about staying effective.\nMy Mission # My goal is simple: solve complex problems in real-time. Whether it\u0026rsquo;s deploying a site, training a neural network, or automating a workflow, I believe in persistence and results. I don\u0026rsquo;t just want to know how something works; I want to make it work better.\nEvery project is an opportunity to push boundaries. Every failure is a lesson. Every success is a stepping stone to the next challenge. This is how I approach my work, and this is what makes me a builder.\nFinal Thought # \u0026ldquo;Talk is cheap. Show me the code.\u0026rdquo; ‚Äì Linus Torvalds\nThis is my philosophy. I\u0026rsquo;d rather spend hours debugging a script than hours explaining what I plan to do. Code speaks louder than words, and results speak louder than promises. This is how I work, and this is what I bring to every project.\n","externalUrl":null,"permalink":"/about/","section":"About","summary":"","title":"About","type":"about"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":" Education # Bachelor of Science in Computer Science\nNational University of Computer and Emerging Sciences (FAST-NUCES)\nCurrently 6th Semester\nTechnical Arsenal # Category Skills \u0026amp; Tools Languages Python, C++, C, JavaScript, Bash, SQL DevOps Docker, Kubernetes, Terraform, Ansible, Prometheus, Grafana Systems Arch Linux, Kali, Asterisk (VoIP), FreePBX, SSH AI / ML LLMs, RAG, Ollama, LoRA Fine-Tuning Web \u0026amp; Cloud Django, Hugo, Oracle Cloud (OCI), Git/GitHub Certifications # GitHub for Open Standards Development (LFD140)\nThe Linux Foundation | Issued Nov 2025\nOracle Cloud Infrastructure 2025 Foundations Associate (1Z0-1085-25)\nOracle\nExperience # COLAB NU ‚Äî Tier III Member # Oct 2024 - Present ¬∑ 1 yr 4 mos | Peshawar, Khyber Pakhtunkhwa, Pakistan\nContributed to DevOps automation and AI/ML research in a research-driven environment, working with Docker, Kubernetes, Prometheus, Grafana, Ansible, AWS, PyTorch, OpenVINO, and LangChain.\nCOLAB NU ‚Äî Tier II Member # Mar 2024 - Oct 2024 ¬∑ 8 mos | Peshawar, Khyber Pakhtunkhwa, Pakistan\nMaintained and secured Linux servers, configured services like SSH, FTP (ProFTPD), and monitoring with Monit, with hands-on experience in rsync, bash scripting, and server hardening.\nCOLAB NU ‚Äî Tier I Member # Sep 2023 - Mar 2024 ¬∑ 7 mos\nBuilt foundation in Linux system administration, Git \u0026amp; GitHub, and contributed to web projects using HTML, CSS, and JavaScript while developing technical documentation skills.\nFiverr ‚Äî Freelance # Jul 2025 - Present ¬∑ 7 mos | Remote\nGDGoC FAST Peshawar Campus ‚Äî Team Lead Devs # Oct 2025 - Present ¬∑ 4 mos | On-site\nAIM Lab ‚Äî AI/ML Intern # Jun 2025 - Aug 2025 ¬∑ 3 mos | Islamabad, Pakistan\nEracon Technologies (Pvt) Ltd. ‚Äî Verification Lead # May 2022 - Mar 2023 ¬∑ 11 mos | Islamabad, Pakistan | On-site\nPrime BPO 2 ‚Äî Customer Support Specialist # Jul 2021 - Feb 2022 ¬∑ 8 mos | Islamabad, Pakistan | On-site\nContact # ","externalUrl":null,"permalink":"/resume/","section":"Resume","summary":"","title":"Resume","type":"resume"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]